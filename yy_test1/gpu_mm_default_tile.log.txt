// -----// IR Dump After StablehloLegalizeToHloPass (stablehlo-legalize-to-hlo) //----- //
module {
  func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
    %0 = "mhlo.dot"(%arg0, %arg1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
    return %0 : tensor<512x1024xf32>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
  %0 = "mhlo.dot"(%arg0, %arg1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  return %0 : tensor<512x1024xf32>
}

// -----// IR Dump After LegalizeControlFlowPass (mhlo-legalize-control-flow) //----- //
func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
  %0 = "mhlo.dot"(%arg0, %arg1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  return %0 : tensor<512x1024xf32>
}

// -----// IR Dump After TopLevelSCFToCFG (iree-top-level-scf-to-cfg) //----- //
func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
  %0 = "mhlo.dot"(%arg0, %arg1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  return %0 : tensor<512x1024xf32>
}

// -----// IR Dump After MHLOToMHLOPreprocessing (iree-mhlo-to-mhlo-preprocessing) //----- //
func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
  %0 = "mhlo.dot"(%arg0, %arg1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  return %0 : tensor<512x1024xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
  %0 = "mhlo.dot"(%arg0, %arg1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  return %0 : tensor<512x1024xf32>
}

// -----// IR Dump After ShapeToShapeLowering (shape-to-shape-lowering) //----- //
func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
  %0 = "mhlo.dot"(%arg0, %arg1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  return %0 : tensor<512x1024xf32>
}

// -----// IR Dump After ConvertShapeToStandard (convert-shape-to-std) //----- //
module {
  func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
    %0 = "mhlo.dot"(%arg0, %arg1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
    return %0 : tensor<512x1024xf32>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
  %0 = "mhlo.dot"(%arg0, %arg1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  return %0 : tensor<512x1024xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
  %0 = "mhlo.dot"(%arg0, %arg1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  return %0 : tensor<512x1024xf32>
}

// -----// IR Dump After Inliner (inline) //----- //
module {
  func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
    %0 = "mhlo.dot"(%arg0, %arg1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
    return %0 : tensor<512x1024xf32>
  }
}


// -----// IR Dump After DemoteI64ToI32 (iree-util-demote-i64-to-i32) //----- //
module {
  func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
    %0 = "mhlo.dot"(%arg0, %arg1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
    return %0 : tensor<512x1024xf32>
  }
}


// -----// IR Dump After DemoteF64ToF32 (iree-util-demote-f64-to-f32) //----- //
module {
  func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
    %0 = "mhlo.dot"(%arg0, %arg1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
    return %0 : tensor<512x1024xf32>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
  %0 = "mhlo.dot"(%arg0, %arg1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  return %0 : tensor<512x1024xf32>
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
  %0 = "mhlo.dot"(%arg0, %arg1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  return %0 : tensor<512x1024xf32>
}

// -----// IR Dump After HloLegalizeShapeComputationsPass (hlo-legalize-shape-computations) //----- //
func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
  %0 = "mhlo.dot"(%arg0, %arg1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  return %0 : tensor<512x1024xf32>
}

// -----// IR Dump After ConvertMHLOToLinalgExt (iree-mhlo-to-linalg-ext) //----- //
func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
  %0 = "mhlo.dot"(%arg0, %arg1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  return %0 : tensor<512x1024xf32>
}

// -----// IR Dump After ConvertMHLOToLinalgOnTensors (iree-mhlo-to-linalg-on-tensors) //----- //
module {
  func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<512x1024xf32>
    %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %2 = linalg.matmul ins(%arg0, %arg1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%1 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    return %2 : tensor<512x1024xf32>
  }
}


// -----// IR Dump After ReconcileUnrealizedCasts (reconcile-unrealized-casts) //----- //
module {
  func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<512x1024xf32>
    %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %2 = linalg.matmul ins(%arg0, %arg1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%1 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    return %2 : tensor<512x1024xf32>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = tensor.empty() : tensor<512x1024xf32>
  %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %2 = linalg.matmul ins(%arg0, %arg1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%1 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  return %2 : tensor<512x1024xf32>
}

// -----// IR Dump After VerifyCompilerMHLOInputLegality (iree-mhlo-verify-compiler-input-legality) //----- //
module {
  func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<512x1024xf32>
    %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %2 = linalg.matmul ins(%arg0, %arg1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%1 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    return %2 : tensor<512x1024xf32>
  }
}


// -----// IR Dump After IREEImportPublic (iree-import-public) //----- //
module {
  func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<512x1024xf32>
    %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %2 = linalg.matmul ins(%arg0, %arg1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%1 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    return %2 : tensor<512x1024xf32>
  }
}


// -----// IR Dump After ImportMLProgram (iree-import-ml-program) //----- //
module {
  func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<512x1024xf32>
    %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %2 = linalg.matmul ins(%arg0, %arg1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%1 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    return %2 : tensor<512x1024xf32>
  }
}


// -----// IR Dump After SanitizeModuleNames (iree-sanitize-module-names) //----- //
module {
  func.func @matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<512x1024xf32>
    %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %2 = linalg.matmul ins(%arg0, %arg1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%1 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    return %2 : tensor<512x1024xf32>
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::ABI::WrapEntryPointsPass (iree-abi-wrap-entry-points) //----- //
module {
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x1024xf32>
    %3 = call @_matmul_static(%0, %1, %2) : (tensor<512x256xf32>, tensor<256x1024xf32>, tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %4 = hal.tensor.export %3 : tensor<512x1024xf32> -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
  func.func private @_matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<512x1024xf32>
    %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %2 = linalg.matmul ins(%arg0, %arg1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%1 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    return %2 : tensor<512x1024xf32>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_matmul_static(%arg0: tensor<512x256xf32>, %arg1: tensor<256x1024xf32>, %arg2: tensor<512x1024xf32>) -> tensor<512x1024xf32> {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = tensor.empty() : tensor<512x1024xf32>
  %1 = linalg.fill ins(%cst : f32) outs(%0 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %2 = linalg.matmul ins(%arg0, %arg1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%1 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  return %2 : tensor<512x1024xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x1024xf32>
  %3 = call @_matmul_static(%0, %1, %2) : (tensor<512x256xf32>, tensor<256x1024xf32>, tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = hal.tensor.export %3 : tensor<512x1024xf32> -> !hal.buffer_view
  return %4 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After Inliner (inline) //----- //
module {
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = tensor.empty() : tensor<512x1024xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
module {
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = tensor.empty() : tensor<512x1024xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After DemoteF64ToF32 (iree-util-demote-f64-to-f32) //----- //
module {
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = tensor.empty() : tensor<512x1024xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After DetachElementwiseFromNamedOps (iree-flow-detach-elementwise-from-named-ops) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After LinalgNamedOpConversion (linalg-named-op-conversion) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After Convert1X1FilterConv2DToMatmul (iree-flow-convert-1x1-filter-conv2d-to-matmul) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After VerifyInputLegality (iree-verify-input-legality) //----- //
module {
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = tensor.empty() : tensor<512x1024xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After ExpandTensorShapes (iree-flow-expand-tensor-shapes) //----- //
module {
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = tensor.empty() : tensor<512x1024xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
module attributes {iree.fixedpoint.iteration = 0 : index} {
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = tensor.empty() : tensor<512x1024xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
module attributes {iree.fixedpoint.iteration = 0 : index} {
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = tensor.empty() : tensor<512x1024xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
module attributes {iree.fixedpoint.iteration = 0 : index} {
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = tensor.empty() : tensor<512x1024xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
module {
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = tensor.empty() : tensor<512x1024xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After TensorPadToTensorInsertSlice (iree-flow-tensor-pad-to-tensor-insert-slice) //----- //
module {
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = tensor.empty() : tensor<512x1024xf32>
    %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After ConvertElementwiseToLinalg (convert-elementwise-to-linalg) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After LinalgFoldUnitExtentDims (linalg-fold-unit-extent-dims) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After InterchangeGenericOps (iree-flow-interchange-generic-ops) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After FusionOfTensorOps (iree-flow-fusion-of-tensor-ops) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After LinalgDetensorize (linalg-detensorize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After CollapseDims (iree-flow-collapse-dims) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After SplitReduction (iree-flow-split-reduction-ops) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After InterchangeGenericOps (iree-flow-interchange-generic-ops) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %5 = hal.tensor.export %4 : tensor<512x1024xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After FormDispatchRegions (iree-flow-form-dispatch-regions) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = tensor.empty() : tensor<512x1024xf32>
  %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1_0 = arith.constant 1 : index
  %4 = affine.apply affine_map<()[s0, s1, s2] -> ((s1 - s0) ceildiv s2)>()[%c0, %c512, %c1_0]
  %c0_1 = arith.constant 0 : index
  %c1024 = arith.constant 1024 : index
  %c1_2 = arith.constant 1 : index
  %5 = affine.apply affine_map<()[s0, s1, s2] -> ((s1 - s0) ceildiv s2)>()[%c0_1, %c1024, %c1_2]
  %c0_3 = arith.constant 0 : index
  %c1_4 = arith.constant 1 : index
  %6 = affine.apply affine_map<()[s0, s1, s2] -> ((s1 - s0) ceildiv s2)>()[%c0_3, %c1, %c1_4]
  %7 = flow.dispatch.region[%4, %5, %6] -> (tensor<512x1024xf32>) {
    %9 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    flow.return %9 : tensor<512x1024xf32>
  } count(%arg3: index, %arg4: index, %arg5: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg3, %arg4, %arg5
    flow.return %x, %y, %z : index, index, index
  }
  %8 = hal.tensor.export %7 : tensor<512x1024xf32> -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After FormDispatchWorkgroups (iree-flow-form-dispatch-workgroups) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = flow.dispatch.workgroups[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32> =
      (%arg3: !flow.dispatch.tensor<readonly:tensor<512x256xf32>>, %arg4: !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>, %arg5: !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>) {
    %cst = arith.constant 0.000000e+00 : f32
    %4 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
    %5 = flow.dispatch.tensor.load %arg4, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
    %6 = tensor.empty() : tensor<512x1024xf32>
    %7 = linalg.fill ins(%cst : f32) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %8 = linalg.matmul ins(%4, %5 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%7 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    flow.dispatch.tensor.store %8, %arg5, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    flow.return
  } count(%arg3: index, %arg4: index, %arg5: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg3, %arg4, %arg5
    flow.return %x, %y, %z : index, index, index
  }
  %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After CaptureDispatchDynamicDims (iree-flow-capture-dispatch-dynamic-dims) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = flow.dispatch.workgroups[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32> =
      (%arg3: !flow.dispatch.tensor<readonly:tensor<512x256xf32>>, %arg4: !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>, %arg5: !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>) {
    %cst = arith.constant 0.000000e+00 : f32
    %4 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
    %5 = flow.dispatch.tensor.load %arg4, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
    %6 = tensor.empty() : tensor<512x1024xf32>
    %7 = linalg.fill ins(%cst : f32) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %8 = linalg.matmul ins(%4, %5 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%7 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    flow.dispatch.tensor.store %8, %arg5, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    flow.return
  } count(%arg3: index, %arg4: index, %arg5: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg3, %arg4, %arg5
    flow.return %x, %y, %z : index, index, index
  }
  %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = flow.dispatch.workgroups[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32> =
      (%arg3: !flow.dispatch.tensor<readonly:tensor<512x256xf32>>, %arg4: !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>, %arg5: !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>) {
    %cst = arith.constant 0.000000e+00 : f32
    %4 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
    %5 = flow.dispatch.tensor.load %arg4, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
    %6 = tensor.empty() : tensor<512x1024xf32>
    %7 = linalg.fill ins(%cst : f32) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %8 = linalg.matmul ins(%4, %5 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%7 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    flow.dispatch.tensor.store %8, %arg5, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    flow.return
  } count(%arg3: index, %arg4: index, %arg5: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg3, %arg4, %arg5
    flow.return %x, %y, %z : index, index, index
  }
  %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = flow.dispatch.workgroups[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32> =
      (%arg3: !flow.dispatch.tensor<readonly:tensor<512x256xf32>>, %arg4: !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>, %arg5: !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>) {
    %cst = arith.constant 0.000000e+00 : f32
    %4 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
    %5 = flow.dispatch.tensor.load %arg4, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
    %6 = tensor.empty() : tensor<512x1024xf32>
    %7 = linalg.fill ins(%cst : f32) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %8 = linalg.matmul ins(%4, %5 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%7 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    flow.dispatch.tensor.store %8, %arg5, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    flow.return
  } count(%arg3: index, %arg4: index, %arg5: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg3, %arg4, %arg5
    flow.return %x, %y, %z : index, index, index
  }
  %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After InitializeEmptyTensors (iree-flow-initialize-empty-tensors) //----- //
module {
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = flow.dispatch.workgroups[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32> =
        (%arg3: !flow.dispatch.tensor<readonly:tensor<512x256xf32>>, %arg4: !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>, %arg5: !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>) {
      %cst = arith.constant 0.000000e+00 : f32
      %4 = flow.dispatch.tensor.load %arg3, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
      %5 = flow.dispatch.tensor.load %arg4, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
      %6 = tensor.empty() : tensor<512x1024xf32>
      %7 = linalg.fill ins(%cst : f32) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
      %8 = linalg.matmul ins(%4, %5 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%7 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
      flow.dispatch.tensor.store %8, %arg5, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
      flow.return
    } count(%arg3: index, %arg4: index, %arg5: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg3, %arg4, %arg5
      flow.return %x, %y, %z : index, index, index
    }
    %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After OutlineDispatchRegions (iree-flow-outline-dispatch-regions) //----- //
module {
  flow.executable private @matmul_static_dispatch_0 {
    flow.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !flow.dispatch.tensor<readonly:tensor<512x256xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %2 = tensor.empty() : tensor<512x1024xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = flow.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
    %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = flow.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After StripDebugOps (iree-util-strip-debug-ops) //----- //
flow.executable private @matmul_static_dispatch_0 {
  flow.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !flow.dispatch.tensor<readonly:tensor<512x256xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>) {
      %cst = arith.constant 0.000000e+00 : f32
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
      %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
      %2 = tensor.empty() : tensor<512x1024xf32>
      %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
      %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
      flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
      return
    }
  }
}

// -----// IR Dump After DeduplicateExecutables (iree-flow-deduplicate-executables) //----- //
module {
  flow.executable private @matmul_static_dispatch_0 {
    flow.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !flow.dispatch.tensor<readonly:tensor<512x256xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %2 = tensor.empty() : tensor<512x1024xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = flow.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
    %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After CleanupTensorShapes (iree-flow-cleanup-tensor-shapes) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = flow.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = flow.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
flow.executable private @matmul_static_dispatch_0 {
  flow.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !flow.dispatch.tensor<readonly:tensor<512x256xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>) {
      %cst = arith.constant 0.000000e+00 : f32
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
      %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
      %2 = tensor.empty() : tensor<512x1024xf32>
      %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
      %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
      flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
      return
    }
  }
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = flow.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
flow.executable private @matmul_static_dispatch_0 {
  flow.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !flow.dispatch.tensor<readonly:tensor<512x256xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>) {
      %cst = arith.constant 0.000000e+00 : f32
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
      %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
      %2 = tensor.empty() : tensor<512x1024xf32>
      %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
      %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
      flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
      return
    }
  }
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
module {
  flow.executable private @matmul_static_dispatch_0 {
    flow.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !flow.dispatch.tensor<readonly:tensor<512x256xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %2 = tensor.empty() : tensor<512x1024xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = flow.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
    %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyInput (iree-stream-verify-input) //----- //
module {
  flow.executable private @matmul_static_dispatch_0 {
    flow.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !flow.dispatch.tensor<readonly:tensor<512x256xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %2 = tensor.empty() : tensor<512x1024xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = flow.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
    %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After OutlineConstants (iree-stream-outline-constants) //----- //
module {
  flow.executable private @matmul_static_dispatch_0 {
    flow.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !flow.dispatch.tensor<readonly:tensor<512x256xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %2 = tensor.empty() : tensor<512x1024xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = flow.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
    %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = flow.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = flow.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
  %2 = flow.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
  %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
module {
  flow.executable private @matmul_static_dispatch_0 {
    flow.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !flow.dispatch.tensor<readonly:tensor<512x256xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %2 = tensor.empty() : tensor<512x1024xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = flow.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
    %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
module {
  flow.executable private @matmul_static_dispatch_0 {
    flow.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !flow.dispatch.tensor<readonly:tensor<512x256xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %2 = tensor.empty() : tensor<512x1024xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = flow.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
    %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
module {
  flow.executable private @matmul_static_dispatch_0 {
    flow.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !flow.dispatch.tensor<readonly:tensor<512x256xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %2 = tensor.empty() : tensor<512x1024xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = flow.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
    %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
module {
  flow.executable private @matmul_static_dispatch_0 {
    flow.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !flow.dispatch.tensor<readonly:tensor<512x256xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %2 = tensor.empty() : tensor<512x1024xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %4 = linalg.matmul ins(%0, %1 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%3 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %4, %arg2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32>
    %2 = flow.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0, %1) : (tensor<512x256xf32>, tensor<256x1024xf32>) -> tensor<512x1024xf32>
    %3 = hal.tensor.export %2 : tensor<512x1024xf32> -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After ConvertToStream (iree-stream-conversion) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %cst = arith.constant 0.000000e+00 : f32
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512_0 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512_0, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<512x256xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %c553648160_i32_1 = arith.constant 553648160 : i32
    %c1_i32_2 = arith.constant 1 : i32
    %c256_3 = arith.constant 256 : index
    %c1024_4 = arith.constant 1024 : index
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256_3, %c1024_4]) type(%c553648160_i32_1) encoding(%c1_i32_2)
    %3 = stream.tensor.sizeof tensor<256x1024xf32> : index
    %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
    %c0 = arith.constant 0 : index
    %6 = stream.tensor.sizeof tensor<512x1024xf32> : index
    %7 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%2[%c0 to %0 for %0], %5[%c0 to %3 for %3]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}) -> !stream.resource<*>{%6}
    %8 = stream.async.transfer %7 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
    %9 = stream.tensor.export %8 : tensor<512x1024xf32> in !stream.resource<external>{%6} -> !hal.buffer_view
    return %9 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToTensors (iree-stream-verify-lowering-to-tensors) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %cst = arith.constant 0.000000e+00 : f32
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512_0 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512_0, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<512x256xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %c553648160_i32_1 = arith.constant 553648160 : i32
    %c1_i32_2 = arith.constant 1 : i32
    %c256_3 = arith.constant 256 : index
    %c1024_4 = arith.constant 1024 : index
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256_3, %c1024_4]) type(%c553648160_i32_1) encoding(%c1_i32_2)
    %3 = stream.tensor.sizeof tensor<256x1024xf32> : index
    %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
    %c0 = arith.constant 0 : index
    %6 = stream.tensor.sizeof tensor<512x1024xf32> : index
    %7 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%2[%c0 to %0 for %0], %5[%c0 to %3 for %3]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}) -> !stream.resource<*>{%6}
    %8 = stream.async.transfer %7 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
    %9 = stream.tensor.export %8 : tensor<512x1024xf32> in !stream.resource<external>{%6} -> !hal.buffer_view
    return %9 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.sizeof tensor<512x256xf32> : index
  %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %3 = stream.tensor.sizeof tensor<256x1024xf32> : index
  %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%3}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
  %6 = stream.tensor.sizeof tensor<512x1024xf32> : index
  %7 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%2[%c0 to %0 for %0], %5[%c0 to %3 for %3]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}) -> !stream.resource<*>{%6}
  %8 = stream.async.transfer %7 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
  %9 = stream.tensor.export %8 : tensor<512x1024xf32> in !stream.resource<external>{%6} -> !hal.buffer_view
  return %9 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.sizeof tensor<512x256xf32> : index
  %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %3 = stream.tensor.sizeof tensor<256x1024xf32> : index
  %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%3}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
  %6 = stream.tensor.sizeof tensor<512x1024xf32> : index
  %7 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%2[%c0 to %0 for %0], %5[%c0 to %3 for %3]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}) -> !stream.resource<*>{%6}
  %8 = stream.async.transfer %7 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
  %9 = stream.tensor.export %8 : tensor<512x1024xf32> in !stream.resource<external>{%6} -> !hal.buffer_view
  return %9 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.sizeof tensor<512x256xf32> : index
  %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %3 = stream.tensor.sizeof tensor<256x1024xf32> : index
  %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%3}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
  %6 = stream.tensor.sizeof tensor<512x1024xf32> : index
  %7 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%2[%c0 to %0 for %0], %5[%c0 to %3 for %3]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}) -> !stream.resource<*>{%6}
  %8 = stream.async.transfer %7 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
  %9 = stream.tensor.export %8 : tensor<512x1024xf32> in !stream.resource<external>{%6} -> !hal.buffer_view
  return %9 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<512x256xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %3 = stream.tensor.sizeof tensor<256x1024xf32> : index
    %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
    %6 = stream.tensor.sizeof tensor<512x1024xf32> : index
    %7 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%2[%c0 to %0 for %0], %5[%c0 to %3 for %3]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}) -> !stream.resource<*>{%6}
    %8 = stream.async.transfer %7 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
    %9 = stream.tensor.export %8 : tensor<512x1024xf32> in !stream.resource<external>{%6} -> !hal.buffer_view
    return %9 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<512x256xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %3 = stream.tensor.sizeof tensor<256x1024xf32> : index
    %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
    %6 = stream.tensor.sizeof tensor<512x1024xf32> : index
    %7 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%2[%c0 to %0 for %0], %5[%c0 to %3 for %3]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}) -> !stream.resource<*>{%6}
    %8 = stream.async.transfer %7 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
    %9 = stream.tensor.export %8 : tensor<512x1024xf32> in !stream.resource<external>{%6} -> !hal.buffer_view
    return %9 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<512x256xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %3 = stream.tensor.sizeof tensor<256x1024xf32> : index
    %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
    %6 = stream.tensor.sizeof tensor<512x1024xf32> : index
    %7 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%2[%c0 to %0 for %0], %5[%c0 to %3 for %3]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}) -> !stream.resource<*>{%6}
    %8 = stream.async.transfer %7 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
    %9 = stream.tensor.export %8 : tensor<512x1024xf32> in !stream.resource<external>{%6} -> !hal.buffer_view
    return %9 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<512x256xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %3 = stream.tensor.sizeof tensor<256x1024xf32> : index
    %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
    %6 = stream.tensor.sizeof tensor<512x1024xf32> : index
    %7 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%2[%c0 to %0 for %0], %5[%c0 to %3 for %3]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}) -> !stream.resource<*>{%6}
    %8 = stream.async.transfer %7 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
    %9 = stream.tensor.export %8 : tensor<512x1024xf32> in !stream.resource<external>{%6} -> !hal.buffer_view
    return %9 : !hal.buffer_view
  }
}


// -----// IR Dump After CombineInitializers (iree-util-combine-initializers) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<512x256xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %3 = stream.tensor.sizeof tensor<256x1024xf32> : index
    %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
    %6 = stream.tensor.sizeof tensor<512x1024xf32> : index
    %7 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%2[%c0 to %0 for %0], %5[%c0 to %3 for %3]) : (!stream.resource<*>{%0}, !stream.resource<*>{%3}) -> !stream.resource<*>{%6}
    %8 = stream.async.transfer %7 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
    %9 = stream.tensor.export %8 : tensor<512x1024xf32> in !stream.resource<external>{%6} -> !hal.buffer_view
    return %9 : !hal.buffer_view
  }
}


// -----// IR Dump After EncodeHostTensors (iree-stream-encode-host-tensors) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c1048576} -> !stream.resource<*>{%c1048576}
  %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c1048576 for %c1048576]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c1048576}) -> !stream.resource<*>{%c2097152}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c2097152} -> !stream.resource<external>{%c2097152}
  %6 = stream.tensor.export %5 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After EncodeDeviceTensors (iree-stream-encode-device-tensors) //----- //
stream.executable private @matmul_static_dispatch_0 {
  stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
    stream.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
      %cst = arith.constant 0.000000e+00 : f32
      %c0 = arith.constant 0 : index
      %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
      %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
      %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
      %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
      %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
      %5 = tensor.empty() : tensor<512x1024xf32>
      %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
      %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
      flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
      return
    }
  }
}

// -----// IR Dump After MaterializeBuiltins (iree-stream-materialize-builtins) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c1048576} -> !stream.resource<*>{%c1048576}
    %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c1048576 for %c1048576]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c1048576}) -> !stream.resource<*>{%c2097152}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c2097152} -> !stream.resource<external>{%c2097152}
    %6 = stream.tensor.export %5 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c1048576} -> !stream.resource<*>{%c1048576}
  %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c1048576 for %c1048576]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c1048576}) -> !stream.resource<*>{%c2097152}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c2097152} -> !stream.resource<external>{%c2097152}
  %6 = stream.tensor.export %5 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c1048576} -> !stream.resource<*>{%c1048576}
  %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c1048576 for %c1048576]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c1048576}) -> !stream.resource<*>{%c2097152}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c2097152} -> !stream.resource<external>{%c2097152}
  %6 = stream.tensor.export %5 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c1048576} -> !stream.resource<*>{%c1048576}
  %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c1048576 for %c1048576]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c1048576}) -> !stream.resource<*>{%c2097152}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c2097152} -> !stream.resource<external>{%c2097152}
  %6 = stream.tensor.export %5 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c1048576} -> !stream.resource<*>{%c1048576}
    %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c1048576 for %c1048576]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c1048576}) -> !stream.resource<*>{%c2097152}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c2097152} -> !stream.resource<external>{%c2097152}
    %6 = stream.tensor.export %5 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c1048576} -> !stream.resource<*>{%c1048576}
    %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c1048576 for %c1048576]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c1048576}) -> !stream.resource<*>{%c2097152}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c2097152} -> !stream.resource<external>{%c2097152}
    %6 = stream.tensor.export %5 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c1048576} -> !stream.resource<*>{%c1048576}
    %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c1048576 for %c1048576]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c1048576}) -> !stream.resource<*>{%c2097152}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c2097152} -> !stream.resource<external>{%c2097152}
    %6 = stream.tensor.export %5 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c1048576} -> !stream.resource<*>{%c1048576}
    %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c1048576 for %c1048576]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c1048576}) -> !stream.resource<*>{%c2097152}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c2097152} -> !stream.resource<external>{%c2097152}
    %6 = stream.tensor.export %5 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeCopyOnWrite (iree-stream-materialize-copy-on-write) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c1048576} -> !stream.resource<*>{%c1048576}
  %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c1048576 for %c1048576]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c1048576}) -> !stream.resource<*>{%c2097152}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c2097152} -> !stream.resource<external>{%c2097152}
  %6 = stream.tensor.export %5 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After ElideAsyncCopies (iree-stream-elide-async-copies) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c1048576} -> !stream.resource<*>{%c1048576}
    %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c1048576 for %c1048576]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c1048576}) -> !stream.resource<*>{%c2097152}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c2097152} -> !stream.resource<external>{%c2097152}
    %6 = stream.tensor.export %5 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c1048576} -> !stream.resource<*>{%c1048576}
  %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c1048576 for %c1048576]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c1048576}) -> !stream.resource<*>{%c2097152}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c2097152} -> !stream.resource<external>{%c2097152}
  %6 = stream.tensor.export %5 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After EmplaceAllocations (iree-stream-emplace-allocations) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c1048576} -> !stream.resource<*>{%c1048576}
  %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c1048576 for %c1048576]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c1048576}) -> !stream.resource<*>{%c2097152}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c2097152} -> !stream.resource<external>{%c2097152}
  %6 = stream.tensor.export %5 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After RefineUsage (iree-stream-refine-usage) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0[%c0 to %c524288 for %c524288], %1[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
    %3 = stream.tensor.export %2 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %2 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0[%c0 to %c524288 for %c524288], %1[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
  %3 = stream.tensor.export %2 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %2 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0[%c0 to %c524288 for %c524288], %1[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
  %3 = stream.tensor.export %2 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %2 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0[%c0 to %c524288 for %c524288], %1[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
  %3 = stream.tensor.export %2 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0[%c0 to %c524288 for %c524288], %1[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
    %3 = stream.tensor.export %2 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0[%c0 to %c524288 for %c524288], %1[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
    %3 = stream.tensor.export %2 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0[%c0 to %c524288 for %c524288], %1[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
    %3 = stream.tensor.export %2 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%0[%c0 to %c524288 for %c524288], %1[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
    %3 = stream.tensor.export %2 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After ScheduleExecution (iree-stream-schedule-execution) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152} {
    %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
    stream.yield %4 : !stream.resource<external>{%c2097152}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2097152}
  %3 = stream.tensor.export %2 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After ScheduleConcurrency (iree-stream-schedule-concurrency) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152} {
    %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
    stream.yield %4 : !stream.resource<external>{%c2097152}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2097152}
  %3 = stream.tensor.export %2 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After PropagateTimepoints (iree-stream-propagate-timepoints) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.timepoint.immediate => !stream.timepoint
    %3 = stream.timepoint.immediate => !stream.timepoint
    %4 = stream.timepoint.immediate => !stream.timepoint
    %results, %result_timepoint = stream.async.execute await(%4) => with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152} {
      %7 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
      stream.yield %7 : !stream.resource<external>{%c2097152}
    } => !stream.timepoint
    %5 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2097152}
    %6 = stream.tensor.export %5 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152} {
    %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
    stream.yield %4 : !stream.resource<external>{%c2097152}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2097152}
  %3 = stream.tensor.export %2 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152} {
    %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
    stream.yield %4 : !stream.resource<external>{%c2097152}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2097152}
  %3 = stream.tensor.export %2 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152} {
    %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
    stream.yield %4 : !stream.resource<external>{%c2097152}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2097152}
  %3 = stream.tensor.export %2 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %3 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152} {
      %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
      stream.yield %4 : !stream.resource<external>{%c2097152}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2097152}
    %3 = stream.tensor.export %2 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152} {
      %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
      stream.yield %4 : !stream.resource<external>{%c2097152}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2097152}
    %3 = stream.tensor.export %2 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152} {
      %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
      stream.yield %4 : !stream.resource<external>{%c2097152}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2097152}
    %3 = stream.tensor.export %2 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152} {
      %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
      stream.yield %4 : !stream.resource<external>{%c2097152}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2097152}
    %3 = stream.tensor.export %2 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToAsync (iree-stream-verify-lowering-to-async) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152} {
      %4 = stream.async.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c1048576 for %c1048576]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c1048576}) -> !stream.resource<external>{%c2097152}
      stream.yield %4 : !stream.resource<external>{%c2097152}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2097152}
    %3 = stream.tensor.export %2 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After ScheduleAllocation (iree-stream-schedule-allocation) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %c0_0 = arith.constant 0 : index
  %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
  %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
    stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
      wo %arg5[%c0_0 for %c2097152] : !stream.resource<external>{%c2097152}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
  %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After PackConstants (iree-stream-pack-constants) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %c0_0 = arith.constant 0 : index
  %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
  %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
    stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
      wo %arg5[%c0_0 for %c2097152] : !stream.resource<external>{%c2097152}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
  %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After PackAllocations (iree-stream-pack-allocations) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %c0_0 = arith.constant 0 : index
  %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
  %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
    stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
      wo %arg5[%c0_0 for %c2097152] : !stream.resource<external>{%c2097152}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
  %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After LayoutSlices (iree-stream-layout-slices) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %c0_0 = arith.constant 0 : index
  %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
  %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
    stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
      wo %arg5[%c0_0 for %c2097152] : !stream.resource<external>{%c2097152}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
  %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After PropagateSubranges (iree-util-propagate-subranges) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %c0_0 = arith.constant 0 : index
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0_0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
  %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
    stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
      wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
  %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
  %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
    stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
      wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
  %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
  %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
    stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
      wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
  %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToCmd (iree-stream-verify-lowering-to-cmd) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
  %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
    stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
      wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
  %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
  %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
    stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
      wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
  %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
  %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
    stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
      wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
  %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
  %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
    stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
      wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
  %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
  %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
    stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
      wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
  %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
  %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
    stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
      wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
  %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
  %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
    stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
      wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
  %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
module attributes {iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
module attributes {iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
module attributes {iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
module attributes {iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After ElideTimepoints (iree-stream-elide-timepoints) //----- //
module attributes {iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseDispatchBindings (iree-stream-fuse-dispatch-bindings) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: index, %arg4: index, %arg5: index) {
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = arith.addi %c0, %arg3 : index
        %1 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %2 = arith.addi %c0, %arg4 : index
        %3 = stream.binding.subspan %arg1[%2] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %4 = arith.addi %c0, %arg5 : index
        %5 = stream.binding.subspan %arg2[%4] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %6 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %7 = flow.dispatch.tensor.load %3, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %8 = tensor.empty() : tensor<512x1024xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%8 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %10 = linalg.matmul ins(%6, %7 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%9 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %10, %5, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %c0_0 = arith.constant 0 : index
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%c0, %c0, %c0 : index, index, index) {
        ro %arg3[%c0_0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0_0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0_0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After PackDispatchOperands (iree-stream-pack-dispatch-operands) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: i32, %arg4: i32, %arg5: i32) {
        %0 = arith.index_castui %arg3 : i32 to index
        %1 = arith.index_castui %arg4 : i32 to index
        %2 = arith.index_castui %arg5 : i32 to index
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %3 = arith.addi %c0, %0 : index
        %4 = stream.binding.subspan %arg0[%3] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %5 = arith.addi %c0, %1 : index
        %6 = stream.binding.subspan %arg1[%5] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %7 = arith.addi %c0, %2 : index
        %8 = stream.binding.subspan %arg2[%7] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %9 = flow.dispatch.tensor.load %4, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %10 = flow.dispatch.tensor.load %6, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %11 = tensor.empty() : tensor<512x1024xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%11 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %13 = linalg.matmul ins(%9, %10 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%12 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %13, %8, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c0 = arith.constant 0 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %c0_0 = arith.constant 0 : index
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c0_i32_2 = arith.constant 0 : i32
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%c0_i32, %c0_i32_1, %c0_i32_2 : i32, i32, i32) {
        ro %arg3[%c0_0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0_0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0_0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: i32, %arg4: i32, %arg5: i32) {
        %0 = arith.index_castui %arg3 : i32 to index
        %1 = arith.index_castui %arg4 : i32 to index
        %2 = arith.index_castui %arg5 : i32 to index
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %3 = arith.addi %c0, %0 : index
        %4 = stream.binding.subspan %arg0[%3] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %5 = arith.addi %c0, %1 : index
        %6 = stream.binding.subspan %arg1[%5] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %7 = arith.addi %c0, %2 : index
        %8 = stream.binding.subspan %arg2[%7] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %9 = flow.dispatch.tensor.load %4, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %10 = flow.dispatch.tensor.load %6, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %11 = tensor.empty() : tensor<512x1024xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%11 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %13 = linalg.matmul ins(%9, %10 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%12 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %13, %8, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %c0 = arith.constant 0 : index
    %c0_i32 = arith.constant 0 : i32
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1](%c0_i32, %c0_i32, %c0_i32 : i32, i32, i32) {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldUniformOperands (iree-stream-fold-uniform-operands) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %0 = arith.index_castui %c0_i32 : i32 to index
        %1 = arith.index_castui %c0_i32 : i32 to index
        %2 = arith.index_castui %c0_i32 : i32 to index
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %3 = arith.addi %c0, %0 : index
        %4 = stream.binding.subspan %arg0[%3] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %5 = arith.addi %c0, %1 : index
        %6 = stream.binding.subspan %arg1[%5] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %7 = arith.addi %c0, %2 : index
        %8 = stream.binding.subspan %arg2[%7] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %9 = flow.dispatch.tensor.load %4, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %10 = flow.dispatch.tensor.load %6, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %11 = tensor.empty() : tensor<512x1024xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%11 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %13 = linalg.matmul ins(%9, %10 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%12 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %13, %8, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %c0 = arith.constant 0 : index
    %c0_i32 = arith.constant 0 : i32
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After AnnotateDispatchArguments (iree-stream-annotate-dispatch-arguments) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %0 = arith.index_castui %c0_i32 : i32 to index
        %1 = arith.index_castui %c0_i32 : i32 to index
        %2 = arith.index_castui %c0_i32 : i32 to index
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %3 = arith.addi %c0, %0 : index
        %4 = stream.binding.subspan %arg0[%3] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %5 = arith.addi %c0, %1 : index
        %6 = stream.binding.subspan %arg1[%5] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %7 = arith.addi %c0, %2 : index
        %8 = stream.binding.subspan %arg2[%7] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %9 = flow.dispatch.tensor.load %4, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %10 = flow.dispatch.tensor.load %6, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %11 = tensor.empty() : tensor<512x1024xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%11 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %13 = linalg.matmul ins(%9, %10 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%12 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %13, %8, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %c0 = arith.constant 0 : index
    %c0_i32 = arith.constant 0 : i32
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After MemoizeChannels (iree-stream-memoize-channels) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %0 = arith.index_castui %c0_i32 : i32 to index
        %1 = arith.index_castui %c0_i32 : i32 to index
        %2 = arith.index_castui %c0_i32 : i32 to index
        %cst = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %3 = arith.addi %c0, %0 : index
        %4 = stream.binding.subspan %arg0[%3] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %5 = arith.addi %c0, %1 : index
        %6 = stream.binding.subspan %arg1[%5] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %7 = arith.addi %c0, %2 : index
        %8 = stream.binding.subspan %arg2[%7] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %9 = flow.dispatch.tensor.load %4, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %10 = flow.dispatch.tensor.load %6, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %11 = tensor.empty() : tensor<512x1024xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%11 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %13 = linalg.matmul ins(%9, %10 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%12 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %13, %8, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %c0 = arith.constant 0 : index
    %c0_i32 = arith.constant 0 : i32
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
  %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
    stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
      wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
  %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
  %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
    stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
      wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
  %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
  %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
    stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
      wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
  %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
  %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
  %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
    stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
      wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
    }
  } => !stream.timepoint
  %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
  %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
module {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::AssignTargetDevicesPass (iree-hal-assign-target-devices) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::VerifyTargetEnvironmentPass (iree-hal-verify-target-environment) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  stream.executable private @matmul_static_dispatch_0 {
    stream.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 workgroups(%arg0: index, %arg1: index, %arg2: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0, %arg1, %arg2
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0.000000e+00 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
        %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
        %5 = tensor.empty() : tensor<512x1024xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
        flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        return
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      }
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::(anonymous namespace)::MaterializeInterfacesPass (iree-hal-materialize-interfaces) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg1, %arg2, %arg3
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
          %c0 = arith.constant 0 : index
          %cst = arith.constant 0.000000e+00 : f32
          %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
          %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
          %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
          %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
          %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
          %5 = tensor.empty() : tensor<512x1024xf32>
          %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
          %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
          flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
          return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<256x1024xf32> in !stream.resource<external>{%c1048576}
    %2 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2097152}
    %3 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c1048576}, %2 as %arg5: !stream.resource<external>{%c2097152}) {
      stream.cmd.dispatch @matmul_static_dispatch_0::@matmul_static_dispatch_0_matmul_512x1024x256[%c512, %c1024, %c1] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c1048576] : !stream.resource<external>{%c1048576},
        wo %arg5[%c0 for %c2097152] : !stream.resource<external>{%c2097152}
      } attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>, #hal.interface.binding<0, 2>]}
    } => !stream.timepoint
    %4 = stream.timepoint.await %3 => %2 : !stream.resource<external>{%c2097152}
    %5 = stream.tensor.export %4 : tensor<512x1024xf32> in !stream.resource<external>{%c2097152} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After TypePropagation (iree-codegen-type-propagation) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
  %5 = tensor.empty() : tensor<512x1024xf32>
  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  return
}

// -----// IR Dump After BufferizeCopyOnlyDispatches (iree-codegen-bufferize-copy-only-dispatches) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
    %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
    %5 = tensor.empty() : tensor<512x1024xf32>
    %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
    flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    return
  }
}

// -----// IR Dump After EraseHALDescriptorTypeFromMemRef (iree-codegen-erase-hal-descriptor-type-from-memref) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
  %5 = tensor.empty() : tensor<512x1024xf32>
  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %7 = linalg.matmul ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  flow.dispatch.tensor.store %7, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  return
}

Yufan:: [TileAndDistributeToWorkgroupsPass] 
Yufan:: [TileAndDistributeToWorkgroupsPass] tiledLoops.empty() 1
Yufan:: Again [TileAndDistributeToWorkgroupsPass] tiledLoops.empty() 0
ts 3
32
128
0
Yufan:: tiled Loop scf.for
Yufan:: tiled Loop info scf.for
Yufan:: tiled size 32
Yufan:: tiled Loop scf.for
Yufan:: tiled Loop info scf.for
Yufan:: tiled size 128
// -----// IR Dump After TileAndDistributeToWorkgroups (iree-codegen-tile-and-distribute-to-workgroups) //----- //
hal.executable.variant public @cuda_nvptx_fb, target = <"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}> {
  hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<LLVMGPUMatmulSimt>, workgroup_size = [32 : index, 8 : index, 1 : index]} {
  ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
    %c8 = arith.constant 8 : index
    %c16 = arith.constant 16 : index
    %c1 = arith.constant 1 : index
    hal.return %c8, %c16, %c1 : index, index, index
  }
  builtin.module {
    func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
      %c512 = arith.constant 512 : index
      %c1024 = arith.constant 1024 : index
      %c0 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f32
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
      %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
      %workgroup_id_x = hal.interface.workgroup.id[0] : index
      %workgroup_count_x = hal.interface.workgroup.count[0] : index
      %workgroup_id_y = hal.interface.workgroup.id[1] : index
      %workgroup_count_y = hal.interface.workgroup.count[1] : index
      %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
      %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_count_y]
      scf.for %arg0 = %3 to %c512 step %4 {
        %5 = affine.min affine_map<(d0) -> (32, -d0 + 512)>(%arg0)
        %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
        %7 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
        scf.for %arg1 = %6 to %c1024 step %7 {
          %8 = affine.min affine_map<(d0) -> (128, -d0 + 1024)>(%arg1)
          %9 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%5, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
          %10 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %8], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
          %11 = tensor.empty(%5, %8) : tensor<?x?xf32>
          %12 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%11 : tensor<?x?xf32>) -> tensor<?x?xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%9, %10 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%12 : tensor<?x?xf32>) -> tensor<?x?xf32>
          flow.dispatch.tensor.store %13, %2, offsets = [%arg0, %arg1], sizes = [%5, %8], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        }
      }
      return
    }
  }
}

// -----// IR Dump After ConvertToDestinationPassingStyle (iree-codegen-convert-to-destination-passing-style) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.min affine_map<(d0) -> (32, -d0 + 512)>(%arg0)
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %7 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %6 to %c1024 step %7 {
      %8 = affine.min affine_map<(d0) -> (32, -d0 + 512)>(%arg0)
      %9 = affine.min affine_map<(d0) -> (128, -d0 + 1024)>(%arg1)
      %10 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [%8, %9], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<?x?xf32>
      %11 = affine.min affine_map<(d0) -> (128, -d0 + 1024)>(%arg1)
      %12 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%5, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
      %13 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %11], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
      %14 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%10 : tensor<?x?xf32>) -> tensor<?x?xf32>
      %15 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%12, %13 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%14 : tensor<?x?xf32>) -> tensor<?x?xf32>
      flow.dispatch.tensor.store %15, %2, offsets = [%arg0, %arg1], sizes = [%5, %11], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_count_y]
    scf.for %arg0 = %3 to %c512 step %4 {
      %5 = affine.min affine_map<(d0) -> (-d0 + 512, 32)>(%arg0)
      %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
      %7 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
      scf.for %arg1 = %6 to %c1024 step %7 {
        %8 = affine.min affine_map<(d0) -> (-d0 + 512, 32)>(%arg0)
        %9 = affine.min affine_map<(d0) -> (-d0 + 1024, 128)>(%arg1)
        %10 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [%8, %9], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<?x?xf32>
        %11 = affine.min affine_map<(d0) -> (-d0 + 1024, 128)>(%arg1)
        %12 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%5, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
        %13 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %11], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
        %14 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%10 : tensor<?x?xf32>) -> tensor<?x?xf32>
        %15 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%12, %13 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%14 : tensor<?x?xf32>) -> tensor<?x?xf32>
        flow.dispatch.tensor.store %15, %2, offsets = [%arg0, %arg1], sizes = [%5, %11], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_count_y]
    scf.for %arg0 = %3 to %c512 step %4 {
      %5 = affine.min affine_map<(d0) -> (-d0 + 512, 32)>(%arg0)
      %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
      %7 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
      scf.for %arg1 = %6 to %c1024 step %7 {
        %8 = affine.min affine_map<(d0) -> (-d0 + 1024, 128)>(%arg1)
        %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [%5, %8], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<?x?xf32>
        %10 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%5, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
        %11 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %8], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
        %12 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%9 : tensor<?x?xf32>) -> tensor<?x?xf32>
        %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%10, %11 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%12 : tensor<?x?xf32>) -> tensor<?x?xf32>
        flow.dispatch.tensor.store %13, %2, offsets = [%arg0, %arg1], sizes = [%5, %8], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After WorkgroupSpecialization (iree-codegen-workgroup-specialization) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.min affine_map<(d0) -> (-d0 + 512, 32)>(%arg0)
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %7 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %6 to %c1024 step %7 {
      %8 = affine.min affine_map<(d0) -> (-d0 + 1024, 128)>(%arg1)
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [%5, %8], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<?x?xf32>
      %10 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%5, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
      %11 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %8], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
      %12 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%9 : tensor<?x?xf32>) -> tensor<?x?xf32>
      %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%10, %11 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%12 : tensor<?x?xf32>) -> tensor<?x?xf32>
      flow.dispatch.tensor.store %13, %2, offsets = [%arg0, %arg1], sizes = [%5, %8], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_count_y]
    scf.for %arg0 = %3 to %c512 step %4 {
      %5 = affine.min affine_map<(d0) -> (-d0 + 512, 32)>(%arg0)
      %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
      %7 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
      scf.for %arg1 = %6 to %c1024 step %7 {
        %8 = affine.min affine_map<(d0) -> (-d0 + 1024, 128)>(%arg1)
        %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [%5, %8], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<?x?xf32>
        %10 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%5, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
        %11 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %8], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
        %12 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%9 : tensor<?x?xf32>) -> tensor<?x?xf32>
        %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%10, %11 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%12 : tensor<?x?xf32>) -> tensor<?x?xf32>
        flow.dispatch.tensor.store %13, %2, offsets = [%arg0, %arg1], sizes = [%5, %8], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_count_y]
    scf.for %arg0 = %3 to %c512 step %4 {
      %5 = affine.min affine_map<(d0) -> (-d0 + 512, 32)>(%arg0)
      %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
      %7 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
      scf.for %arg1 = %6 to %c1024 step %7 {
        %8 = affine.min affine_map<(d0) -> (-d0 + 1024, 128)>(%arg1)
        %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [%5, %8], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<?x?xf32>
        %10 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%5, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
        %11 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %8], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
        %12 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%9 : tensor<?x?xf32>) -> tensor<?x?xf32>
        %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%10, %11 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%12 : tensor<?x?xf32>) -> tensor<?x?xf32>
        flow.dispatch.tensor.store %13, %2, offsets = [%arg0, %arg1], sizes = [%5, %8], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After RemoveSingleIterationLoop (iree-codegen-remove-single-iteration-loop) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<(d0) -> (-d0 + 512, 32)>(%3)
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<(d0) -> (-d0 + 1024, 128)>(%5)
  %7 = flow.dispatch.tensor.load %2, offsets = [%3, %5], sizes = [%4, %6], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<?x?xf32>
  %8 = flow.dispatch.tensor.load %0, offsets = [%3, 0], sizes = [%4, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
  %9 = flow.dispatch.tensor.load %1, offsets = [0, %5], sizes = [256, %6], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
  %10 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%7 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %11 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%8, %9 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%10 : tensor<?x?xf32>) -> tensor<?x?xf32>
  flow.dispatch.tensor.store %11, %2, offsets = [%3, %5], sizes = [%4, %6], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  return
}

// -----// IR Dump After LLVMGPUTensorAlloc (iree-llvmgpu-alloc) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %7 = flow.dispatch.tensor.load %2, offsets = [%3, %5], sizes = [%4, %6], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<?x?xf32>
  %8 = flow.dispatch.tensor.load %0, offsets = [%3, 0], sizes = [%4, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
  %9 = flow.dispatch.tensor.load %1, offsets = [0, %5], sizes = [256, %6], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
  %10 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%7 : tensor<?x?xf32>) -> tensor<?x?xf32>
  %11 = scf.for %arg0 = %c0 to %c256 step %c32 iter_args(%arg1 = %10) -> (tensor<?x?xf32>) {
    %dim = tensor.dim %8, %c0 : tensor<?x256xf32>
    %dim_0 = tensor.dim %9, %c1 : tensor<256x?xf32>
    %dim_1 = tensor.dim %10, %c0 : tensor<?x?xf32>
    %dim_2 = tensor.dim %10, %c1 : tensor<?x?xf32>
    %extracted_slice = tensor.extract_slice %8[0, %arg0] [%dim, 32] [1, 1] : tensor<?x256xf32> to tensor<?x32xf32>
    %extracted_slice_3 = tensor.extract_slice %9[%arg0, 0] [32, %dim_0] [1, 1] : tensor<256x?xf32> to tensor<32x?xf32>
    %extracted_slice_4 = tensor.extract_slice %arg1[0, 0] [%dim_1, %dim_2] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
    %12 = linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%extracted_slice, %extracted_slice_3 : tensor<?x32xf32>, tensor<32x?xf32>) outs(%extracted_slice_4 : tensor<?x?xf32>) -> tensor<?x?xf32>
    %inserted_slice = tensor.insert_slice %12 into %arg1[0, 0] [%dim_1, %dim_2] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
    scf.yield %inserted_slice : tensor<?x?xf32>
  }
  flow.dispatch.tensor.store %11, %2, offsets = [%3, %5], sizes = [%4, %6], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  return
}

// -----// IR Dump After LLVMGPUTileTensor (iree-llvmgpu-tile-tensor) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %7 = flow.dispatch.tensor.load %2, offsets = [%3, %5], sizes = [%4, %6], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<?x?xf32>
  %8 = flow.dispatch.tensor.load %0, offsets = [%3, 0], sizes = [%4, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
  %9 = flow.dispatch.tensor.load %1, offsets = [0, %5], sizes = [256, %6], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
  %dim = tensor.dim %7, %c0 : tensor<?x?xf32>
  %dim_0 = tensor.dim %7, %c1 : tensor<?x?xf32>
  %10 = scf.foreach_thread (%arg0, %arg1) in (%c8, %c32) shared_outs(%arg2 = %7) -> (tensor<?x?xf32>) {
    %12 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg0)[%dim]
    %13 = affine.max affine_map<(d0) -> (0, d0)>(%12)
    %14 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg1)[%dim_0]
    %15 = affine.max affine_map<(d0) -> (0, d0)>(%14)
    %16 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg0)[%dim]
    %17 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg1)[%dim_0]
    %extracted_slice = tensor.extract_slice %arg2[%16, %17] [%13, %15] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
    %18 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%extracted_slice : tensor<?x?xf32>) -> tensor<?x?xf32>
    %19 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg0)[%dim]
    %20 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg1)[%dim_0]
    scf.foreach_thread.perform_concurrently {
      tensor.parallel_insert_slice %18 into %arg2[%19, %20] [%13, %15] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
    }
  } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
  %11 = scf.for %arg0 = %c0 to %c256 step %c32 iter_args(%arg1 = %10) -> (tensor<?x?xf32>) {
    %dim_1 = tensor.dim %8, %c0 : tensor<?x256xf32>
    %dim_2 = tensor.dim %9, %c1 : tensor<256x?xf32>
    %dim_3 = tensor.dim %10, %c0 : tensor<?x?xf32>
    %dim_4 = tensor.dim %10, %c1 : tensor<?x?xf32>
    %extracted_slice = tensor.extract_slice %8[0, %arg0] [%dim_1, 32] [1, 1] : tensor<?x256xf32> to tensor<?x32xf32>
    %extracted_slice_5 = tensor.extract_slice %9[%arg0, 0] [32, %dim_2] [1, 1] : tensor<256x?xf32> to tensor<32x?xf32>
    %extracted_slice_6 = tensor.extract_slice %arg1[0, 0] [%dim_3, %dim_4] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
    %12 = scf.foreach_thread (%arg2, %arg3) in (%c8, %c32) shared_outs(%arg4 = %extracted_slice_6) -> (tensor<?x?xf32>) {
      %13 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg2)[%dim_1]
      %14 = affine.max affine_map<(d0) -> (0, d0)>(%13)
      %15 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg3)[%dim_2]
      %16 = affine.max affine_map<(d0) -> (0, d0)>(%15)
      %17 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg2)[%dim_1]
      %18 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg3)[%dim_2]
      %19 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg2)[%dim_1]
      %20 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg3)[%dim_2]
      %extracted_slice_7 = tensor.extract_slice %extracted_slice[%17, 0] [%14, 32] [1, 1] : tensor<?x32xf32> to tensor<?x32xf32>
      %extracted_slice_8 = tensor.extract_slice %extracted_slice_5[0, %18] [32, %16] [1, 1] : tensor<32x?xf32> to tensor<32x?xf32>
      %extracted_slice_9 = tensor.extract_slice %arg4[%19, %20] [%14, %16] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
      %extracted_slice_10 = tensor.extract_slice %extracted_slice_7[0, 0] [%14, 32] [1, 1] : tensor<?x32xf32> to tensor<?x32xf32>
      %extracted_slice_11 = tensor.extract_slice %extracted_slice_8[0, 0] [32, %16] [1, 1] : tensor<32x?xf32> to tensor<32x?xf32>
      %extracted_slice_12 = tensor.extract_slice %extracted_slice_9[0, 0] [%14, %16] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
      %21 = linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%extracted_slice_10, %extracted_slice_11 : tensor<?x32xf32>, tensor<32x?xf32>) outs(%extracted_slice_12 : tensor<?x?xf32>) -> tensor<?x?xf32>
      %inserted_slice_13 = tensor.insert_slice %21 into %extracted_slice_9[0, 0] [%14, %16] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
      %22 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg2)[%dim_1]
      %23 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg3)[%dim_2]
      scf.foreach_thread.perform_concurrently {
        tensor.parallel_insert_slice %inserted_slice_13 into %arg4[%22, %23] [%14, %16] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
      }
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
    %inserted_slice = tensor.insert_slice %12 into %arg1[0, 0] [%dim_3, %dim_4] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
    scf.yield %inserted_slice : tensor<?x?xf32>
  }
  flow.dispatch.tensor.store %11, %2, offsets = [%3, %5], sizes = [%4, %6], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  return
}

// -----// IR Dump After GPUVectorization (iree-codegen-gpu-vectorization) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %7 = flow.dispatch.tensor.load %2, offsets = [%3, %5], sizes = [%4, %6], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<?x?xf32>
  %8 = flow.dispatch.tensor.load %0, offsets = [%3, 0], sizes = [%4, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
  %9 = flow.dispatch.tensor.load %1, offsets = [0, %5], sizes = [256, %6], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
  %dim = tensor.dim %7, %c0 : tensor<?x?xf32>
  %dim_0 = tensor.dim %7, %c1 : tensor<?x?xf32>
  %10 = scf.foreach_thread (%arg0, %arg1) in (%c8, %c32) shared_outs(%arg2 = %7) -> (tensor<?x?xf32>) {
    %12 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg0)[%dim]
    %13 = affine.max affine_map<(d0) -> (0, d0)>(%12)
    %14 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg1)[%dim_0]
    %15 = affine.max affine_map<(d0) -> (0, d0)>(%14)
    %16 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg0)[%dim]
    %17 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg1)[%dim_0]
    %extracted_slice = tensor.extract_slice %arg2[%16, %17] [%13, %15] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
    %18 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%extracted_slice : tensor<?x?xf32>) -> tensor<?x?xf32>
    %19 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg0)[%dim]
    %20 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg1)[%dim_0]
    scf.foreach_thread.perform_concurrently {
      tensor.parallel_insert_slice %18 into %arg2[%19, %20] [%13, %15] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
    }
  } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
  %11 = scf.for %arg0 = %c0 to %c256 step %c32 iter_args(%arg1 = %10) -> (tensor<?x?xf32>) {
    %dim_1 = tensor.dim %8, %c0 : tensor<?x256xf32>
    %dim_2 = tensor.dim %9, %c1 : tensor<256x?xf32>
    %dim_3 = tensor.dim %10, %c0 : tensor<?x?xf32>
    %dim_4 = tensor.dim %10, %c1 : tensor<?x?xf32>
    %extracted_slice = tensor.extract_slice %8[0, %arg0] [%dim_1, 32] [1, 1] : tensor<?x256xf32> to tensor<?x32xf32>
    %extracted_slice_5 = tensor.extract_slice %9[%arg0, 0] [32, %dim_2] [1, 1] : tensor<256x?xf32> to tensor<32x?xf32>
    %extracted_slice_6 = tensor.extract_slice %arg1[0, 0] [%dim_3, %dim_4] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
    %12 = scf.foreach_thread (%arg2, %arg3) in (%c8, %c32) shared_outs(%arg4 = %extracted_slice_6) -> (tensor<?x?xf32>) {
      %13 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg2)[%dim_1]
      %14 = affine.max affine_map<(d0) -> (0, d0)>(%13)
      %15 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg3)[%dim_2]
      %16 = affine.max affine_map<(d0) -> (0, d0)>(%15)
      %17 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg2)[%dim_1]
      %18 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg3)[%dim_2]
      %19 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg2)[%dim_1]
      %20 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg3)[%dim_2]
      %extracted_slice_7 = tensor.extract_slice %extracted_slice[%17, 0] [%14, 32] [1, 1] : tensor<?x32xf32> to tensor<?x32xf32>
      %extracted_slice_8 = tensor.extract_slice %extracted_slice_5[0, %18] [32, %16] [1, 1] : tensor<32x?xf32> to tensor<32x?xf32>
      %extracted_slice_9 = tensor.extract_slice %arg4[%19, %20] [%14, %16] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
      %extracted_slice_10 = tensor.extract_slice %extracted_slice_7[0, 0] [%14, 32] [1, 1] : tensor<?x32xf32> to tensor<?x32xf32>
      %extracted_slice_11 = tensor.extract_slice %extracted_slice_8[0, 0] [32, %16] [1, 1] : tensor<32x?xf32> to tensor<32x?xf32>
      %extracted_slice_12 = tensor.extract_slice %extracted_slice_9[0, 0] [%14, %16] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
      %21 = linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%extracted_slice_10, %extracted_slice_11 : tensor<?x32xf32>, tensor<32x?xf32>) outs(%extracted_slice_12 : tensor<?x?xf32>) -> tensor<?x?xf32>
      %inserted_slice_13 = tensor.insert_slice %21 into %extracted_slice_9[0, 0] [%14, %16] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
      %22 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg2)[%dim_1]
      %23 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg3)[%dim_2]
      scf.foreach_thread.perform_concurrently {
        tensor.parallel_insert_slice %inserted_slice_13 into %arg4[%22, %23] [%14, %16] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
      }
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
    %inserted_slice = tensor.insert_slice %12 into %arg1[0, 0] [%dim_3, %dim_4] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
    scf.yield %inserted_slice : tensor<?x?xf32>
  }
  flow.dispatch.tensor.store %11, %2, offsets = [%3, %5], sizes = [%4, %6], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %7 = flow.dispatch.tensor.load %2, offsets = [%3, %5], sizes = [%4, %6], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<?x?xf32>
  %8 = flow.dispatch.tensor.load %0, offsets = [%3, 0], sizes = [%4, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
  %9 = flow.dispatch.tensor.load %1, offsets = [0, %5], sizes = [256, %6], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
  %10 = scf.foreach_thread (%arg0, %arg1) in (%c8, %c32) shared_outs(%arg2 = %7) -> (tensor<?x?xf32>) {
    %12 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg0)[%4]
    %13 = affine.max affine_map<(d0) -> (0, d0)>(%12)
    %14 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg1)[%6]
    %15 = affine.max affine_map<(d0) -> (0, d0)>(%14)
    %16 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg0)[%4]
    %17 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg1)[%6]
    %extracted_slice = tensor.extract_slice %arg2[%16, %17] [%13, %15] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
    %18 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%extracted_slice : tensor<?x?xf32>) -> tensor<?x?xf32>
    %19 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg0)[%4]
    %20 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg1)[%6]
    scf.foreach_thread.perform_concurrently {
      tensor.parallel_insert_slice %18 into %arg2[%19, %20] [%13, %15] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
    }
  } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
  %11 = scf.for %arg0 = %c0 to %c256 step %c32 iter_args(%arg1 = %10) -> (tensor<?x?xf32>) {
    %extracted_slice = tensor.extract_slice %8[0, %arg0] [%4, 32] [1, 1] : tensor<?x256xf32> to tensor<?x32xf32>
    %extracted_slice_0 = tensor.extract_slice %9[%arg0, 0] [32, %6] [1, 1] : tensor<256x?xf32> to tensor<32x?xf32>
    %extracted_slice_1 = tensor.extract_slice %arg1[0, 0] [%4, %6] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
    %12 = scf.foreach_thread (%arg2, %arg3) in (%c8, %c32) shared_outs(%arg4 = %extracted_slice_1) -> (tensor<?x?xf32>) {
      %13 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg2)[%4]
      %14 = affine.max affine_map<(d0) -> (0, d0)>(%13)
      %15 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg3)[%6]
      %16 = affine.max affine_map<(d0) -> (0, d0)>(%15)
      %17 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg2)[%4]
      %18 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg3)[%6]
      %19 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg2)[%4]
      %20 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg3)[%6]
      %extracted_slice_2 = tensor.extract_slice %extracted_slice[%17, 0] [%14, 32] [1, 1] : tensor<?x32xf32> to tensor<?x32xf32>
      %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[0, %18] [32, %16] [1, 1] : tensor<32x?xf32> to tensor<32x?xf32>
      %extracted_slice_4 = tensor.extract_slice %arg4[%19, %20] [%14, %16] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
      %extracted_slice_5 = tensor.extract_slice %extracted_slice_2[0, 0] [%14, 32] [1, 1] : tensor<?x32xf32> to tensor<?x32xf32>
      %extracted_slice_6 = tensor.extract_slice %extracted_slice_3[0, 0] [32, %16] [1, 1] : tensor<32x?xf32> to tensor<32x?xf32>
      %extracted_slice_7 = tensor.extract_slice %extracted_slice_4[0, 0] [%14, %16] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
      %21 = linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%extracted_slice_5, %extracted_slice_6 : tensor<?x32xf32>, tensor<32x?xf32>) outs(%extracted_slice_7 : tensor<?x?xf32>) -> tensor<?x?xf32>
      %inserted_slice_8 = tensor.insert_slice %21 into %extracted_slice_4[0, 0] [%14, %16] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
      %22 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg2)[%4]
      %23 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg3)[%6]
      scf.foreach_thread.perform_concurrently {
        tensor.parallel_insert_slice %inserted_slice_8 into %arg4[%22, %23] [%14, %16] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
      }
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
    %inserted_slice = tensor.insert_slice %12 into %arg1[0, 0] [%4, %6] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
    scf.yield %inserted_slice : tensor<?x?xf32>
  }
  flow.dispatch.tensor.store %11, %2, offsets = [%3, %5], sizes = [%4, %6], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %7 = flow.dispatch.tensor.load %2, offsets = [%3, %5], sizes = [%4, %6], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<?x?xf32>
  %8 = flow.dispatch.tensor.load %0, offsets = [%3, 0], sizes = [%4, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
  %9 = flow.dispatch.tensor.load %1, offsets = [0, %5], sizes = [256, %6], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
  %10 = scf.foreach_thread (%arg0, %arg1) in (%c8, %c32) shared_outs(%arg2 = %7) -> (tensor<?x?xf32>) {
    %12 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg0)[%4]
    %13 = affine.max affine_map<(d0) -> (0, d0)>(%12)
    %14 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg1)[%6]
    %15 = affine.max affine_map<(d0) -> (0, d0)>(%14)
    %16 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg0)[%4]
    %17 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg1)[%6]
    %extracted_slice = tensor.extract_slice %arg2[%16, %17] [%13, %15] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
    %18 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%extracted_slice : tensor<?x?xf32>) -> tensor<?x?xf32>
    scf.foreach_thread.perform_concurrently {
      tensor.parallel_insert_slice %18 into %arg2[%16, %17] [%13, %15] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
    }
  } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
  %11 = scf.for %arg0 = %c0 to %c256 step %c32 iter_args(%arg1 = %10) -> (tensor<?x?xf32>) {
    %extracted_slice = tensor.extract_slice %8[0, %arg0] [%4, 32] [1, 1] : tensor<?x256xf32> to tensor<?x32xf32>
    %extracted_slice_0 = tensor.extract_slice %9[%arg0, 0] [32, %6] [1, 1] : tensor<256x?xf32> to tensor<32x?xf32>
    %extracted_slice_1 = tensor.extract_slice %arg1[0, 0] [%4, %6] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
    %12 = scf.foreach_thread (%arg2, %arg3) in (%c8, %c32) shared_outs(%arg4 = %extracted_slice_1) -> (tensor<?x?xf32>) {
      %13 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg2)[%4]
      %14 = affine.max affine_map<(d0) -> (0, d0)>(%13)
      %15 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg3)[%6]
      %16 = affine.max affine_map<(d0) -> (0, d0)>(%15)
      %17 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg2)[%4]
      %18 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg3)[%6]
      %extracted_slice_2 = tensor.extract_slice %extracted_slice[%17, 0] [%14, 32] [1, 1] : tensor<?x32xf32> to tensor<?x32xf32>
      %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[0, %18] [32, %16] [1, 1] : tensor<32x?xf32> to tensor<32x?xf32>
      %extracted_slice_4 = tensor.extract_slice %arg4[%17, %18] [%14, %16] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
      %extracted_slice_5 = tensor.extract_slice %extracted_slice_2[0, 0] [%14, 32] [1, 1] : tensor<?x32xf32> to tensor<?x32xf32>
      %extracted_slice_6 = tensor.extract_slice %extracted_slice_3[0, 0] [32, %16] [1, 1] : tensor<32x?xf32> to tensor<32x?xf32>
      %extracted_slice_7 = tensor.extract_slice %extracted_slice_4[0, 0] [%14, %16] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
      %19 = linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%extracted_slice_5, %extracted_slice_6 : tensor<?x32xf32>, tensor<32x?xf32>) outs(%extracted_slice_7 : tensor<?x?xf32>) -> tensor<?x?xf32>
      %inserted_slice_8 = tensor.insert_slice %19 into %extracted_slice_4[0, 0] [%14, %16] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
      scf.foreach_thread.perform_concurrently {
        tensor.parallel_insert_slice %inserted_slice_8 into %arg4[%17, %18] [%14, %16] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
      }
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
    %inserted_slice = tensor.insert_slice %12 into %arg1[0, 0] [%4, %6] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
    scf.yield %inserted_slice : tensor<?x?xf32>
  }
  flow.dispatch.tensor.store %11, %2, offsets = [%3, %5], sizes = [%4, %6], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  return
}

// -----// IR Dump After EliminateEmptyTensors (iree-eliminate-empty-tensors) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
    %7 = flow.dispatch.tensor.load %2, offsets = [%3, %5], sizes = [%4, %6], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<?x?xf32>
    %8 = flow.dispatch.tensor.load %0, offsets = [%3, 0], sizes = [%4, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
    %9 = flow.dispatch.tensor.load %1, offsets = [0, %5], sizes = [256, %6], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
    %10 = scf.foreach_thread (%arg0, %arg1) in (%c8, %c32) shared_outs(%arg2 = %7) -> (tensor<?x?xf32>) {
      %12 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg0)[%4]
      %13 = affine.max affine_map<(d0) -> (0, d0)>(%12)
      %14 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg1)[%6]
      %15 = affine.max affine_map<(d0) -> (0, d0)>(%14)
      %16 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg0)[%4]
      %17 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg1)[%6]
      %extracted_slice = tensor.extract_slice %arg2[%16, %17] [%13, %15] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
      %18 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%extracted_slice : tensor<?x?xf32>) -> tensor<?x?xf32>
      scf.foreach_thread.perform_concurrently {
        tensor.parallel_insert_slice %18 into %arg2[%16, %17] [%13, %15] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
      }
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
    %11 = scf.for %arg0 = %c0 to %c256 step %c32 iter_args(%arg1 = %10) -> (tensor<?x?xf32>) {
      %extracted_slice = tensor.extract_slice %8[0, %arg0] [%4, 32] [1, 1] : tensor<?x256xf32> to tensor<?x32xf32>
      %extracted_slice_0 = tensor.extract_slice %9[%arg0, 0] [32, %6] [1, 1] : tensor<256x?xf32> to tensor<32x?xf32>
      %extracted_slice_1 = tensor.extract_slice %arg1[0, 0] [%4, %6] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
      %12 = scf.foreach_thread (%arg2, %arg3) in (%c8, %c32) shared_outs(%arg4 = %extracted_slice_1) -> (tensor<?x?xf32>) {
        %13 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg2)[%4]
        %14 = affine.max affine_map<(d0) -> (0, d0)>(%13)
        %15 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg3)[%6]
        %16 = affine.max affine_map<(d0) -> (0, d0)>(%15)
        %17 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg2)[%4]
        %18 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg3)[%6]
        %extracted_slice_2 = tensor.extract_slice %extracted_slice[%17, 0] [%14, 32] [1, 1] : tensor<?x32xf32> to tensor<?x32xf32>
        %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[0, %18] [32, %16] [1, 1] : tensor<32x?xf32> to tensor<32x?xf32>
        %extracted_slice_4 = tensor.extract_slice %arg4[%17, %18] [%14, %16] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
        %extracted_slice_5 = tensor.extract_slice %extracted_slice_2[0, 0] [%14, 32] [1, 1] : tensor<?x32xf32> to tensor<?x32xf32>
        %extracted_slice_6 = tensor.extract_slice %extracted_slice_3[0, 0] [32, %16] [1, 1] : tensor<32x?xf32> to tensor<32x?xf32>
        %extracted_slice_7 = tensor.extract_slice %extracted_slice_4[0, 0] [%14, %16] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
        %19 = linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%extracted_slice_5, %extracted_slice_6 : tensor<?x32xf32>, tensor<32x?xf32>) outs(%extracted_slice_7 : tensor<?x?xf32>) -> tensor<?x?xf32>
        %inserted_slice_8 = tensor.insert_slice %19 into %extracted_slice_4[0, 0] [%14, %16] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
        scf.foreach_thread.perform_concurrently {
          tensor.parallel_insert_slice %inserted_slice_8 into %arg4[%17, %18] [%14, %16] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
        }
      } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
      %inserted_slice = tensor.insert_slice %12 into %arg1[0, 0] [%4, %6] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
      scf.yield %inserted_slice : tensor<?x?xf32>
    }
    flow.dispatch.tensor.store %11, %2, offsets = [%3, %5], sizes = [%4, %6], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    return
  }
}

// -----// IR Dump After EmptyTensorToAllocTensor (empty-tensor-to-alloc-tensor) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
    %7 = flow.dispatch.tensor.load %2, offsets = [%3, %5], sizes = [%4, %6], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<?x?xf32>
    %8 = flow.dispatch.tensor.load %0, offsets = [%3, 0], sizes = [%4, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
    %9 = flow.dispatch.tensor.load %1, offsets = [0, %5], sizes = [256, %6], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
    %10 = scf.foreach_thread (%arg0, %arg1) in (%c8, %c32) shared_outs(%arg2 = %7) -> (tensor<?x?xf32>) {
      %12 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg0)[%4]
      %13 = affine.max affine_map<(d0) -> (0, d0)>(%12)
      %14 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg1)[%6]
      %15 = affine.max affine_map<(d0) -> (0, d0)>(%14)
      %16 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg0)[%4]
      %17 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg1)[%6]
      %extracted_slice = tensor.extract_slice %arg2[%16, %17] [%13, %15] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
      %18 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%extracted_slice : tensor<?x?xf32>) -> tensor<?x?xf32>
      scf.foreach_thread.perform_concurrently {
        tensor.parallel_insert_slice %18 into %arg2[%16, %17] [%13, %15] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
      }
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
    %11 = scf.for %arg0 = %c0 to %c256 step %c32 iter_args(%arg1 = %10) -> (tensor<?x?xf32>) {
      %extracted_slice = tensor.extract_slice %8[0, %arg0] [%4, 32] [1, 1] : tensor<?x256xf32> to tensor<?x32xf32>
      %extracted_slice_0 = tensor.extract_slice %9[%arg0, 0] [32, %6] [1, 1] : tensor<256x?xf32> to tensor<32x?xf32>
      %extracted_slice_1 = tensor.extract_slice %arg1[0, 0] [%4, %6] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
      %12 = scf.foreach_thread (%arg2, %arg3) in (%c8, %c32) shared_outs(%arg4 = %extracted_slice_1) -> (tensor<?x?xf32>) {
        %13 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg2)[%4]
        %14 = affine.max affine_map<(d0) -> (0, d0)>(%13)
        %15 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg3)[%6]
        %16 = affine.max affine_map<(d0) -> (0, d0)>(%15)
        %17 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg2)[%4]
        %18 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg3)[%6]
        %extracted_slice_2 = tensor.extract_slice %extracted_slice[%17, 0] [%14, 32] [1, 1] : tensor<?x32xf32> to tensor<?x32xf32>
        %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[0, %18] [32, %16] [1, 1] : tensor<32x?xf32> to tensor<32x?xf32>
        %extracted_slice_4 = tensor.extract_slice %arg4[%17, %18] [%14, %16] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
        %extracted_slice_5 = tensor.extract_slice %extracted_slice_2[0, 0] [%14, 32] [1, 1] : tensor<?x32xf32> to tensor<?x32xf32>
        %extracted_slice_6 = tensor.extract_slice %extracted_slice_3[0, 0] [32, %16] [1, 1] : tensor<32x?xf32> to tensor<32x?xf32>
        %extracted_slice_7 = tensor.extract_slice %extracted_slice_4[0, 0] [%14, %16] [1, 1] : tensor<?x?xf32> to tensor<?x?xf32>
        %19 = linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%extracted_slice_5, %extracted_slice_6 : tensor<?x32xf32>, tensor<32x?xf32>) outs(%extracted_slice_7 : tensor<?x?xf32>) -> tensor<?x?xf32>
        %inserted_slice_8 = tensor.insert_slice %19 into %extracted_slice_4[0, 0] [%14, %16] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
        scf.foreach_thread.perform_concurrently {
          tensor.parallel_insert_slice %inserted_slice_8 into %arg4[%17, %18] [%14, %16] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
        }
      } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
      %inserted_slice = tensor.insert_slice %12 into %arg1[0, 0] [%4, %6] [1, 1] : tensor<?x?xf32> into tensor<?x?xf32>
      scf.yield %inserted_slice : tensor<?x?xf32>
    }
    flow.dispatch.tensor.store %11, %2, offsets = [%3, %5], sizes = [%4, %6], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    return
  }
}

// -----// IR Dump After IREEComprehensiveBufferize (iree-codegen-iree-comprehensive-bufferize) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %6 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %7 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
    %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %9 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
    %subview = memref.subview %4[%6, %8] [%7, %9] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_0 = memref.subview %0[%6, 0] [%7, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<?x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_1 = memref.subview %2[0, %8] [256, %9] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.foreach_thread (%arg0, %arg1) in (%c8, %c32) {
      %11 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg0)[%7]
      %12 = affine.max affine_map<(d0) -> (0, d0)>(%11)
      %13 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg1)[%9]
      %14 = affine.max affine_map<(d0) -> (0, d0)>(%13)
      %15 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg0)[%7]
      %16 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg1)[%9]
      %subview_3 = memref.subview %subview[%15, %16] [%12, %14] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_3 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
      %subview_4 = memref.subview %subview[%15, %16] [%12, %14] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      memref.copy %subview_3, %subview_4 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
    %10 = scf.for %arg0 = %c0 to %c256 step %c32 iter_args(%arg1 = %subview) -> (memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
      %subview_3 = memref.subview %subview_0[0, %arg0] [%7, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_4 = memref.subview %subview_1[%arg0, 0] [32, %9] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_5 = memref.subview %arg1[0, 0] [%7, %9] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.foreach_thread (%arg2, %arg3) in (%c8, %c32) {
        %11 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg2)[%7]
        %12 = affine.max affine_map<(d0) -> (0, d0)>(%11)
        %13 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg3)[%9]
        %14 = affine.max affine_map<(d0) -> (0, d0)>(%13)
        %15 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg2)[%7]
        %16 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg3)[%9]
        %subview_7 = memref.subview %subview_3[%15, 0] [%12, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_8 = memref.subview %subview_4[0, %16] [32, %14] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_9 = memref.subview %subview_5[%15, %16] [%12, %14] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_10 = memref.subview %subview_7[0, 0] [%12, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_11 = memref.subview %subview_8[0, 0] [32, %14] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_12 = memref.subview %subview_9[0, 0] [%12, %14] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_10, %subview_11 : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_12 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
        %subview_13 = memref.subview %subview_9[0, 0] [%12, %14] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        memref.copy %subview_12, %subview_13 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_14 = memref.subview %subview_5[%15, %16] [%12, %14] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        memref.copy %subview_9, %subview_14 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
      %subview_6 = memref.subview %arg1[0, 0] [%7, %9] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      memref.copy %subview_5, %subview_6 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.yield %arg1 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    }
    %subview_2 = memref.subview %4[%6, %8] [%7, %9] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    memref.copy %10, %subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    return
  }
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %6 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %7 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
    %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %9 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
    %subview = memref.subview %4[%6, %8] [%7, %9] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_0 = memref.subview %0[%6, 0] [%7, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<?x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_1 = memref.subview %2[0, %8] [256, %9] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.foreach_thread (%arg0, %arg1) in (%c8, %c32) {
      %11 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg0)[%7]
      %12 = affine.max affine_map<(d0) -> (0, d0)>(%11)
      %13 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg1)[%9]
      %14 = affine.max affine_map<(d0) -> (0, d0)>(%13)
      %15 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg0)[%7]
      %16 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg1)[%9]
      %subview_3 = memref.subview %subview[%15, %16] [%12, %14] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_3 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
      %subview_4 = memref.subview %subview[%15, %16] [%12, %14] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      memref.copy %subview_3, %subview_4 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
    %10 = scf.for %arg0 = %c0 to %c256 step %c32 iter_args(%arg1 = %subview) -> (memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
      %subview_3 = memref.subview %subview_0[0, %arg0] [%7, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_4 = memref.subview %subview_1[%arg0, 0] [32, %9] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_5 = memref.subview %arg1[0, 0] [%7, %9] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.foreach_thread (%arg2, %arg3) in (%c8, %c32) {
        %11 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg2)[%7]
        %12 = affine.max affine_map<(d0) -> (0, d0)>(%11)
        %13 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg3)[%9]
        %14 = affine.max affine_map<(d0) -> (0, d0)>(%13)
        %15 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg2)[%7]
        %16 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg3)[%9]
        %subview_7 = memref.subview %subview_3[%15, 0] [%12, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_8 = memref.subview %subview_4[0, %16] [32, %14] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_9 = memref.subview %subview_5[%15, %16] [%12, %14] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_10 = memref.subview %subview_7[0, 0] [%12, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_11 = memref.subview %subview_8[0, 0] [32, %14] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_12 = memref.subview %subview_9[0, 0] [%12, %14] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_10, %subview_11 : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_12 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
        %subview_13 = memref.subview %subview_9[0, 0] [%12, %14] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        memref.copy %subview_12, %subview_13 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_14 = memref.subview %subview_5[%15, %16] [%12, %14] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        memref.copy %subview_9, %subview_14 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
      %subview_6 = memref.subview %arg1[0, 0] [%7, %9] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      memref.copy %subview_5, %subview_6 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.yield %arg1 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    }
    %subview_2 = memref.subview %4[%6, %8] [%7, %9] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    memref.copy %10, %subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %6 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %7 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %9 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %4[%6, %8] [%7, %9] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_0 = memref.subview %0[%6, 0] [%7, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<?x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_1 = memref.subview %2[0, %8] [256, %9] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  scf.foreach_thread (%arg0, %arg1) in (%c8, %c32) {
    %10 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg0)[%7]
    %11 = affine.max affine_map<(d0) -> (0, d0)>(%10)
    %12 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg1)[%9]
    %13 = affine.max affine_map<(d0) -> (0, d0)>(%12)
    %14 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg0)[%7]
    %15 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg1)[%9]
    %subview_3 = memref.subview %subview[%14, %15] [%11, %13] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_3 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
    %subview_4 = memref.subview %subview[%14, %15] [%11, %13] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    memref.copy %subview_3, %subview_4 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_3 = memref.subview %subview_0[0, %arg0] [%7, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_4 = memref.subview %subview_1[%arg0, 0] [32, %9] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_5 = memref.subview %subview[0, 0] [%7, %9] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.foreach_thread (%arg1, %arg2) in (%c8, %c32) {
      %10 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg1)[%7]
      %11 = affine.max affine_map<(d0) -> (0, d0)>(%10)
      %12 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg2)[%9]
      %13 = affine.max affine_map<(d0) -> (0, d0)>(%12)
      %14 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg1)[%7]
      %15 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg2)[%9]
      %subview_7 = memref.subview %subview_3[%14, 0] [%11, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_8 = memref.subview %subview_4[0, %15] [32, %13] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_9 = memref.subview %subview_5[%14, %15] [%11, %13] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_10 = memref.subview %subview_7[0, 0] [%11, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_11 = memref.subview %subview_8[0, 0] [32, %13] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_12 = memref.subview %subview_9[0, 0] [%11, %13] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_10, %subview_11 : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_12 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
      %subview_13 = memref.subview %subview_9[0, 0] [%11, %13] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      memref.copy %subview_12, %subview_13 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_14 = memref.subview %subview_5[%14, %15] [%11, %13] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      memref.copy %subview_9, %subview_14 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
    %subview_6 = memref.subview %subview[0, 0] [%7, %9] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    memref.copy %subview_5, %subview_6 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  }
  %subview_2 = memref.subview %4[%6, %8] [%7, %9] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  memref.copy %subview, %subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %6 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %7 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %9 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %4[%6, %8] [%7, %9] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_0 = memref.subview %0[%6, 0] [%7, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<?x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_1 = memref.subview %2[0, %8] [256, %9] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  scf.foreach_thread (%arg0, %arg1) in (%c8, %c32) {
    %10 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg0)[%7]
    %11 = affine.max affine_map<(d0) -> (0, d0)>(%10)
    %12 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg1)[%9]
    %13 = affine.max affine_map<(d0) -> (0, d0)>(%12)
    %14 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg0)[%7]
    %15 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg1)[%9]
    %subview_2 = memref.subview %subview[%14, %15] [%11, %13] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
    memref.copy %subview_2, %subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_2 = memref.subview %subview_0[0, %arg0] [%7, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_3 = memref.subview %subview_1[%arg0, 0] [32, %9] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_4 = memref.subview %subview[0, 0] [%7, %9] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.foreach_thread (%arg1, %arg2) in (%c8, %c32) {
      %10 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg1)[%7]
      %11 = affine.max affine_map<(d0) -> (0, d0)>(%10)
      %12 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg2)[%9]
      %13 = affine.max affine_map<(d0) -> (0, d0)>(%12)
      %14 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg1)[%7]
      %15 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg2)[%9]
      %subview_5 = memref.subview %subview_2[%14, 0] [%11, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_6 = memref.subview %subview_3[0, %15] [32, %13] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_7 = memref.subview %subview_4[%14, %15] [%11, %13] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_8 = memref.subview %subview_5[0, 0] [%11, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_9 = memref.subview %subview_6[0, 0] [32, %13] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_10 = memref.subview %subview_7[0, 0] [%11, %13] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_8, %subview_9 : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_10 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
      memref.copy %subview_10, %subview_10 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      memref.copy %subview_7, %subview_7 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
    memref.copy %subview_4, %subview_4 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  }
  memref.copy %subview, %subview : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %6 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %7 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %9 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %4[%6, %8] [%7, %9] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_0 = memref.subview %0[%6, 0] [%7, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<?x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_1 = memref.subview %2[0, %8] [256, %9] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  scf.foreach_thread (%arg0, %arg1) in (%c8, %c32) {
    %10 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg0)[%7]
    %11 = affine.max affine_map<(d0) -> (0, d0)>(%10)
    %12 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg1)[%9]
    %13 = affine.max affine_map<(d0) -> (0, d0)>(%12)
    %14 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg0)[%7]
    %15 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg1)[%9]
    %subview_2 = memref.subview %subview[%14, %15] [%11, %13] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
  } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_2 = memref.subview %subview_0[0, %arg0] [%7, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_3 = memref.subview %subview_1[%arg0, 0] [32, %9] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_4 = memref.subview %subview[0, 0] [%7, %9] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.foreach_thread (%arg1, %arg2) in (%c8, %c32) {
      %10 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg1)[%7]
      %11 = affine.max affine_map<(d0) -> (0, d0)>(%10)
      %12 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg2)[%9]
      %13 = affine.max affine_map<(d0) -> (0, d0)>(%12)
      %14 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg1)[%7]
      %15 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg2)[%9]
      %subview_5 = memref.subview %subview_2[%14, 0] [%11, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_6 = memref.subview %subview_3[0, %15] [32, %13] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_7 = memref.subview %subview_4[%14, %15] [%11, %13] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_8 = memref.subview %subview_5[0, 0] [%11, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_9 = memref.subview %subview_6[0, 0] [32, %13] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_10 = memref.subview %subview_7[0, 0] [%11, %13] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_8, %subview_9 : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_10 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
  }
  return
}

// -----// IR Dump After CleanupBufferAllocView (iree-codegen-cleanup-buffer-alloc-view) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<?x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  scf.foreach_thread (%arg0, %arg1) in (%c8, %c32) {
    %7 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg0)[%4]
    %8 = affine.max affine_map<(d0) -> (0, d0)>(%7)
    %9 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg1)[%6]
    %10 = affine.max affine_map<(d0) -> (0, d0)>(%9)
    %11 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg0)[%4]
    %12 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg1)[%6]
    %subview_2 = memref.subview %subview[%11, %12] [%8, %10] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
  } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_2 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_3 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_4 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.foreach_thread (%arg1, %arg2) in (%c8, %c32) {
      %7 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg1)[%4]
      %8 = affine.max affine_map<(d0) -> (0, d0)>(%7)
      %9 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg2)[%6]
      %10 = affine.max affine_map<(d0) -> (0, d0)>(%9)
      %11 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg1)[%4]
      %12 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg2)[%6]
      %subview_5 = memref.subview %subview_2[%11, 0] [%8, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_6 = memref.subview %subview_3[0, %12] [32, %10] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_7 = memref.subview %subview_4[%11, %12] [%8, %10] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_8 = memref.subview %subview_5[0, 0] [%8, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_9 = memref.subview %subview_6[0, 0] [32, %10] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_10 = memref.subview %subview_7[0, 0] [%8, %10] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_8, %subview_9 : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_10 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
    %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<?x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.foreach_thread (%arg0, %arg1) in (%c8, %c32) {
      %7 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg0)[%4]
      %8 = affine.max affine_map<(d0) -> (0, d0)>(%7)
      %9 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg1)[%6]
      %10 = affine.max affine_map<(d0) -> (0, d0)>(%9)
      %11 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg0)[%4]
      %12 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg1)[%6]
      %subview_2 = memref.subview %subview[%11, %12] [%8, %10] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
    scf.for %arg0 = %c0 to %c256 step %c32 {
      %subview_2 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_3 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_4 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.foreach_thread (%arg1, %arg2) in (%c8, %c32) {
        %7 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg1)[%4]
        %8 = affine.max affine_map<(d0) -> (0, d0)>(%7)
        %9 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg2)[%6]
        %10 = affine.max affine_map<(d0) -> (0, d0)>(%9)
        %11 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg1)[%4]
        %12 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg2)[%6]
        %subview_5 = memref.subview %subview_2[%11, 0] [%8, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_6 = memref.subview %subview_3[0, %12] [32, %10] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_7 = memref.subview %subview_4[%11, %12] [%8, %10] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_8 = memref.subview %subview_5[0, 0] [%8, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_9 = memref.subview %subview_6[0, 0] [32, %10] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_10 = memref.subview %subview_7[0, 0] [%8, %10] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_8, %subview_9 : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_10 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
      } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
    %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<?x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.foreach_thread (%arg0, %arg1) in (%c8, %c32) {
      %7 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg0)[%4]
      %8 = affine.max affine_map<(d0) -> (0, d0)>(%7)
      %9 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg1)[%6]
      %10 = affine.max affine_map<(d0) -> (0, d0)>(%9)
      %11 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg0)[%4]
      %12 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg1)[%6]
      %subview_2 = memref.subview %subview[%11, %12] [%8, %10] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
    scf.for %arg0 = %c0 to %c256 step %c32 {
      %subview_2 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_3 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_4 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.foreach_thread (%arg1, %arg2) in (%c8, %c32) {
        %7 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg1)[%4]
        %8 = affine.max affine_map<(d0) -> (0, d0)>(%7)
        %9 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg2)[%6]
        %10 = affine.max affine_map<(d0) -> (0, d0)>(%9)
        %11 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg1)[%4]
        %12 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg2)[%6]
        %subview_5 = memref.subview %subview_2[%11, 0] [%8, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_6 = memref.subview %subview_3[0, %12] [32, %10] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_7 = memref.subview %subview_4[%11, %12] [%8, %10] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_8 = memref.subview %subview_5[0, 0] [%8, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_9 = memref.subview %subview_6[0, 0] [32, %10] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_10 = memref.subview %subview_7[0, 0] [%8, %10] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_8, %subview_9 : memref<?x32xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, memref<32x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_10 : memref<?x?xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>)
      } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
    }
    return
  }
}

// -----// IR Dump After EraseHALDescriptorTypeFromMemRef (iree-codegen-erase-hal-descriptor-type-from-memref) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
  scf.foreach_thread (%arg0, %arg1) in (%c8, %c32) {
    %7 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg0)[%4]
    %8 = affine.max affine_map<(d0) -> (0, d0)>(%7)
    %9 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg1)[%6]
    %10 = affine.max affine_map<(d0) -> (0, d0)>(%9)
    %11 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg0)[%4]
    %12 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg1)[%6]
    %subview_2 = memref.subview %subview[%11, %12] [%8, %10] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_2 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_3 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_4 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    scf.foreach_thread (%arg1, %arg2) in (%c8, %c32) {
      %7 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%arg1)[%4]
      %8 = affine.max affine_map<(d0) -> (0, d0)>(%7)
      %9 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%arg2)[%6]
      %10 = affine.max affine_map<(d0) -> (0, d0)>(%9)
      %11 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%arg1)[%4]
      %12 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%arg2)[%6]
      %subview_5 = memref.subview %subview_2[%11, 0] [%8, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_6 = memref.subview %subview_3[0, %12] [32, %10] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_7 = memref.subview %subview_4[%11, %12] [%8, %10] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      %subview_8 = memref.subview %subview_5[0, 0] [%8, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_9 = memref.subview %subview_6[0, 0] [32, %10] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_10 = memref.subview %subview_7[0, 0] [%8, %10] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_8, %subview_9 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_10 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    } {mapping = [#gpu.thread<y>, #gpu.thread<x>]}
  }
  return
}

// -----// IR Dump After LLVMGPUDistribute (iree-llvmgpu-distribute) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
  %c1 = arith.constant 1 : index
  %7 = gpu.thread_id  x
  %8 = gpu.thread_id  y
  %9 = gpu.thread_id  z
  %10 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%8)[%4]
  %11 = affine.max affine_map<(d0) -> (0, d0)>(%10)
  %12 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%7)[%6]
  %13 = affine.max affine_map<(d0) -> (0, d0)>(%12)
  %14 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%8)[%4]
  %15 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%7)[%6]
  %subview_2 = memref.subview %subview[%14, %15] [%11, %13] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_3 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_4 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_5 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %c1_6 = arith.constant 1 : index
    %16 = gpu.thread_id  x
    %17 = gpu.thread_id  y
    %18 = gpu.thread_id  z
    %19 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%17)[%4]
    %20 = affine.max affine_map<(d0) -> (0, d0)>(%19)
    %21 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%16)[%6]
    %22 = affine.max affine_map<(d0) -> (0, d0)>(%21)
    %23 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%17)[%4]
    %24 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%16)[%6]
    %subview_7 = memref.subview %subview_3[%23, 0] [%20, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_8 = memref.subview %subview_4[0, %24] [32, %22] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_9 = memref.subview %subview_5[%23, %24] [%20, %22] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_10 = memref.subview %subview_7[0, 0] [%20, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_11 = memref.subview %subview_8[0, 0] [32, %22] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_12 = memref.subview %subview_9[0, 0] [%20, %22] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_10, %subview_11 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_12 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  }
  return
}

// -----// IR Dump After MemrefCopyToLinalgPass (iree-codegen-memrefcopy-to-linalg) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
  %7 = gpu.thread_id  x
  %8 = gpu.thread_id  y
  %9 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%8)[%4]
  %10 = affine.max affine_map<(d0) -> (0, d0)>(%9)
  %11 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%7)[%6]
  %12 = affine.max affine_map<(d0) -> (0, d0)>(%11)
  %13 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%8)[%4]
  %14 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%7)[%6]
  %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_3 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_4 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_5 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %15 = gpu.thread_id  x
    %16 = gpu.thread_id  y
    %17 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%16)[%4]
    %18 = affine.max affine_map<(d0) -> (0, d0)>(%17)
    %19 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%15)[%6]
    %20 = affine.max affine_map<(d0) -> (0, d0)>(%19)
    %21 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%16)[%4]
    %22 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%15)[%6]
    %subview_6 = memref.subview %subview_3[%21, 0] [%18, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_7 = memref.subview %subview_4[0, %22] [32, %20] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_8 = memref.subview %subview_5[%21, %22] [%18, %20] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_9 = memref.subview %subview_6[0, 0] [%18, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_10 = memref.subview %subview_7[0, 0] [32, %20] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_11 = memref.subview %subview_8[0, 0] [%18, %20] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_9, %subview_10 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_11 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  }
  return
}

// -----// IR Dump After GPUDistributeSharedMemoryCopy (iree-gpu-distribute-shared-memory-copy) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
  %7 = gpu.thread_id  x
  %8 = gpu.thread_id  y
  %9 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%8)[%4]
  %10 = affine.max affine_map<(d0) -> (0, d0)>(%9)
  %11 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%7)[%6]
  %12 = affine.max affine_map<(d0) -> (0, d0)>(%11)
  %13 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%8)[%4]
  %14 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%7)[%6]
  %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_3 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_4 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_5 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %15 = gpu.thread_id  x
    %16 = gpu.thread_id  y
    %17 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%16)[%4]
    %18 = affine.max affine_map<(d0) -> (0, d0)>(%17)
    %19 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%15)[%6]
    %20 = affine.max affine_map<(d0) -> (0, d0)>(%19)
    %21 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%16)[%4]
    %22 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%15)[%6]
    %subview_6 = memref.subview %subview_3[%21, 0] [%18, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_7 = memref.subview %subview_4[0, %22] [32, %20] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_8 = memref.subview %subview_5[%21, %22] [%18, %20] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_9 = memref.subview %subview_6[0, 0] [%18, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_10 = memref.subview %subview_7[0, 0] [32, %20] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_11 = memref.subview %subview_8[0, 0] [%18, %20] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_9, %subview_10 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_11 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c0 = arith.constant 0 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
    memref.assume_alignment %0, 64 : memref<512x256xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
    memref.assume_alignment %1, 64 : memref<256x1024xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
    memref.assume_alignment %2, 64 : memref<512x1024xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
    %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
    %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
    %7 = gpu.thread_id  x
    %8 = gpu.thread_id  y
    %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
    %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
    %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
    %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
    %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
    %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
    %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    scf.for %arg0 = %c0 to %c256 step %c32 {
      %subview_3 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_4 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_5 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      %15 = gpu.thread_id  x
      %16 = gpu.thread_id  y
      %17 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%16)[%4]
      %18 = affine.max affine_map<(d0) -> (0, d0)>(%17)
      %19 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%15)[%6]
      %20 = affine.max affine_map<(d0) -> (0, d0)>(%19)
      %21 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%16)[%4]
      %22 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%15)[%6]
      %subview_6 = memref.subview %subview_3[%21, 0] [%18, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_7 = memref.subview %subview_4[0, %22] [32, %20] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_8 = memref.subview %subview_5[%21, %22] [%18, %20] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      %subview_9 = memref.subview %subview_6[0, 0] [%18, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_10 = memref.subview %subview_7[0, 0] [32, %20] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_11 = memref.subview %subview_8[0, 0] [%18, %20] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_9, %subview_10 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_11 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c0 = arith.constant 0 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
    memref.assume_alignment %0, 64 : memref<512x256xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
    memref.assume_alignment %1, 64 : memref<256x1024xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
    memref.assume_alignment %2, 64 : memref<512x1024xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
    %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
    %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
    %7 = gpu.thread_id  x
    %8 = gpu.thread_id  y
    %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
    %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
    %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
    %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
    %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
    %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
    %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    scf.for %arg0 = %c0 to %c256 step %c32 {
      %subview_3 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_4 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_5 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      %15 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%8)[%4]
      %16 = affine.max affine_map<(d0) -> (0, d0)>(%15)
      %17 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%7)[%6]
      %18 = affine.max affine_map<(d0) -> (0, d0)>(%17)
      %19 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%8)[%4]
      %20 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%7)[%6]
      %subview_6 = memref.subview %subview_3[%19, 0] [%16, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_7 = memref.subview %subview_4[0, %20] [32, %18] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_8 = memref.subview %subview_5[%19, %20] [%16, %18] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      %subview_9 = memref.subview %subview_6[0, 0] [%16, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_10 = memref.subview %subview_7[0, 0] [32, %18] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_11 = memref.subview %subview_8[0, 0] [%16, %18] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_9, %subview_10 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_11 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    }
    return
  }
}

// -----// IR Dump After GPUReduceBankConflicts (iree-gpu-reduce-bank-conflicts) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
  %7 = gpu.thread_id  x
  %8 = gpu.thread_id  y
  %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
  %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
  %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
  %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
  %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
  %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
  %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_3 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_4 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_5 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %15 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%8)[%4]
    %16 = affine.max affine_map<(d0) -> (0, d0)>(%15)
    %17 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%7)[%6]
    %18 = affine.max affine_map<(d0) -> (0, d0)>(%17)
    %19 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%8)[%4]
    %20 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%7)[%6]
    %subview_6 = memref.subview %subview_3[%19, 0] [%16, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_7 = memref.subview %subview_4[0, %20] [32, %18] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_8 = memref.subview %subview_5[%19, %20] [%16, %18] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_9 = memref.subview %subview_6[0, 0] [%16, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_10 = memref.subview %subview_7[0, 0] [32, %18] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_11 = memref.subview %subview_8[0, 0] [%16, %18] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_9, %subview_10 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_11 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  }
  return
}

// -----// IR Dump After WorkGroupSwizzle (iree-workgroup-swizzle) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
  %7 = gpu.thread_id  x
  %8 = gpu.thread_id  y
  %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
  %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
  %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
  %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
  %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
  %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
  %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_3 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_4 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_5 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %15 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 8)) + s0, s0 ceildiv 8)>(%8)[%4]
    %16 = affine.max affine_map<(d0) -> (0, d0)>(%15)
    %17 = affine.min affine_map<(d0)[s0] -> (-(d0 * (s0 ceildiv 32)) + s0, s0 ceildiv 32)>(%7)[%6]
    %18 = affine.max affine_map<(d0) -> (0, d0)>(%17)
    %19 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 8))>(%8)[%4]
    %20 = affine.apply affine_map<(d0)[s0] -> (d0 * (s0 ceildiv 32))>(%7)[%6]
    %subview_6 = memref.subview %subview_3[%19, 0] [%16, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_7 = memref.subview %subview_4[0, %20] [32, %18] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_8 = memref.subview %subview_5[%19, %20] [%16, %18] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_9 = memref.subview %subview_6[0, 0] [%16, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_10 = memref.subview %subview_7[0, 0] [32, %18] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_11 = memref.subview %subview_8[0, 0] [%16, %18] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_9, %subview_10 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_11 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c0 = arith.constant 0 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
    memref.assume_alignment %0, 64 : memref<512x256xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
    memref.assume_alignment %1, 64 : memref<256x1024xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
    memref.assume_alignment %2, 64 : memref<512x1024xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
    %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
    %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
    %7 = gpu.thread_id  x
    %8 = gpu.thread_id  y
    %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
    %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
    %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
    %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
    %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
    %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
    %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    scf.for %arg0 = %c0 to %c256 step %c32 {
      %subview_3 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_4 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_5 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      %15 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
      %16 = affine.max affine_map<(d0) -> (0, d0)>(%15)
      %17 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
      %18 = affine.max affine_map<(d0) -> (0, d0)>(%17)
      %19 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
      %20 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
      %subview_6 = memref.subview %subview_3[%19, 0] [%16, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_7 = memref.subview %subview_4[0, %20] [32, %18] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_8 = memref.subview %subview_5[%19, %20] [%16, %18] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      %subview_9 = memref.subview %subview_6[0, 0] [%16, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_10 = memref.subview %subview_7[0, 0] [32, %18] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_11 = memref.subview %subview_8[0, 0] [%16, %18] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_9, %subview_10 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_11 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c0 = arith.constant 0 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
    memref.assume_alignment %0, 64 : memref<512x256xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
    memref.assume_alignment %1, 64 : memref<256x1024xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
    memref.assume_alignment %2, 64 : memref<512x1024xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
    %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
    %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
    %7 = gpu.thread_id  x
    %8 = gpu.thread_id  y
    %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
    %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
    %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
    %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
    %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
    %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
    %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    scf.for %arg0 = %c0 to %c256 step %c32 {
      %subview_3 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_4 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_5 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      %15 = affine.max affine_map<(d0) -> (0, d0)>(%9)
      %16 = affine.max affine_map<(d0) -> (0, d0)>(%11)
      %subview_6 = memref.subview %subview_3[%13, 0] [%15, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_7 = memref.subview %subview_4[0, %14] [32, %16] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_8 = memref.subview %subview_5[%13, %14] [%15, %16] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      %subview_9 = memref.subview %subview_6[0, 0] [%15, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_10 = memref.subview %subview_7[0, 0] [32, %16] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_11 = memref.subview %subview_8[0, 0] [%15, %16] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_9, %subview_10 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_11 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    }
    return
  }
}

// -----// IR Dump After FoldMemRefAliasOps (fold-memref-alias-ops) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
  %7 = gpu.thread_id  x
  %8 = gpu.thread_id  y
  %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
  %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
  %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
  %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
  %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
  %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
  %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_3 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_4 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_5 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %15 = affine.max affine_map<(d0) -> (0, d0)>(%9)
    %16 = affine.max affine_map<(d0) -> (0, d0)>(%11)
    %subview_6 = memref.subview %subview_3[%13, 0] [%15, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_7 = memref.subview %subview_4[0, %14] [32, %16] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_8 = memref.subview %subview_5[%13, %14] [%15, %16] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_9 = memref.subview %subview_6[0, 0] [%15, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_10 = memref.subview %subview_7[0, 0] [32, %16] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_11 = memref.subview %subview_8[0, 0] [%15, %16] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_9, %subview_10 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_11 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c0 = arith.constant 0 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
    memref.assume_alignment %0, 64 : memref<512x256xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
    memref.assume_alignment %1, 64 : memref<256x1024xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
    memref.assume_alignment %2, 64 : memref<512x1024xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
    %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
    %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
    %7 = gpu.thread_id  x
    %8 = gpu.thread_id  y
    %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
    %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
    %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
    %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
    %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
    %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
    %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    scf.for %arg0 = %c0 to %c256 step %c32 {
      %subview_3 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_4 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_5 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      %15 = affine.max affine_map<(d0) -> (0, d0)>(%9)
      %16 = affine.max affine_map<(d0) -> (0, d0)>(%11)
      %subview_6 = memref.subview %subview_3[%13, 0] [%15, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_7 = memref.subview %subview_4[0, %14] [32, %16] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_8 = memref.subview %subview_5[%13, %14] [%15, %16] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      %subview_9 = memref.subview %subview_6[0, 0] [%15, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_10 = memref.subview %subview_7[0, 0] [32, %16] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_11 = memref.subview %subview_8[0, 0] [%15, %16] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_9, %subview_10 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_11 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    }
    return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c0 = arith.constant 0 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
    memref.assume_alignment %0, 64 : memref<512x256xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
    memref.assume_alignment %1, 64 : memref<256x1024xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
    memref.assume_alignment %2, 64 : memref<512x1024xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
    %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
    %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
    %7 = gpu.thread_id  x
    %8 = gpu.thread_id  y
    %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
    %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
    %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
    %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
    %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
    %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
    %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    scf.for %arg0 = %c0 to %c256 step %c32 {
      %subview_3 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_4 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_5 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      %15 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
      %16 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
      %subview_6 = memref.subview %subview_3[%13, 0] [%15, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_7 = memref.subview %subview_4[0, %14] [32, %16] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_8 = memref.subview %subview_5[%13, %14] [%15, %16] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      %subview_9 = memref.subview %subview_6[0, 0] [%15, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_10 = memref.subview %subview_7[0, 0] [32, %16] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_11 = memref.subview %subview_8[0, 0] [%15, %16] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_9, %subview_10 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_11 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c0 = arith.constant 0 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
    memref.assume_alignment %0, 64 : memref<512x256xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
    memref.assume_alignment %1, 64 : memref<256x1024xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
    memref.assume_alignment %2, 64 : memref<512x1024xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
    %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
    %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
    %7 = gpu.thread_id  x
    %8 = gpu.thread_id  y
    %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
    %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
    %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
    %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
    %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
    %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
    %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    scf.for %arg0 = %c0 to %c256 step %c32 {
      %subview_3 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_4 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_5 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      %subview_6 = memref.subview %subview_3[%13, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_7 = memref.subview %subview_4[0, %14] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_8 = memref.subview %subview_5[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      %subview_9 = memref.subview %subview_6[0, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_10 = memref.subview %subview_7[0, 0] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_11 = memref.subview %subview_8[0, 0] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_9, %subview_10 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_11 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    }
    return
  }
}

// -----// IR Dump After OptimizeVectorTransfer (iree-codegen-optimize-vector-transfer) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
  %7 = gpu.thread_id  x
  %8 = gpu.thread_id  y
  %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
  %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
  %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
  %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
  %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
  %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
  %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  %subview_3 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_4 = memref.subview %subview_3[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_5 = memref.subview %subview_4[0, 0] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_6 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_7 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_8 = memref.subview %subview_6[%13, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_9 = memref.subview %subview_7[0, %14] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_10 = memref.subview %subview_8[0, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_11 = memref.subview %subview_9[0, 0] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_10, %subview_11 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_5 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  }
  return
}

// -----// IR Dump After GPUPipelining (iree-gpu-pipelining) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
  %7 = gpu.thread_id  x
  %8 = gpu.thread_id  y
  %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
  %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
  %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
  %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
  %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
  %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
  %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  %subview_3 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_4 = memref.subview %subview_3[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_5 = memref.subview %subview_4[0, 0] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_6 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_7 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_8 = memref.subview %subview_6[%13, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_9 = memref.subview %subview_7[0, %14] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_10 = memref.subview %subview_8[0, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_11 = memref.subview %subview_9[0, 0] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_10, %subview_11 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_5 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  }
  return
}

// -----// IR Dump After LLVMGPULowerExecutableTarget (iree-llvmgpu-lower-executable-target) //----- //
hal.executable.variant public @cuda_nvptx_fb, target = <"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}> {
  hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<LLVMGPUMatmulSimt>, workgroup_size = [32 : index, 8 : index, 1 : index]} {
  ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
    %c8 = arith.constant 8 : index
    %c16 = arith.constant 16 : index
    %c1 = arith.constant 1 : index
    hal.return %c8, %c16, %c1 : index, index, index
  }
  builtin.module {
    func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
      %c0 = arith.constant 0 : index
      %c256 = arith.constant 256 : index
      %c32 = arith.constant 32 : index
      %cst = arith.constant 0.000000e+00 : f32
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
      memref.assume_alignment %0, 64 : memref<512x256xf32>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
      memref.assume_alignment %1, 64 : memref<256x1024xf32>
      %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
      memref.assume_alignment %2, 64 : memref<512x1024xf32>
      %workgroup_id_x = hal.interface.workgroup.id[0] : index
      %workgroup_id_y = hal.interface.workgroup.id[1] : index
      %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
      %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
      %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
      %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
      %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
      %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
      %7 = gpu.thread_id  x
      %8 = gpu.thread_id  y
      %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
      %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
      %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
      %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
      %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
      %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
      %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
      %subview_3 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      %subview_4 = memref.subview %subview_3[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      %subview_5 = memref.subview %subview_4[0, 0] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
      scf.for %arg0 = %c0 to %c256 step %c32 {
        %subview_6 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
        %subview_7 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
        %subview_8 = memref.subview %subview_6[%13, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
        %subview_9 = memref.subview %subview_7[0, %14] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
        %subview_10 = memref.subview %subview_8[0, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
        %subview_11 = memref.subview %subview_9[0, 0] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
        linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_10, %subview_11 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_5 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
      }
      return
    }
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c0 = arith.constant 0 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
    memref.assume_alignment %0, 64 : memref<512x256xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
    memref.assume_alignment %1, 64 : memref<256x1024xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
    memref.assume_alignment %2, 64 : memref<512x1024xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
    %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
    %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
    %7 = gpu.thread_id  x
    %8 = gpu.thread_id  y
    %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
    %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
    %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
    %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
    %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
    %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
    %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    %subview_3 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_4 = memref.subview %subview_3[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_5 = memref.subview %subview_4[0, 0] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    scf.for %arg0 = %c0 to %c256 step %c32 {
      %subview_6 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_7 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_8 = memref.subview %subview_6[%13, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_9 = memref.subview %subview_7[0, %14] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_10 = memref.subview %subview_8[0, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_11 = memref.subview %subview_9[0, 0] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_10, %subview_11 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_5 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c0 = arith.constant 0 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
    memref.assume_alignment %0, 64 : memref<512x256xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
    memref.assume_alignment %1, 64 : memref<256x1024xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
    memref.assume_alignment %2, 64 : memref<512x1024xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
    %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
    %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
    %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
    %7 = gpu.thread_id  x
    %8 = gpu.thread_id  y
    %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
    %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
    %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
    %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
    %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
    %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
    %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    %subview_3 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_4 = memref.subview %subview_3[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_5 = memref.subview %subview_4[0, 0] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    scf.for %arg0 = %c0 to %c256 step %c32 {
      %subview_6 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_7 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_8 = memref.subview %subview_6[%13, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_9 = memref.subview %subview_7[0, %14] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_10 = memref.subview %subview_8[0, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_11 = memref.subview %subview_9[0, 0] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_10, %subview_11 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_5 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
    }
    return
  }
}

// -----// IR Dump After LinalgExtToLoops (iree-linalg-ext-to-loops) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
  %7 = gpu.thread_id  x
  %8 = gpu.thread_id  y
  %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
  %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
  %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
  %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
  %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
  %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
  %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  %subview_3 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_4 = memref.subview %subview_3[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_5 = memref.subview %subview_4[0, 0] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_6 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_7 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_8 = memref.subview %subview_6[%13, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_9 = memref.subview %subview_7[0, %14] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_10 = memref.subview %subview_8[0, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_11 = memref.subview %subview_9[0, 0] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_10, %subview_11 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_5 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  }
  return
}

// -----// IR Dump After MemrefCopyToLinalgPass (iree-codegen-memrefcopy-to-linalg) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
  %7 = gpu.thread_id  x
  %8 = gpu.thread_id  y
  %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
  %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
  %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
  %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
  %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
  %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
  %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%cst : f32) outs(%subview_2 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  %subview_3 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_4 = memref.subview %subview_3[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_5 = memref.subview %subview_4[0, 0] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_6 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_7 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_8 = memref.subview %subview_6[%13, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_9 = memref.subview %subview_7[0, %14] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_10 = memref.subview %subview_8[0, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_11 = memref.subview %subview_9[0, 0] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    linalg.matmul {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[32, 128, 32]]>} ins(%subview_10, %subview_11 : memref<?x32xf32, strided<[256, 1], offset: ?>>, memref<32x?xf32, strided<[1024, 1], offset: ?>>) outs(%subview_5 : memref<?x?xf32, strided<[1024, 1], offset: ?>>)
  }
  return
}

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
  %7 = gpu.thread_id  x
  %8 = gpu.thread_id  y
  %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
  %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
  %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
  %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
  %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
  %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
  %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %10 step %c1 {
    scf.for %arg1 = %c0 to %12 step %c1 {
      memref.store %cst, %subview_2[%arg0, %arg1] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
    }
  }
  %subview_3 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_4 = memref.subview %subview_3[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_5 = memref.subview %subview_4[0, 0] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_6 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_7 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_8 = memref.subview %subview_6[%13, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_9 = memref.subview %subview_7[0, %14] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_10 = memref.subview %subview_8[0, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_11 = memref.subview %subview_9[0, 0] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    scf.for %arg1 = %c0 to %10 step %c1 {
      scf.for %arg2 = %c0 to %12 step %c1 {
        scf.for %arg3 = %c0 to %c32 step %c1 {
          %15 = memref.load %subview_10[%arg1, %arg3] : memref<?x32xf32, strided<[256, 1], offset: ?>>
          %16 = memref.load %subview_11[%arg3, %arg2] : memref<32x?xf32, strided<[1024, 1], offset: ?>>
          %17 = memref.load %subview_5[%arg1, %arg2] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
          %18 = arith.mulf %15, %16 : f32
          %19 = arith.addf %17, %18 : f32
          memref.store %19, %subview_5[%arg1, %arg2] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
        }
      }
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
  %7 = gpu.thread_id  x
  %8 = gpu.thread_id  y
  %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
  %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
  %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
  %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
  %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
  %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
  %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %10 step %c1 {
    scf.for %arg1 = %c0 to %12 step %c1 {
      memref.store %cst, %subview_2[%arg0, %arg1] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
    }
  }
  %subview_3 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_4 = memref.subview %subview_3[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_5 = memref.subview %subview_4[0, 0] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_6 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_7 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_8 = memref.subview %subview_6[%13, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_9 = memref.subview %subview_7[0, %14] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_10 = memref.subview %subview_8[0, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_11 = memref.subview %subview_9[0, 0] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    scf.for %arg1 = %c0 to %10 step %c1 {
      scf.for %arg2 = %c0 to %12 step %c1 {
        scf.for %arg3 = %c0 to %c32 step %c1 {
          %15 = memref.load %subview_10[%arg1, %arg3] : memref<?x32xf32, strided<[256, 1], offset: ?>>
          %16 = memref.load %subview_11[%arg3, %arg2] : memref<32x?xf32, strided<[1024, 1], offset: ?>>
          %17 = memref.load %subview_5[%arg1, %arg2] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
          %18 = arith.mulf %15, %16 : f32
          %19 = arith.addf %17, %18 : f32
          memref.store %19, %subview_5[%arg1, %arg2] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
        }
      }
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
  %7 = gpu.thread_id  x
  %8 = gpu.thread_id  y
  %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
  %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
  %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
  %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
  %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
  %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
  %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %10 step %c1 {
    scf.for %arg1 = %c0 to %12 step %c1 {
      memref.store %cst, %subview_2[%arg0, %arg1] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
    }
  }
  %subview_3 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_4 = memref.subview %subview_3[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_5 = memref.subview %subview_4[0, 0] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_6 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_7 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_8 = memref.subview %subview_6[%13, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_9 = memref.subview %subview_7[0, %14] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_10 = memref.subview %subview_8[0, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_11 = memref.subview %subview_9[0, 0] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    scf.for %arg1 = %c0 to %10 step %c1 {
      scf.for %arg2 = %c0 to %12 step %c1 {
        scf.for %arg3 = %c0 to %c32 step %c1 {
          %15 = memref.load %subview_10[%arg1, %arg3] : memref<?x32xf32, strided<[256, 1], offset: ?>>
          %16 = memref.load %subview_11[%arg3, %arg2] : memref<32x?xf32, strided<[1024, 1], offset: ?>>
          %17 = memref.load %subview_5[%arg1, %arg2] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
          %18 = arith.mulf %15, %16 : f32
          %19 = arith.addf %17, %18 : f32
          memref.store %19, %subview_5[%arg1, %arg2] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
        }
      }
    }
  }
  return
}

// -----// IR Dump After PadDynamicAlloc (iree-codegen-pad-dynamic-alloc) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 32)>()[%workgroup_id_y]
  %4 = affine.min affine_map<()[s0] -> (s0 * -32 + 512, 32)>()[%workgroup_id_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.min affine_map<()[s0] -> (s0 * -128 + 1024, 128)>()[%workgroup_id_x]
  %subview = memref.subview %2[%3, %5] [%4, %6] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_0 = memref.subview %0[%3, 0] [%4, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %5] [256, %6] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
  %7 = gpu.thread_id  x
  %8 = gpu.thread_id  y
  %9 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 8), s0 ceildiv 8)>()[%4, %8]
  %10 = affine.max affine_map<()[s0] -> (0, s0)>()[%9]
  %11 = affine.min affine_map<()[s0, s1] -> (s0 - s1 * (s0 ceildiv 32), s0 ceildiv 32)>()[%6, %7]
  %12 = affine.max affine_map<()[s0] -> (0, s0)>()[%11]
  %13 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 8))>()[%4, %8]
  %14 = affine.apply affine_map<()[s0, s1] -> (s1 * (s0 ceildiv 32))>()[%6, %7]
  %subview_2 = memref.subview %subview[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %10 step %c1 {
    scf.for %arg1 = %c0 to %12 step %c1 {
      memref.store %cst, %subview_2[%arg0, %arg1] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
    }
  }
  %subview_3 = memref.subview %subview[0, 0] [%4, %6] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_4 = memref.subview %subview_3[%13, %14] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  %subview_5 = memref.subview %subview_4[0, 0] [%10, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c32 {
    %subview_6 = memref.subview %subview_0[0, %arg0] [%4, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_7 = memref.subview %subview_1[%arg0, 0] [32, %6] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_8 = memref.subview %subview_6[%13, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_9 = memref.subview %subview_7[0, %14] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    %subview_10 = memref.subview %subview_8[0, 0] [%10, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
    %subview_11 = memref.subview %subview_9[0, 0] [32, %12] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
    scf.for %arg1 = %c0 to %10 step %c1 {
      scf.for %arg2 = %c0 to %12 step %c1 {
        scf.for %arg3 = %c0 to %c32 step %c1 {
          %15 = memref.load %subview_10[%arg1, %arg3] : memref<?x32xf32, strided<[256, 1], offset: ?>>
          %16 = memref.load %subview_11[%arg3, %arg2] : memref<32x?xf32, strided<[1024, 1], offset: ?>>
          %17 = memref.load %subview_5[%arg1, %arg2] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
          %18 = arith.mulf %15, %16 : f32
          %19 = arith.addf %17, %18 : f32
          memref.store %19, %subview_5[%arg1, %arg2] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
        }
      }
    }
  }
  return
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
    memref.assume_alignment %0, 64 : memref<512x256xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
    memref.assume_alignment %1, 64 : memref<256x1024xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
    memref.assume_alignment %2, 64 : memref<512x1024xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %c32_0 = arith.constant 32 : index
    %3 = arith.muli %workgroup_id_y, %c32_0 : index
    %c-32 = arith.constant -32 : index
    %4 = arith.muli %workgroup_id_y, %c-32 : index
    %c512 = arith.constant 512 : index
    %5 = arith.addi %4, %c512 : index
    %c32_1 = arith.constant 32 : index
    %6 = arith.cmpi slt, %5, %c32_1 : index
    %7 = arith.select %6, %5, %c32_1 : index
    %c128 = arith.constant 128 : index
    %8 = arith.muli %workgroup_id_x, %c128 : index
    %c-128 = arith.constant -128 : index
    %9 = arith.muli %workgroup_id_x, %c-128 : index
    %c1024 = arith.constant 1024 : index
    %10 = arith.addi %9, %c1024 : index
    %c128_2 = arith.constant 128 : index
    %11 = arith.cmpi slt, %10, %c128_2 : index
    %12 = arith.select %11, %10, %c128_2 : index
    %subview = memref.subview %2[%3, %8] [%7, %12] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_3 = memref.subview %0[%3, 0] [%7, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
    %subview_4 = memref.subview %1[0, %8] [256, %12] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
    %13 = gpu.thread_id  x
    %14 = gpu.thread_id  y
    %c8 = arith.constant 8 : index
    %c0_5 = arith.constant 0 : index
    %c1_6 = arith.constant 1 : index
    %15 = arith.cmpi sle, %7, %c0_5 : index
    %16 = arith.subi %c0_5, %7 : index
    %17 = arith.subi %7, %c1_6 : index
    %18 = arith.select %15, %16, %17 : index
    %19 = arith.divsi %18, %c8 : index
    %20 = arith.subi %c0_5, %19 : index
    %21 = arith.addi %19, %c1_6 : index
    %22 = arith.select %15, %20, %21 : index
    %23 = arith.muli %14, %22 : index
    %c-1 = arith.constant -1 : index
    %24 = arith.muli %23, %c-1 : index
    %25 = arith.addi %7, %24 : index
    %c8_7 = arith.constant 8 : index
    %c0_8 = arith.constant 0 : index
    %c1_9 = arith.constant 1 : index
    %26 = arith.cmpi sle, %7, %c0_8 : index
    %27 = arith.subi %c0_8, %7 : index
    %28 = arith.subi %7, %c1_9 : index
    %29 = arith.select %26, %27, %28 : index
    %30 = arith.divsi %29, %c8_7 : index
    %31 = arith.subi %c0_8, %30 : index
    %32 = arith.addi %30, %c1_9 : index
    %33 = arith.select %26, %31, %32 : index
    %34 = arith.cmpi slt, %25, %33 : index
    %35 = arith.select %34, %25, %33 : index
    %c0_10 = arith.constant 0 : index
    %36 = arith.cmpi sgt, %c0_10, %35 : index
    %37 = arith.select %36, %c0_10, %35 : index
    %c32_11 = arith.constant 32 : index
    %c0_12 = arith.constant 0 : index
    %c1_13 = arith.constant 1 : index
    %38 = arith.cmpi sle, %12, %c0_12 : index
    %39 = arith.subi %c0_12, %12 : index
    %40 = arith.subi %12, %c1_13 : index
    %41 = arith.select %38, %39, %40 : index
    %42 = arith.divsi %41, %c32_11 : index
    %43 = arith.subi %c0_12, %42 : index
    %44 = arith.addi %42, %c1_13 : index
    %45 = arith.select %38, %43, %44 : index
    %46 = arith.muli %13, %45 : index
    %c-1_14 = arith.constant -1 : index
    %47 = arith.muli %46, %c-1_14 : index
    %48 = arith.addi %12, %47 : index
    %c32_15 = arith.constant 32 : index
    %c0_16 = arith.constant 0 : index
    %c1_17 = arith.constant 1 : index
    %49 = arith.cmpi sle, %12, %c0_16 : index
    %50 = arith.subi %c0_16, %12 : index
    %51 = arith.subi %12, %c1_17 : index
    %52 = arith.select %49, %50, %51 : index
    %53 = arith.divsi %52, %c32_15 : index
    %54 = arith.subi %c0_16, %53 : index
    %55 = arith.addi %53, %c1_17 : index
    %56 = arith.select %49, %54, %55 : index
    %57 = arith.cmpi slt, %48, %56 : index
    %58 = arith.select %57, %48, %56 : index
    %c0_18 = arith.constant 0 : index
    %59 = arith.cmpi sgt, %c0_18, %58 : index
    %60 = arith.select %59, %c0_18, %58 : index
    %c8_19 = arith.constant 8 : index
    %c0_20 = arith.constant 0 : index
    %c1_21 = arith.constant 1 : index
    %61 = arith.cmpi sle, %7, %c0_20 : index
    %62 = arith.subi %c0_20, %7 : index
    %63 = arith.subi %7, %c1_21 : index
    %64 = arith.select %61, %62, %63 : index
    %65 = arith.divsi %64, %c8_19 : index
    %66 = arith.subi %c0_20, %65 : index
    %67 = arith.addi %65, %c1_21 : index
    %68 = arith.select %61, %66, %67 : index
    %69 = arith.muli %14, %68 : index
    %c32_22 = arith.constant 32 : index
    %c0_23 = arith.constant 0 : index
    %c1_24 = arith.constant 1 : index
    %70 = arith.cmpi sle, %12, %c0_23 : index
    %71 = arith.subi %c0_23, %12 : index
    %72 = arith.subi %12, %c1_24 : index
    %73 = arith.select %70, %71, %72 : index
    %74 = arith.divsi %73, %c32_22 : index
    %75 = arith.subi %c0_23, %74 : index
    %76 = arith.addi %74, %c1_24 : index
    %77 = arith.select %70, %75, %76 : index
    %78 = arith.muli %13, %77 : index
    %subview_25 = memref.subview %subview[%69, %78] [%37, %60] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    scf.for %arg0 = %c0 to %37 step %c1 {
      scf.for %arg1 = %c0 to %60 step %c1 {
        memref.store %cst, %subview_25[%arg0, %arg1] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
      }
    }
    %subview_26 = memref.subview %subview[0, 0] [%7, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_27 = memref.subview %subview_26[%69, %78] [%37, %60] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_28 = memref.subview %subview_27[0, 0] [%37, %60] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    scf.for %arg0 = %c0 to %c256 step %c32 {
      %subview_29 = memref.subview %subview_3[0, %arg0] [%7, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_30 = memref.subview %subview_4[%arg0, 0] [32, %12] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_31 = memref.subview %subview_29[%69, 0] [%37, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_32 = memref.subview %subview_30[0, %78] [32, %60] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_33 = memref.subview %subview_31[0, 0] [%37, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_34 = memref.subview %subview_32[0, 0] [32, %60] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      scf.for %arg1 = %c0 to %37 step %c1 {
        scf.for %arg2 = %c0 to %60 step %c1 {
          scf.for %arg3 = %c0 to %c32 step %c1 {
            %79 = memref.load %subview_33[%arg1, %arg3] : memref<?x32xf32, strided<[256, 1], offset: ?>>
            %80 = memref.load %subview_34[%arg3, %arg2] : memref<32x?xf32, strided<[1024, 1], offset: ?>>
            %81 = memref.load %subview_28[%arg1, %arg2] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
            %82 = arith.mulf %79, %80 : f32
            %83 = arith.addf %81, %82 : f32
            memref.store %83, %subview_28[%arg1, %arg2] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
          }
        }
      }
    }
    return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c-1 = arith.constant -1 : index
    %c8 = arith.constant 8 : index
    %c1024 = arith.constant 1024 : index
    %c-128 = arith.constant -128 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %c-32 = arith.constant -32 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
    memref.assume_alignment %0, 64 : memref<512x256xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
    memref.assume_alignment %1, 64 : memref<256x1024xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
    memref.assume_alignment %2, 64 : memref<512x1024xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = arith.muli %workgroup_id_y, %c32 : index
    %4 = arith.muli %workgroup_id_y, %c-32 : index
    %5 = arith.addi %4, %c512 : index
    %6 = arith.cmpi slt, %5, %c32 : index
    %7 = arith.select %6, %5, %c32 : index
    %8 = arith.muli %workgroup_id_x, %c128 : index
    %9 = arith.muli %workgroup_id_x, %c-128 : index
    %10 = arith.addi %9, %c1024 : index
    %11 = arith.cmpi slt, %10, %c128 : index
    %12 = arith.select %11, %10, %c128 : index
    %subview = memref.subview %2[%3, %8] [%7, %12] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_0 = memref.subview %0[%3, 0] [%7, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
    %subview_1 = memref.subview %1[0, %8] [256, %12] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
    %13 = gpu.thread_id  x
    %14 = gpu.thread_id  y
    %15 = arith.cmpi sle, %7, %c0 : index
    %16 = arith.subi %c0, %7 : index
    %17 = arith.subi %7, %c1 : index
    %18 = arith.select %15, %16, %17 : index
    %19 = arith.divsi %18, %c8 : index
    %20 = arith.subi %c0, %19 : index
    %21 = arith.addi %19, %c1 : index
    %22 = arith.select %15, %20, %21 : index
    %23 = arith.muli %14, %22 : index
    %24 = arith.muli %23, %c-1 : index
    %25 = arith.addi %7, %24 : index
    %26 = arith.cmpi sle, %7, %c0 : index
    %27 = arith.subi %c0, %7 : index
    %28 = arith.subi %7, %c1 : index
    %29 = arith.select %26, %27, %28 : index
    %30 = arith.divsi %29, %c8 : index
    %31 = arith.subi %c0, %30 : index
    %32 = arith.addi %30, %c1 : index
    %33 = arith.select %26, %31, %32 : index
    %34 = arith.cmpi slt, %25, %33 : index
    %35 = arith.select %34, %25, %33 : index
    %36 = arith.cmpi slt, %35, %c0 : index
    %37 = arith.select %36, %c0, %35 : index
    %38 = arith.cmpi sle, %12, %c0 : index
    %39 = arith.subi %c0, %12 : index
    %40 = arith.subi %12, %c1 : index
    %41 = arith.select %38, %39, %40 : index
    %42 = arith.divsi %41, %c32 : index
    %43 = arith.subi %c0, %42 : index
    %44 = arith.addi %42, %c1 : index
    %45 = arith.select %38, %43, %44 : index
    %46 = arith.muli %13, %45 : index
    %47 = arith.muli %46, %c-1 : index
    %48 = arith.addi %12, %47 : index
    %49 = arith.cmpi sle, %12, %c0 : index
    %50 = arith.subi %c0, %12 : index
    %51 = arith.subi %12, %c1 : index
    %52 = arith.select %49, %50, %51 : index
    %53 = arith.divsi %52, %c32 : index
    %54 = arith.subi %c0, %53 : index
    %55 = arith.addi %53, %c1 : index
    %56 = arith.select %49, %54, %55 : index
    %57 = arith.cmpi slt, %48, %56 : index
    %58 = arith.select %57, %48, %56 : index
    %59 = arith.cmpi slt, %58, %c0 : index
    %60 = arith.select %59, %c0, %58 : index
    %61 = arith.cmpi sle, %7, %c0 : index
    %62 = arith.subi %c0, %7 : index
    %63 = arith.subi %7, %c1 : index
    %64 = arith.select %61, %62, %63 : index
    %65 = arith.divsi %64, %c8 : index
    %66 = arith.subi %c0, %65 : index
    %67 = arith.addi %65, %c1 : index
    %68 = arith.select %61, %66, %67 : index
    %69 = arith.muli %14, %68 : index
    %70 = arith.cmpi sle, %12, %c0 : index
    %71 = arith.subi %c0, %12 : index
    %72 = arith.subi %12, %c1 : index
    %73 = arith.select %70, %71, %72 : index
    %74 = arith.divsi %73, %c32 : index
    %75 = arith.subi %c0, %74 : index
    %76 = arith.addi %74, %c1 : index
    %77 = arith.select %70, %75, %76 : index
    %78 = arith.muli %13, %77 : index
    %subview_2 = memref.subview %subview[%69, %78] [%37, %60] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    scf.for %arg0 = %c0 to %37 step %c1 {
      scf.for %arg1 = %c0 to %60 step %c1 {
        memref.store %cst, %subview_2[%arg0, %arg1] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
      }
    }
    %subview_3 = memref.subview %subview[0, 0] [%7, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_4 = memref.subview %subview_3[%69, %78] [%37, %60] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_5 = memref.subview %subview_4[0, 0] [%37, %60] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    scf.for %arg0 = %c0 to %c256 step %c32 {
      %subview_6 = memref.subview %subview_0[0, %arg0] [%7, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_7 = memref.subview %subview_1[%arg0, 0] [32, %12] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_8 = memref.subview %subview_6[%69, 0] [%37, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_9 = memref.subview %subview_7[0, %78] [32, %60] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_10 = memref.subview %subview_8[0, 0] [%37, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_11 = memref.subview %subview_9[0, 0] [32, %60] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      scf.for %arg1 = %c0 to %37 step %c1 {
        scf.for %arg2 = %c0 to %60 step %c1 {
          scf.for %arg3 = %c0 to %c32 step %c1 {
            %79 = memref.load %subview_10[%arg1, %arg3] : memref<?x32xf32, strided<[256, 1], offset: ?>>
            %80 = memref.load %subview_11[%arg3, %arg2] : memref<32x?xf32, strided<[1024, 1], offset: ?>>
            %81 = memref.load %subview_5[%arg1, %arg2] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
            %82 = arith.mulf %79, %80 : f32
            %83 = arith.addf %81, %82 : f32
            memref.store %83, %subview_5[%arg1, %arg2] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
          }
        }
      }
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c-1 = arith.constant -1 : index
    %c8 = arith.constant 8 : index
    %c1024 = arith.constant 1024 : index
    %c-128 = arith.constant -128 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %c-32 = arith.constant -32 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
    memref.assume_alignment %0, 64 : memref<512x256xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
    memref.assume_alignment %1, 64 : memref<256x1024xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
    memref.assume_alignment %2, 64 : memref<512x1024xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = arith.muli %workgroup_id_y, %c32 : index
    %4 = arith.muli %workgroup_id_y, %c-32 : index
    %5 = arith.addi %4, %c512 : index
    %6 = arith.cmpi slt, %5, %c32 : index
    %7 = arith.select %6, %5, %c32 : index
    %8 = arith.muli %workgroup_id_x, %c128 : index
    %9 = arith.muli %workgroup_id_x, %c-128 : index
    %10 = arith.addi %9, %c1024 : index
    %11 = arith.cmpi slt, %10, %c128 : index
    %12 = arith.select %11, %10, %c128 : index
    %subview = memref.subview %2[%3, %8] [%7, %12] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_0 = memref.subview %0[%3, 0] [%7, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
    %subview_1 = memref.subview %1[0, %8] [256, %12] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
    %13 = gpu.thread_id  x
    %14 = gpu.thread_id  y
    %15 = arith.cmpi sle, %7, %c0 : index
    %16 = arith.subi %c0, %7 : index
    %17 = arith.subi %7, %c1 : index
    %18 = arith.select %15, %16, %17 : index
    %19 = arith.divsi %18, %c8 : index
    %20 = arith.subi %c0, %19 : index
    %21 = arith.addi %19, %c1 : index
    %22 = arith.select %15, %20, %21 : index
    %23 = arith.muli %14, %22 : index
    %24 = arith.muli %23, %c-1 : index
    %25 = arith.addi %7, %24 : index
    %26 = arith.cmpi slt, %25, %22 : index
    %27 = arith.select %26, %25, %22 : index
    %28 = arith.cmpi slt, %27, %c0 : index
    %29 = arith.select %28, %c0, %27 : index
    %30 = arith.cmpi sle, %12, %c0 : index
    %31 = arith.subi %c0, %12 : index
    %32 = arith.subi %12, %c1 : index
    %33 = arith.select %30, %31, %32 : index
    %34 = arith.divsi %33, %c32 : index
    %35 = arith.subi %c0, %34 : index
    %36 = arith.addi %34, %c1 : index
    %37 = arith.select %30, %35, %36 : index
    %38 = arith.muli %13, %37 : index
    %39 = arith.muli %38, %c-1 : index
    %40 = arith.addi %12, %39 : index
    %41 = arith.cmpi slt, %40, %37 : index
    %42 = arith.select %41, %40, %37 : index
    %43 = arith.cmpi slt, %42, %c0 : index
    %44 = arith.select %43, %c0, %42 : index
    %subview_2 = memref.subview %subview[%23, %38] [%29, %44] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    scf.for %arg0 = %c0 to %29 step %c1 {
      scf.for %arg1 = %c0 to %44 step %c1 {
        memref.store %cst, %subview_2[%arg0, %arg1] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
      }
    }
    %subview_3 = memref.subview %subview[0, 0] [%7, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_4 = memref.subview %subview_3[%23, %38] [%29, %44] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_5 = memref.subview %subview_4[0, 0] [%29, %44] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    scf.for %arg0 = %c0 to %c256 step %c32 {
      %subview_6 = memref.subview %subview_0[0, %arg0] [%7, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_7 = memref.subview %subview_1[%arg0, 0] [32, %12] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_8 = memref.subview %subview_6[%23, 0] [%29, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_9 = memref.subview %subview_7[0, %38] [32, %44] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_10 = memref.subview %subview_8[0, 0] [%29, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_11 = memref.subview %subview_9[0, 0] [32, %44] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      scf.for %arg1 = %c0 to %29 step %c1 {
        scf.for %arg2 = %c0 to %44 step %c1 {
          scf.for %arg3 = %c0 to %c32 step %c1 {
            %45 = memref.load %subview_10[%arg1, %arg3] : memref<?x32xf32, strided<[256, 1], offset: ?>>
            %46 = memref.load %subview_11[%arg3, %arg2] : memref<32x?xf32, strided<[1024, 1], offset: ?>>
            %47 = memref.load %subview_5[%arg1, %arg2] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
            %48 = arith.mulf %45, %46 : f32
            %49 = arith.addf %47, %48 : f32
            memref.store %49, %subview_5[%arg1, %arg2] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
          }
        }
      }
    }
    return
  }
}

// -----// IR Dump After ArithBufferize (arith-bufferize) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c-1 = arith.constant -1 : index
    %c8 = arith.constant 8 : index
    %c1024 = arith.constant 1024 : index
    %c-128 = arith.constant -128 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %c-32 = arith.constant -32 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
    memref.assume_alignment %0, 64 : memref<512x256xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
    memref.assume_alignment %1, 64 : memref<256x1024xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
    memref.assume_alignment %2, 64 : memref<512x1024xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = arith.muli %workgroup_id_y, %c32 : index
    %4 = arith.muli %workgroup_id_y, %c-32 : index
    %5 = arith.addi %4, %c512 : index
    %6 = arith.cmpi slt, %5, %c32 : index
    %7 = arith.select %6, %5, %c32 : index
    %8 = arith.muli %workgroup_id_x, %c128 : index
    %9 = arith.muli %workgroup_id_x, %c-128 : index
    %10 = arith.addi %9, %c1024 : index
    %11 = arith.cmpi slt, %10, %c128 : index
    %12 = arith.select %11, %10, %c128 : index
    %subview = memref.subview %2[%3, %8] [%7, %12] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_0 = memref.subview %0[%3, 0] [%7, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
    %subview_1 = memref.subview %1[0, %8] [256, %12] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
    %13 = gpu.thread_id  x
    %14 = gpu.thread_id  y
    %15 = arith.cmpi sle, %7, %c0 : index
    %16 = arith.subi %c0, %7 : index
    %17 = arith.subi %7, %c1 : index
    %18 = arith.select %15, %16, %17 : index
    %19 = arith.divsi %18, %c8 : index
    %20 = arith.subi %c0, %19 : index
    %21 = arith.addi %19, %c1 : index
    %22 = arith.select %15, %20, %21 : index
    %23 = arith.muli %14, %22 : index
    %24 = arith.muli %23, %c-1 : index
    %25 = arith.addi %7, %24 : index
    %26 = arith.cmpi slt, %25, %22 : index
    %27 = arith.select %26, %25, %22 : index
    %28 = arith.cmpi slt, %27, %c0 : index
    %29 = arith.select %28, %c0, %27 : index
    %30 = arith.cmpi sle, %12, %c0 : index
    %31 = arith.subi %c0, %12 : index
    %32 = arith.subi %12, %c1 : index
    %33 = arith.select %30, %31, %32 : index
    %34 = arith.divsi %33, %c32 : index
    %35 = arith.subi %c0, %34 : index
    %36 = arith.addi %34, %c1 : index
    %37 = arith.select %30, %35, %36 : index
    %38 = arith.muli %13, %37 : index
    %39 = arith.muli %38, %c-1 : index
    %40 = arith.addi %12, %39 : index
    %41 = arith.cmpi slt, %40, %37 : index
    %42 = arith.select %41, %40, %37 : index
    %43 = arith.cmpi slt, %42, %c0 : index
    %44 = arith.select %43, %c0, %42 : index
    %subview_2 = memref.subview %subview[%23, %38] [%29, %44] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    scf.for %arg0 = %c0 to %29 step %c1 {
      scf.for %arg1 = %c0 to %44 step %c1 {
        memref.store %cst, %subview_2[%arg0, %arg1] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
      }
    }
    %subview_3 = memref.subview %subview[0, 0] [%7, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_4 = memref.subview %subview_3[%23, %38] [%29, %44] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_5 = memref.subview %subview_4[0, 0] [%29, %44] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    scf.for %arg0 = %c0 to %c256 step %c32 {
      %subview_6 = memref.subview %subview_0[0, %arg0] [%7, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_7 = memref.subview %subview_1[%arg0, 0] [32, %12] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_8 = memref.subview %subview_6[%23, 0] [%29, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_9 = memref.subview %subview_7[0, %38] [32, %44] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_10 = memref.subview %subview_8[0, 0] [%29, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_11 = memref.subview %subview_9[0, 0] [32, %44] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      scf.for %arg1 = %c0 to %29 step %c1 {
        scf.for %arg2 = %c0 to %44 step %c1 {
          scf.for %arg3 = %c0 to %c32 step %c1 {
            %45 = memref.load %subview_10[%arg1, %arg3] : memref<?x32xf32, strided<[256, 1], offset: ?>>
            %46 = memref.load %subview_11[%arg3, %arg2] : memref<32x?xf32, strided<[1024, 1], offset: ?>>
            %47 = memref.load %subview_5[%arg1, %arg2] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
            %48 = arith.mulf %45, %46 : f32
            %49 = arith.addf %47, %48 : f32
            memref.store %49, %subview_5[%arg1, %arg2] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
          }
        }
      }
    }
    return
  }
}

// -----// IR Dump After FoldTensorExtractOp (iree-codegen-fold-tensor-extract-op) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c-1 = arith.constant -1 : index
    %c8 = arith.constant 8 : index
    %c1024 = arith.constant 1024 : index
    %c-128 = arith.constant -128 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %c-32 = arith.constant -32 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
    memref.assume_alignment %0, 64 : memref<512x256xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
    memref.assume_alignment %1, 64 : memref<256x1024xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
    memref.assume_alignment %2, 64 : memref<512x1024xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = arith.muli %workgroup_id_y, %c32 : index
    %4 = arith.muli %workgroup_id_y, %c-32 : index
    %5 = arith.addi %4, %c512 : index
    %6 = arith.cmpi slt, %5, %c32 : index
    %7 = arith.select %6, %5, %c32 : index
    %8 = arith.muli %workgroup_id_x, %c128 : index
    %9 = arith.muli %workgroup_id_x, %c-128 : index
    %10 = arith.addi %9, %c1024 : index
    %11 = arith.cmpi slt, %10, %c128 : index
    %12 = arith.select %11, %10, %c128 : index
    %subview = memref.subview %2[%3, %8] [%7, %12] [1, 1] : memref<512x1024xf32> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_0 = memref.subview %0[%3, 0] [%7, 256] [1, 1] : memref<512x256xf32> to memref<?x256xf32, strided<[256, 1], offset: ?>>
    %subview_1 = memref.subview %1[0, %8] [256, %12] [1, 1] : memref<256x1024xf32> to memref<256x?xf32, strided<[1024, 1], offset: ?>>
    %13 = gpu.thread_id  x
    %14 = gpu.thread_id  y
    %15 = arith.cmpi sle, %7, %c0 : index
    %16 = arith.subi %c0, %7 : index
    %17 = arith.subi %7, %c1 : index
    %18 = arith.select %15, %16, %17 : index
    %19 = arith.divsi %18, %c8 : index
    %20 = arith.subi %c0, %19 : index
    %21 = arith.addi %19, %c1 : index
    %22 = arith.select %15, %20, %21 : index
    %23 = arith.muli %14, %22 : index
    %24 = arith.muli %23, %c-1 : index
    %25 = arith.addi %7, %24 : index
    %26 = arith.cmpi slt, %25, %22 : index
    %27 = arith.select %26, %25, %22 : index
    %28 = arith.cmpi slt, %27, %c0 : index
    %29 = arith.select %28, %c0, %27 : index
    %30 = arith.cmpi sle, %12, %c0 : index
    %31 = arith.subi %c0, %12 : index
    %32 = arith.subi %12, %c1 : index
    %33 = arith.select %30, %31, %32 : index
    %34 = arith.divsi %33, %c32 : index
    %35 = arith.subi %c0, %34 : index
    %36 = arith.addi %34, %c1 : index
    %37 = arith.select %30, %35, %36 : index
    %38 = arith.muli %13, %37 : index
    %39 = arith.muli %38, %c-1 : index
    %40 = arith.addi %12, %39 : index
    %41 = arith.cmpi slt, %40, %37 : index
    %42 = arith.select %41, %40, %37 : index
    %43 = arith.cmpi slt, %42, %c0 : index
    %44 = arith.select %43, %c0, %42 : index
    %subview_2 = memref.subview %subview[%23, %38] [%29, %44] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    scf.for %arg0 = %c0 to %29 step %c1 {
      scf.for %arg1 = %c0 to %44 step %c1 {
        memref.store %cst, %subview_2[%arg0, %arg1] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
      }
    }
    %subview_3 = memref.subview %subview[0, 0] [%7, %12] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_4 = memref.subview %subview_3[%23, %38] [%29, %44] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    %subview_5 = memref.subview %subview_4[0, 0] [%29, %44] [1, 1] : memref<?x?xf32, strided<[1024, 1], offset: ?>> to memref<?x?xf32, strided<[1024, 1], offset: ?>>
    scf.for %arg0 = %c0 to %c256 step %c32 {
      %subview_6 = memref.subview %subview_0[0, %arg0] [%7, 32] [1, 1] : memref<?x256xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_7 = memref.subview %subview_1[%arg0, 0] [32, %12] [1, 1] : memref<256x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_8 = memref.subview %subview_6[%23, 0] [%29, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_9 = memref.subview %subview_7[0, %38] [32, %44] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      %subview_10 = memref.subview %subview_8[0, 0] [%29, 32] [1, 1] : memref<?x32xf32, strided<[256, 1], offset: ?>> to memref<?x32xf32, strided<[256, 1], offset: ?>>
      %subview_11 = memref.subview %subview_9[0, 0] [32, %44] [1, 1] : memref<32x?xf32, strided<[1024, 1], offset: ?>> to memref<32x?xf32, strided<[1024, 1], offset: ?>>
      scf.for %arg1 = %c0 to %29 step %c1 {
        scf.for %arg2 = %c0 to %44 step %c1 {
          scf.for %arg3 = %c0 to %c32 step %c1 {
            %45 = memref.load %subview_10[%arg1, %arg3] : memref<?x32xf32, strided<[256, 1], offset: ?>>
            %46 = memref.load %subview_11[%arg3, %arg2] : memref<32x?xf32, strided<[1024, 1], offset: ?>>
            %47 = memref.load %subview_5[%arg1, %arg2] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
            %48 = arith.mulf %45, %46 : f32
            %49 = arith.addf %47, %48 : f32
            memref.store %49, %subview_5[%arg1, %arg2] : memref<?x?xf32, strided<[1024, 1], offset: ?>>
          }
        }
      }
    }
    return
  }
}

// -----// IR Dump After LLVMGPUVectorLowering (iree-llvmgpu-vector-lowering) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1024 = arith.constant 1024 : index
  %c-128 = arith.constant -128 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c-32 = arith.constant -32 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = arith.muli %workgroup_id_y, %c32 : index
  %4 = arith.muli %workgroup_id_y, %c-32 : index
  %5 = arith.addi %4, %c512 : index
  %6 = arith.cmpi slt, %5, %c32 : index
  %7 = arith.select %6, %5, %c32 : index
  %8 = arith.muli %workgroup_id_x, %c128 : index
  %9 = arith.muli %workgroup_id_x, %c-128 : index
  %10 = arith.addi %9, %c1024 : index
  %11 = arith.cmpi slt, %10, %c128 : index
  %12 = arith.select %11, %10, %c128 : index
  %13 = gpu.thread_id  x
  %14 = gpu.thread_id  y
  %15 = arith.cmpi sle, %7, %c0 : index
  %16 = arith.subi %c0, %7 : index
  %17 = arith.subi %7, %c1 : index
  %18 = arith.select %15, %16, %17 : index
  %19 = arith.divsi %18, %c8 : index
  %20 = arith.subi %c0, %19 : index
  %21 = arith.addi %19, %c1 : index
  %22 = arith.select %15, %20, %21 : index
  %23 = arith.muli %14, %22 : index
  %24 = arith.muli %23, %c-1 : index
  %25 = arith.addi %7, %24 : index
  %26 = arith.cmpi slt, %25, %22 : index
  %27 = arith.select %26, %25, %22 : index
  %28 = arith.cmpi slt, %27, %c0 : index
  %29 = arith.select %28, %c0, %27 : index
  %30 = arith.cmpi sle, %12, %c0 : index
  %31 = arith.subi %c0, %12 : index
  %32 = arith.subi %12, %c1 : index
  %33 = arith.select %30, %31, %32 : index
  %34 = arith.divsi %33, %c32 : index
  %35 = arith.subi %c0, %34 : index
  %36 = arith.addi %34, %c1 : index
  %37 = arith.select %30, %35, %36 : index
  %38 = arith.muli %13, %37 : index
  %39 = arith.muli %38, %c-1 : index
  %40 = arith.addi %12, %39 : index
  %41 = arith.cmpi slt, %40, %37 : index
  %42 = arith.select %41, %40, %37 : index
  %43 = arith.cmpi slt, %42, %c0 : index
  %44 = arith.select %43, %c0, %42 : index
  scf.for %arg0 = %c0 to %29 step %c1 {
    scf.for %arg1 = %c0 to %44 step %c1 {
      %45 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg0)[%23]
      %46 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg1)[%38]
      %47 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%45)[%3]
      %48 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%46)[%8]
      memref.store %cst, %2[%47, %48] : memref<512x1024xf32>
    }
  }
  scf.for %arg0 = %c0 to %c256 step %c32 {
    scf.for %arg1 = %c0 to %29 step %c1 {
      scf.for %arg2 = %c0 to %44 step %c1 {
        scf.for %arg3 = %c0 to %c32 step %c1 {
          %45 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg1)[%23]
          %46 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg3)[%arg0]
          %47 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%45)[%3]
          %48 = memref.load %0[%47, %46] : memref<512x256xf32>
          %49 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg2)[%38]
          %50 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg3)[%arg0]
          %51 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%49)[%8]
          %52 = memref.load %1[%50, %51] : memref<256x1024xf32>
          %53 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg1)[%23]
          %54 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg2)[%38]
          %55 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%53)[%3]
          %56 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%54)[%8]
          %57 = memref.load %2[%55, %56] : memref<512x1024xf32>
          %58 = arith.mulf %48, %52 : f32
          %59 = arith.addf %57, %58 : f32
          %60 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg1)[%23]
          %61 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg2)[%38]
          %62 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%60)[%3]
          %63 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%61)[%8]
          memref.store %59, %2[%62, %63] : memref<512x1024xf32>
        }
      }
    }
  }
  return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1024 = arith.constant 1024 : index
  %c-128 = arith.constant -128 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c-32 = arith.constant -32 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = arith.muli %workgroup_id_y, %c32 : index
  %4 = arith.muli %workgroup_id_y, %c-32 : index
  %5 = arith.addi %4, %c512 : index
  %6 = arith.cmpi slt, %5, %c32 : index
  %7 = arith.select %6, %5, %c32 : index
  %8 = arith.muli %workgroup_id_x, %c128 : index
  %9 = arith.muli %workgroup_id_x, %c-128 : index
  %10 = arith.addi %9, %c1024 : index
  %11 = arith.cmpi slt, %10, %c128 : index
  %12 = arith.select %11, %10, %c128 : index
  %13 = gpu.thread_id  x
  %14 = gpu.thread_id  y
  %15 = arith.cmpi sle, %7, %c0 : index
  %16 = arith.subi %c0, %7 : index
  %17 = arith.subi %7, %c1 : index
  %18 = arith.select %15, %16, %17 : index
  %19 = arith.divsi %18, %c8 : index
  %20 = arith.subi %c0, %19 : index
  %21 = arith.addi %19, %c1 : index
  %22 = arith.select %15, %20, %21 : index
  %23 = arith.muli %14, %22 : index
  %24 = arith.muli %23, %c-1 : index
  %25 = arith.addi %7, %24 : index
  %26 = arith.cmpi slt, %25, %22 : index
  %27 = arith.select %26, %25, %22 : index
  %28 = arith.cmpi slt, %27, %c0 : index
  %29 = arith.select %28, %c0, %27 : index
  %30 = arith.cmpi sle, %12, %c0 : index
  %31 = arith.subi %c0, %12 : index
  %32 = arith.subi %12, %c1 : index
  %33 = arith.select %30, %31, %32 : index
  %34 = arith.divsi %33, %c32 : index
  %35 = arith.subi %c0, %34 : index
  %36 = arith.addi %34, %c1 : index
  %37 = arith.select %30, %35, %36 : index
  %38 = arith.muli %13, %37 : index
  %39 = arith.muli %38, %c-1 : index
  %40 = arith.addi %12, %39 : index
  %41 = arith.cmpi slt, %40, %37 : index
  %42 = arith.select %41, %40, %37 : index
  %43 = arith.cmpi slt, %42, %c0 : index
  %44 = arith.select %43, %c0, %42 : index
  cf.br ^bb1(%c0 : index)
^bb1(%45: index):  // 2 preds: ^bb0, ^bb5
  %46 = arith.cmpi slt, %45, %29 : index
  cf.cond_br %46, ^bb2, ^bb6
^bb2:  // pred: ^bb1
  cf.br ^bb3(%c0 : index)
^bb3(%47: index):  // 2 preds: ^bb2, ^bb4
  %48 = arith.cmpi slt, %47, %44 : index
  cf.cond_br %48, ^bb4, ^bb5
^bb4:  // pred: ^bb3
  %49 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%45)[%23]
  %50 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%47)[%38]
  %51 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%49)[%3]
  %52 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%50)[%8]
  memref.store %cst, %2[%51, %52] : memref<512x1024xf32>
  %53 = arith.addi %47, %c1 : index
  cf.br ^bb3(%53 : index)
^bb5:  // pred: ^bb3
  %54 = arith.addi %45, %c1 : index
  cf.br ^bb1(%54 : index)
^bb6:  // pred: ^bb1
  cf.br ^bb7(%c0 : index)
^bb7(%55: index):  // 2 preds: ^bb6, ^bb17
  %56 = arith.cmpi slt, %55, %c256 : index
  cf.cond_br %56, ^bb8, ^bb18
^bb8:  // pred: ^bb7
  cf.br ^bb9(%c0 : index)
^bb9(%57: index):  // 2 preds: ^bb8, ^bb16
  %58 = arith.cmpi slt, %57, %29 : index
  cf.cond_br %58, ^bb10, ^bb17
^bb10:  // pred: ^bb9
  cf.br ^bb11(%c0 : index)
^bb11(%59: index):  // 2 preds: ^bb10, ^bb15
  %60 = arith.cmpi slt, %59, %44 : index
  cf.cond_br %60, ^bb12, ^bb16
^bb12:  // pred: ^bb11
  cf.br ^bb13(%c0 : index)
^bb13(%61: index):  // 2 preds: ^bb12, ^bb14
  %62 = arith.cmpi slt, %61, %c32 : index
  cf.cond_br %62, ^bb14, ^bb15
^bb14:  // pred: ^bb13
  %63 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%57)[%23]
  %64 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%61)[%55]
  %65 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%63)[%3]
  %66 = memref.load %0[%65, %64] : memref<512x256xf32>
  %67 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%59)[%38]
  %68 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%61)[%55]
  %69 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%67)[%8]
  %70 = memref.load %1[%68, %69] : memref<256x1024xf32>
  %71 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%57)[%23]
  %72 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%59)[%38]
  %73 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%71)[%3]
  %74 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%72)[%8]
  %75 = memref.load %2[%73, %74] : memref<512x1024xf32>
  %76 = arith.mulf %66, %70 : f32
  %77 = arith.addf %75, %76 : f32
  %78 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%57)[%23]
  %79 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%59)[%38]
  %80 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%78)[%3]
  %81 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%79)[%8]
  memref.store %77, %2[%80, %81] : memref<512x1024xf32>
  %82 = arith.addi %61, %c1 : index
  cf.br ^bb13(%82 : index)
^bb15:  // pred: ^bb13
  %83 = arith.addi %59, %c1 : index
  cf.br ^bb11(%83 : index)
^bb16:  // pred: ^bb11
  %84 = arith.addi %57, %c1 : index
  cf.br ^bb9(%84 : index)
^bb17:  // pred: ^bb9
  %85 = arith.addi %55, %c32 : index
  cf.br ^bb7(%85 : index)
^bb18:  // pred: ^bb7
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1024 = arith.constant 1024 : index
  %c-128 = arith.constant -128 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c-32 = arith.constant -32 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = arith.muli %workgroup_id_y, %c32 : index
  %4 = arith.muli %workgroup_id_y, %c-32 : index
  %5 = arith.addi %4, %c512 : index
  %6 = arith.cmpi slt, %5, %c32 : index
  %7 = arith.select %6, %5, %c32 : index
  %8 = arith.muli %workgroup_id_x, %c128 : index
  %9 = arith.muli %workgroup_id_x, %c-128 : index
  %10 = arith.addi %9, %c1024 : index
  %11 = arith.cmpi slt, %10, %c128 : index
  %12 = arith.select %11, %10, %c128 : index
  %13 = gpu.thread_id  x
  %14 = gpu.thread_id  y
  %15 = arith.cmpi sle, %7, %c0 : index
  %16 = arith.subi %c0, %7 : index
  %17 = arith.subi %7, %c1 : index
  %18 = arith.select %15, %16, %17 : index
  %19 = arith.divsi %18, %c8 : index
  %20 = arith.subi %c0, %19 : index
  %21 = arith.addi %19, %c1 : index
  %22 = arith.select %15, %20, %21 : index
  %23 = arith.muli %14, %22 : index
  %24 = arith.muli %23, %c-1 : index
  %25 = arith.addi %7, %24 : index
  %26 = arith.cmpi slt, %25, %22 : index
  %27 = arith.select %26, %25, %22 : index
  %28 = arith.cmpi slt, %27, %c0 : index
  %29 = arith.select %28, %c0, %27 : index
  %30 = arith.cmpi sle, %12, %c0 : index
  %31 = arith.subi %c0, %12 : index
  %32 = arith.subi %12, %c1 : index
  %33 = arith.select %30, %31, %32 : index
  %34 = arith.divsi %33, %c32 : index
  %35 = arith.subi %c0, %34 : index
  %36 = arith.addi %34, %c1 : index
  %37 = arith.select %30, %35, %36 : index
  %38 = arith.muli %13, %37 : index
  %39 = arith.muli %38, %c-1 : index
  %40 = arith.addi %12, %39 : index
  %41 = arith.cmpi slt, %40, %37 : index
  %42 = arith.select %41, %40, %37 : index
  %43 = arith.cmpi slt, %42, %c0 : index
  %44 = arith.select %43, %c0, %42 : index
  cf.br ^bb1(%c0 : index)
^bb1(%45: index):  // 2 preds: ^bb0, ^bb4
  %46 = arith.cmpi slt, %45, %29 : index
  cf.cond_br %46, ^bb2(%c0 : index), ^bb5(%c0 : index)
^bb2(%47: index):  // 2 preds: ^bb1, ^bb3
  %48 = arith.cmpi slt, %47, %44 : index
  cf.cond_br %48, ^bb3, ^bb4
^bb3:  // pred: ^bb2
  %49 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%3, %23, %45]
  %50 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%8, %38, %47]
  memref.store %cst, %2[%49, %50] : memref<512x1024xf32>
  %51 = arith.addi %47, %c1 : index
  cf.br ^bb2(%51 : index)
^bb4:  // pred: ^bb2
  %52 = arith.addi %45, %c1 : index
  cf.br ^bb1(%52 : index)
^bb5(%53: index):  // 2 preds: ^bb1, ^bb12
  %54 = arith.cmpi slt, %53, %c256 : index
  cf.cond_br %54, ^bb6(%c0 : index), ^bb13
^bb6(%55: index):  // 2 preds: ^bb5, ^bb11
  %56 = arith.cmpi slt, %55, %29 : index
  cf.cond_br %56, ^bb7(%c0 : index), ^bb12
^bb7(%57: index):  // 2 preds: ^bb6, ^bb10
  %58 = arith.cmpi slt, %57, %44 : index
  cf.cond_br %58, ^bb8(%c0 : index), ^bb11
^bb8(%59: index):  // 2 preds: ^bb7, ^bb9
  %60 = arith.cmpi slt, %59, %c32 : index
  cf.cond_br %60, ^bb9, ^bb10
^bb9:  // pred: ^bb8
  %61 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%53, %59]
  %62 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%3, %23, %55]
  %63 = memref.load %0[%62, %61] : memref<512x256xf32>
  %64 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%53, %59]
  %65 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%8, %38, %57]
  %66 = memref.load %1[%64, %65] : memref<256x1024xf32>
  %67 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%3, %23, %55]
  %68 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%8, %38, %57]
  %69 = memref.load %2[%67, %68] : memref<512x1024xf32>
  %70 = arith.mulf %63, %66 : f32
  %71 = arith.addf %69, %70 : f32
  %72 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%3, %23, %55]
  %73 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%8, %38, %57]
  memref.store %71, %2[%72, %73] : memref<512x1024xf32>
  %74 = arith.addi %59, %c1 : index
  cf.br ^bb8(%74 : index)
^bb10:  // pred: ^bb8
  %75 = arith.addi %57, %c1 : index
  cf.br ^bb7(%75 : index)
^bb11:  // pred: ^bb7
  %76 = arith.addi %55, %c1 : index
  cf.br ^bb6(%76 : index)
^bb12:  // pred: ^bb6
  %77 = arith.addi %53, %c32 : index
  cf.br ^bb5(%77 : index)
^bb13:  // pred: ^bb5
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1024 = arith.constant 1024 : index
  %c-128 = arith.constant -128 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c-32 = arith.constant -32 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = arith.muli %workgroup_id_y, %c32 : index
  %4 = arith.muli %workgroup_id_y, %c-32 : index
  %5 = arith.addi %4, %c512 : index
  %6 = arith.cmpi slt, %5, %c32 : index
  %7 = arith.select %6, %5, %c32 : index
  %8 = arith.muli %workgroup_id_x, %c128 : index
  %9 = arith.muli %workgroup_id_x, %c-128 : index
  %10 = arith.addi %9, %c1024 : index
  %11 = arith.cmpi slt, %10, %c128 : index
  %12 = arith.select %11, %10, %c128 : index
  %13 = gpu.thread_id  x
  %14 = gpu.thread_id  y
  %15 = arith.cmpi sle, %7, %c0 : index
  %16 = arith.subi %c0, %7 : index
  %17 = arith.subi %7, %c1 : index
  %18 = arith.select %15, %16, %17 : index
  %19 = arith.divsi %18, %c8 : index
  %20 = arith.subi %c0, %19 : index
  %21 = arith.addi %19, %c1 : index
  %22 = arith.select %15, %20, %21 : index
  %23 = arith.muli %14, %22 : index
  %24 = arith.muli %23, %c-1 : index
  %25 = arith.addi %7, %24 : index
  %26 = arith.cmpi slt, %25, %22 : index
  %27 = arith.select %26, %25, %22 : index
  %28 = arith.cmpi slt, %27, %c0 : index
  %29 = arith.select %28, %c0, %27 : index
  %30 = arith.cmpi sle, %12, %c0 : index
  %31 = arith.subi %c0, %12 : index
  %32 = arith.subi %12, %c1 : index
  %33 = arith.select %30, %31, %32 : index
  %34 = arith.divsi %33, %c32 : index
  %35 = arith.subi %c0, %34 : index
  %36 = arith.addi %34, %c1 : index
  %37 = arith.select %30, %35, %36 : index
  %38 = arith.muli %13, %37 : index
  %39 = arith.muli %38, %c-1 : index
  %40 = arith.addi %12, %39 : index
  %41 = arith.cmpi slt, %40, %37 : index
  %42 = arith.select %41, %40, %37 : index
  %43 = arith.cmpi slt, %42, %c0 : index
  %44 = arith.select %43, %c0, %42 : index
  cf.br ^bb1(%c0 : index)
^bb1(%45: index):  // 2 preds: ^bb0, ^bb4
  %46 = arith.cmpi slt, %45, %29 : index
  cf.cond_br %46, ^bb2(%c0 : index), ^bb5(%c0 : index)
^bb2(%47: index):  // 2 preds: ^bb1, ^bb3
  %48 = arith.cmpi slt, %47, %44 : index
  cf.cond_br %48, ^bb3, ^bb4
^bb3:  // pred: ^bb2
  %49 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%3, %23, %45]
  %50 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%8, %38, %47]
  memref.store %cst, %2[%49, %50] : memref<512x1024xf32>
  %51 = arith.addi %47, %c1 : index
  cf.br ^bb2(%51 : index)
^bb4:  // pred: ^bb2
  %52 = arith.addi %45, %c1 : index
  cf.br ^bb1(%52 : index)
^bb5(%53: index):  // 2 preds: ^bb1, ^bb12
  %54 = arith.cmpi slt, %53, %c256 : index
  cf.cond_br %54, ^bb6(%c0 : index), ^bb13
^bb6(%55: index):  // 2 preds: ^bb5, ^bb11
  %56 = arith.cmpi slt, %55, %29 : index
  cf.cond_br %56, ^bb7(%c0 : index), ^bb12
^bb7(%57: index):  // 2 preds: ^bb6, ^bb10
  %58 = arith.cmpi slt, %57, %44 : index
  cf.cond_br %58, ^bb8(%c0 : index), ^bb11
^bb8(%59: index):  // 2 preds: ^bb7, ^bb9
  %60 = arith.cmpi slt, %59, %c32 : index
  cf.cond_br %60, ^bb9, ^bb10
^bb9:  // pred: ^bb8
  %61 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%53, %59]
  %62 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%3, %23, %55]
  %63 = memref.load %0[%62, %61] : memref<512x256xf32>
  %64 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%8, %38, %57]
  %65 = memref.load %1[%61, %64] : memref<256x1024xf32>
  %66 = memref.load %2[%62, %64] : memref<512x1024xf32>
  %67 = arith.mulf %63, %65 : f32
  %68 = arith.addf %66, %67 : f32
  memref.store %68, %2[%62, %64] : memref<512x1024xf32>
  %69 = arith.addi %59, %c1 : index
  cf.br ^bb8(%69 : index)
^bb10:  // pred: ^bb8
  %70 = arith.addi %57, %c1 : index
  cf.br ^bb7(%70 : index)
^bb11:  // pred: ^bb7
  %71 = arith.addi %55, %c1 : index
  cf.br ^bb6(%71 : index)
^bb12:  // pred: ^bb6
  %72 = arith.addi %53, %c32 : index
  cf.br ^bb5(%72 : index)
^bb13:  // pred: ^bb5
  return
}

// -----// IR Dump After PolynomialApproximationPass (iree-codegen-polynomial-approximation) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1024 = arith.constant 1024 : index
  %c-128 = arith.constant -128 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c-32 = arith.constant -32 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = arith.muli %workgroup_id_y, %c32 : index
  %4 = arith.muli %workgroup_id_y, %c-32 : index
  %5 = arith.addi %4, %c512 : index
  %6 = arith.cmpi slt, %5, %c32 : index
  %7 = arith.select %6, %5, %c32 : index
  %8 = arith.muli %workgroup_id_x, %c128 : index
  %9 = arith.muli %workgroup_id_x, %c-128 : index
  %10 = arith.addi %9, %c1024 : index
  %11 = arith.cmpi slt, %10, %c128 : index
  %12 = arith.select %11, %10, %c128 : index
  %13 = gpu.thread_id  x
  %14 = gpu.thread_id  y
  %15 = arith.cmpi sle, %7, %c0 : index
  %16 = arith.subi %c0, %7 : index
  %17 = arith.subi %7, %c1 : index
  %18 = arith.select %15, %16, %17 : index
  %19 = arith.divsi %18, %c8 : index
  %20 = arith.subi %c0, %19 : index
  %21 = arith.addi %19, %c1 : index
  %22 = arith.select %15, %20, %21 : index
  %23 = arith.muli %14, %22 : index
  %24 = arith.muli %23, %c-1 : index
  %25 = arith.addi %7, %24 : index
  %26 = arith.cmpi slt, %25, %22 : index
  %27 = arith.select %26, %25, %22 : index
  %28 = arith.cmpi slt, %27, %c0 : index
  %29 = arith.select %28, %c0, %27 : index
  %30 = arith.cmpi sle, %12, %c0 : index
  %31 = arith.subi %c0, %12 : index
  %32 = arith.subi %12, %c1 : index
  %33 = arith.select %30, %31, %32 : index
  %34 = arith.divsi %33, %c32 : index
  %35 = arith.subi %c0, %34 : index
  %36 = arith.addi %34, %c1 : index
  %37 = arith.select %30, %35, %36 : index
  %38 = arith.muli %13, %37 : index
  %39 = arith.muli %38, %c-1 : index
  %40 = arith.addi %12, %39 : index
  %41 = arith.cmpi slt, %40, %37 : index
  %42 = arith.select %41, %40, %37 : index
  %43 = arith.cmpi slt, %42, %c0 : index
  %44 = arith.select %43, %c0, %42 : index
  cf.br ^bb1(%c0 : index)
^bb1(%45: index):  // 2 preds: ^bb0, ^bb4
  %46 = arith.cmpi slt, %45, %29 : index
  cf.cond_br %46, ^bb2(%c0 : index), ^bb5(%c0 : index)
^bb2(%47: index):  // 2 preds: ^bb1, ^bb3
  %48 = arith.cmpi slt, %47, %44 : index
  cf.cond_br %48, ^bb3, ^bb4
^bb3:  // pred: ^bb2
  %49 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%3, %23, %45]
  %50 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%8, %38, %47]
  memref.store %cst, %2[%49, %50] : memref<512x1024xf32>
  %51 = arith.addi %47, %c1 : index
  cf.br ^bb2(%51 : index)
^bb4:  // pred: ^bb2
  %52 = arith.addi %45, %c1 : index
  cf.br ^bb1(%52 : index)
^bb5(%53: index):  // 2 preds: ^bb1, ^bb12
  %54 = arith.cmpi slt, %53, %c256 : index
  cf.cond_br %54, ^bb6(%c0 : index), ^bb13
^bb6(%55: index):  // 2 preds: ^bb5, ^bb11
  %56 = arith.cmpi slt, %55, %29 : index
  cf.cond_br %56, ^bb7(%c0 : index), ^bb12
^bb7(%57: index):  // 2 preds: ^bb6, ^bb10
  %58 = arith.cmpi slt, %57, %44 : index
  cf.cond_br %58, ^bb8(%c0 : index), ^bb11
^bb8(%59: index):  // 2 preds: ^bb7, ^bb9
  %60 = arith.cmpi slt, %59, %c32 : index
  cf.cond_br %60, ^bb9, ^bb10
^bb9:  // pred: ^bb8
  %61 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%53, %59]
  %62 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%3, %23, %55]
  %63 = memref.load %0[%62, %61] : memref<512x256xf32>
  %64 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%8, %38, %57]
  %65 = memref.load %1[%61, %64] : memref<256x1024xf32>
  %66 = memref.load %2[%62, %64] : memref<512x1024xf32>
  %67 = arith.mulf %63, %65 : f32
  %68 = arith.addf %66, %67 : f32
  memref.store %68, %2[%62, %64] : memref<512x1024xf32>
  %69 = arith.addi %59, %c1 : index
  cf.br ^bb8(%69 : index)
^bb10:  // pred: ^bb8
  %70 = arith.addi %57, %c1 : index
  cf.br ^bb7(%70 : index)
^bb11:  // pred: ^bb7
  %71 = arith.addi %55, %c1 : index
  cf.br ^bb6(%71 : index)
^bb12:  // pred: ^bb6
  %72 = arith.addi %53, %c32 : index
  cf.br ^bb5(%72 : index)
^bb13:  // pred: ^bb5
  return
}

// -----// IR Dump After ArithExpandOps (arith-expand) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1024 = arith.constant 1024 : index
  %c-128 = arith.constant -128 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c-32 = arith.constant -32 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = arith.muli %workgroup_id_y, %c32 : index
  %4 = arith.muli %workgroup_id_y, %c-32 : index
  %5 = arith.addi %4, %c512 : index
  %6 = arith.cmpi slt, %5, %c32 : index
  %7 = arith.select %6, %5, %c32 : index
  %8 = arith.muli %workgroup_id_x, %c128 : index
  %9 = arith.muli %workgroup_id_x, %c-128 : index
  %10 = arith.addi %9, %c1024 : index
  %11 = arith.cmpi slt, %10, %c128 : index
  %12 = arith.select %11, %10, %c128 : index
  %13 = gpu.thread_id  x
  %14 = gpu.thread_id  y
  %15 = arith.cmpi sle, %7, %c0 : index
  %16 = arith.subi %c0, %7 : index
  %17 = arith.subi %7, %c1 : index
  %18 = arith.select %15, %16, %17 : index
  %19 = arith.divsi %18, %c8 : index
  %20 = arith.subi %c0, %19 : index
  %21 = arith.addi %19, %c1 : index
  %22 = arith.select %15, %20, %21 : index
  %23 = arith.muli %14, %22 : index
  %24 = arith.muli %23, %c-1 : index
  %25 = arith.addi %7, %24 : index
  %26 = arith.cmpi slt, %25, %22 : index
  %27 = arith.select %26, %25, %22 : index
  %28 = arith.cmpi slt, %27, %c0 : index
  %29 = arith.select %28, %c0, %27 : index
  %30 = arith.cmpi sle, %12, %c0 : index
  %31 = arith.subi %c0, %12 : index
  %32 = arith.subi %12, %c1 : index
  %33 = arith.select %30, %31, %32 : index
  %34 = arith.divsi %33, %c32 : index
  %35 = arith.subi %c0, %34 : index
  %36 = arith.addi %34, %c1 : index
  %37 = arith.select %30, %35, %36 : index
  %38 = arith.muli %13, %37 : index
  %39 = arith.muli %38, %c-1 : index
  %40 = arith.addi %12, %39 : index
  %41 = arith.cmpi slt, %40, %37 : index
  %42 = arith.select %41, %40, %37 : index
  %43 = arith.cmpi slt, %42, %c0 : index
  %44 = arith.select %43, %c0, %42 : index
  cf.br ^bb1(%c0 : index)
^bb1(%45: index):  // 2 preds: ^bb0, ^bb4
  %46 = arith.cmpi slt, %45, %29 : index
  cf.cond_br %46, ^bb2(%c0 : index), ^bb5(%c0 : index)
^bb2(%47: index):  // 2 preds: ^bb1, ^bb3
  %48 = arith.cmpi slt, %47, %44 : index
  cf.cond_br %48, ^bb3, ^bb4
^bb3:  // pred: ^bb2
  %49 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%3, %23, %45]
  %50 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%8, %38, %47]
  memref.store %cst, %2[%49, %50] : memref<512x1024xf32>
  %51 = arith.addi %47, %c1 : index
  cf.br ^bb2(%51 : index)
^bb4:  // pred: ^bb2
  %52 = arith.addi %45, %c1 : index
  cf.br ^bb1(%52 : index)
^bb5(%53: index):  // 2 preds: ^bb1, ^bb12
  %54 = arith.cmpi slt, %53, %c256 : index
  cf.cond_br %54, ^bb6(%c0 : index), ^bb13
^bb6(%55: index):  // 2 preds: ^bb5, ^bb11
  %56 = arith.cmpi slt, %55, %29 : index
  cf.cond_br %56, ^bb7(%c0 : index), ^bb12
^bb7(%57: index):  // 2 preds: ^bb6, ^bb10
  %58 = arith.cmpi slt, %57, %44 : index
  cf.cond_br %58, ^bb8(%c0 : index), ^bb11
^bb8(%59: index):  // 2 preds: ^bb7, ^bb9
  %60 = arith.cmpi slt, %59, %c32 : index
  cf.cond_br %60, ^bb9, ^bb10
^bb9:  // pred: ^bb8
  %61 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%53, %59]
  %62 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%3, %23, %55]
  %63 = memref.load %0[%62, %61] : memref<512x256xf32>
  %64 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%8, %38, %57]
  %65 = memref.load %1[%61, %64] : memref<256x1024xf32>
  %66 = memref.load %2[%62, %64] : memref<512x1024xf32>
  %67 = arith.mulf %63, %65 : f32
  %68 = arith.addf %66, %67 : f32
  memref.store %68, %2[%62, %64] : memref<512x1024xf32>
  %69 = arith.addi %59, %c1 : index
  cf.br ^bb8(%69 : index)
^bb10:  // pred: ^bb8
  %70 = arith.addi %57, %c1 : index
  cf.br ^bb7(%70 : index)
^bb11:  // pred: ^bb7
  %71 = arith.addi %55, %c1 : index
  cf.br ^bb6(%71 : index)
^bb12:  // pred: ^bb6
  %72 = arith.addi %53, %c32 : index
  cf.br ^bb5(%72 : index)
^bb13:  // pred: ^bb5
  return
}

// -----// IR Dump After ExpandOps (memref-expand) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c-1 = arith.constant -1 : index
  %c8 = arith.constant 8 : index
  %c1024 = arith.constant 1024 : index
  %c-128 = arith.constant -128 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c-32 = arith.constant -32 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = arith.muli %workgroup_id_y, %c32 : index
  %4 = arith.muli %workgroup_id_y, %c-32 : index
  %5 = arith.addi %4, %c512 : index
  %6 = arith.cmpi slt, %5, %c32 : index
  %7 = arith.select %6, %5, %c32 : index
  %8 = arith.muli %workgroup_id_x, %c128 : index
  %9 = arith.muli %workgroup_id_x, %c-128 : index
  %10 = arith.addi %9, %c1024 : index
  %11 = arith.cmpi slt, %10, %c128 : index
  %12 = arith.select %11, %10, %c128 : index
  %13 = gpu.thread_id  x
  %14 = gpu.thread_id  y
  %15 = arith.cmpi sle, %7, %c0 : index
  %16 = arith.subi %c0, %7 : index
  %17 = arith.subi %7, %c1 : index
  %18 = arith.select %15, %16, %17 : index
  %19 = arith.divsi %18, %c8 : index
  %20 = arith.subi %c0, %19 : index
  %21 = arith.addi %19, %c1 : index
  %22 = arith.select %15, %20, %21 : index
  %23 = arith.muli %14, %22 : index
  %24 = arith.muli %23, %c-1 : index
  %25 = arith.addi %7, %24 : index
  %26 = arith.cmpi slt, %25, %22 : index
  %27 = arith.select %26, %25, %22 : index
  %28 = arith.cmpi slt, %27, %c0 : index
  %29 = arith.select %28, %c0, %27 : index
  %30 = arith.cmpi sle, %12, %c0 : index
  %31 = arith.subi %c0, %12 : index
  %32 = arith.subi %12, %c1 : index
  %33 = arith.select %30, %31, %32 : index
  %34 = arith.divsi %33, %c32 : index
  %35 = arith.subi %c0, %34 : index
  %36 = arith.addi %34, %c1 : index
  %37 = arith.select %30, %35, %36 : index
  %38 = arith.muli %13, %37 : index
  %39 = arith.muli %38, %c-1 : index
  %40 = arith.addi %12, %39 : index
  %41 = arith.cmpi slt, %40, %37 : index
  %42 = arith.select %41, %40, %37 : index
  %43 = arith.cmpi slt, %42, %c0 : index
  %44 = arith.select %43, %c0, %42 : index
  cf.br ^bb1(%c0 : index)
^bb1(%45: index):  // 2 preds: ^bb0, ^bb4
  %46 = arith.cmpi slt, %45, %29 : index
  cf.cond_br %46, ^bb2(%c0 : index), ^bb5(%c0 : index)
^bb2(%47: index):  // 2 preds: ^bb1, ^bb3
  %48 = arith.cmpi slt, %47, %44 : index
  cf.cond_br %48, ^bb3, ^bb4
^bb3:  // pred: ^bb2
  %49 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%3, %23, %45]
  %50 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%8, %38, %47]
  memref.store %cst, %2[%49, %50] : memref<512x1024xf32>
  %51 = arith.addi %47, %c1 : index
  cf.br ^bb2(%51 : index)
^bb4:  // pred: ^bb2
  %52 = arith.addi %45, %c1 : index
  cf.br ^bb1(%52 : index)
^bb5(%53: index):  // 2 preds: ^bb1, ^bb12
  %54 = arith.cmpi slt, %53, %c256 : index
  cf.cond_br %54, ^bb6(%c0 : index), ^bb13
^bb6(%55: index):  // 2 preds: ^bb5, ^bb11
  %56 = arith.cmpi slt, %55, %29 : index
  cf.cond_br %56, ^bb7(%c0 : index), ^bb12
^bb7(%57: index):  // 2 preds: ^bb6, ^bb10
  %58 = arith.cmpi slt, %57, %44 : index
  cf.cond_br %58, ^bb8(%c0 : index), ^bb11
^bb8(%59: index):  // 2 preds: ^bb7, ^bb9
  %60 = arith.cmpi slt, %59, %c32 : index
  cf.cond_br %60, ^bb9, ^bb10
^bb9:  // pred: ^bb8
  %61 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%53, %59]
  %62 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%3, %23, %55]
  %63 = memref.load %0[%62, %61] : memref<512x256xf32>
  %64 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%8, %38, %57]
  %65 = memref.load %1[%61, %64] : memref<256x1024xf32>
  %66 = memref.load %2[%62, %64] : memref<512x1024xf32>
  %67 = arith.mulf %63, %65 : f32
  %68 = arith.addf %66, %67 : f32
  memref.store %68, %2[%62, %64] : memref<512x1024xf32>
  %69 = arith.addi %59, %c1 : index
  cf.br ^bb8(%69 : index)
^bb10:  // pred: ^bb8
  %70 = arith.addi %57, %c1 : index
  cf.br ^bb7(%70 : index)
^bb11:  // pred: ^bb7
  %71 = arith.addi %55, %c1 : index
  cf.br ^bb6(%71 : index)
^bb12:  // pred: ^bb6
  %72 = arith.addi %53, %c32 : index
  cf.br ^bb5(%72 : index)
^bb13:  // pred: ^bb5
  return
}

// -----// IR Dump After ExpandStridedMetadata (expand-strided-metadata) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c-1 = arith.constant -1 : index
    %c8 = arith.constant 8 : index
    %c1024 = arith.constant 1024 : index
    %c-128 = arith.constant -128 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %c-32 = arith.constant -32 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
    memref.assume_alignment %0, 64 : memref<512x256xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
    memref.assume_alignment %1, 64 : memref<256x1024xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
    memref.assume_alignment %2, 64 : memref<512x1024xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = arith.muli %workgroup_id_y, %c32 : index
    %4 = arith.muli %workgroup_id_y, %c-32 : index
    %5 = arith.addi %4, %c512 : index
    %6 = arith.cmpi slt, %5, %c32 : index
    %7 = arith.select %6, %5, %c32 : index
    %8 = arith.muli %workgroup_id_x, %c128 : index
    %9 = arith.muli %workgroup_id_x, %c-128 : index
    %10 = arith.addi %9, %c1024 : index
    %11 = arith.cmpi slt, %10, %c128 : index
    %12 = arith.select %11, %10, %c128 : index
    %13 = gpu.thread_id  x
    %14 = gpu.thread_id  y
    %15 = arith.cmpi sle, %7, %c0 : index
    %16 = arith.subi %c0, %7 : index
    %17 = arith.subi %7, %c1 : index
    %18 = arith.select %15, %16, %17 : index
    %19 = arith.divsi %18, %c8 : index
    %20 = arith.subi %c0, %19 : index
    %21 = arith.addi %19, %c1 : index
    %22 = arith.select %15, %20, %21 : index
    %23 = arith.muli %14, %22 : index
    %24 = arith.muli %23, %c-1 : index
    %25 = arith.addi %7, %24 : index
    %26 = arith.cmpi slt, %25, %22 : index
    %27 = arith.select %26, %25, %22 : index
    %28 = arith.cmpi slt, %27, %c0 : index
    %29 = arith.select %28, %c0, %27 : index
    %30 = arith.cmpi sle, %12, %c0 : index
    %31 = arith.subi %c0, %12 : index
    %32 = arith.subi %12, %c1 : index
    %33 = arith.select %30, %31, %32 : index
    %34 = arith.divsi %33, %c32 : index
    %35 = arith.subi %c0, %34 : index
    %36 = arith.addi %34, %c1 : index
    %37 = arith.select %30, %35, %36 : index
    %38 = arith.muli %13, %37 : index
    %39 = arith.muli %38, %c-1 : index
    %40 = arith.addi %12, %39 : index
    %41 = arith.cmpi slt, %40, %37 : index
    %42 = arith.select %41, %40, %37 : index
    %43 = arith.cmpi slt, %42, %c0 : index
    %44 = arith.select %43, %c0, %42 : index
    cf.br ^bb1(%c0 : index)
  ^bb1(%45: index):  // 2 preds: ^bb0, ^bb4
    %46 = arith.cmpi slt, %45, %29 : index
    cf.cond_br %46, ^bb2(%c0 : index), ^bb5(%c0 : index)
  ^bb2(%47: index):  // 2 preds: ^bb1, ^bb3
    %48 = arith.cmpi slt, %47, %44 : index
    cf.cond_br %48, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %49 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%3, %23, %45]
    %50 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%8, %38, %47]
    memref.store %cst, %2[%49, %50] : memref<512x1024xf32>
    %51 = arith.addi %47, %c1 : index
    cf.br ^bb2(%51 : index)
  ^bb4:  // pred: ^bb2
    %52 = arith.addi %45, %c1 : index
    cf.br ^bb1(%52 : index)
  ^bb5(%53: index):  // 2 preds: ^bb1, ^bb12
    %54 = arith.cmpi slt, %53, %c256 : index
    cf.cond_br %54, ^bb6(%c0 : index), ^bb13
  ^bb6(%55: index):  // 2 preds: ^bb5, ^bb11
    %56 = arith.cmpi slt, %55, %29 : index
    cf.cond_br %56, ^bb7(%c0 : index), ^bb12
  ^bb7(%57: index):  // 2 preds: ^bb6, ^bb10
    %58 = arith.cmpi slt, %57, %44 : index
    cf.cond_br %58, ^bb8(%c0 : index), ^bb11
  ^bb8(%59: index):  // 2 preds: ^bb7, ^bb9
    %60 = arith.cmpi slt, %59, %c32 : index
    cf.cond_br %60, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %61 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%53, %59]
    %62 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%3, %23, %55]
    %63 = memref.load %0[%62, %61] : memref<512x256xf32>
    %64 = affine.apply affine_map<()[s0, s1, s2] -> (s0 + s1 + s2)>()[%8, %38, %57]
    %65 = memref.load %1[%61, %64] : memref<256x1024xf32>
    %66 = memref.load %2[%62, %64] : memref<512x1024xf32>
    %67 = arith.mulf %63, %65 : f32
    %68 = arith.addf %66, %67 : f32
    memref.store %68, %2[%62, %64] : memref<512x1024xf32>
    %69 = arith.addi %59, %c1 : index
    cf.br ^bb8(%69 : index)
  ^bb10:  // pred: ^bb8
    %70 = arith.addi %57, %c1 : index
    cf.br ^bb7(%70 : index)
  ^bb11:  // pred: ^bb7
    %71 = arith.addi %55, %c1 : index
    cf.br ^bb6(%71 : index)
  ^bb12:  // pred: ^bb6
    %72 = arith.addi %53, %c32 : index
    cf.br ^bb5(%72 : index)
  ^bb13:  // pred: ^bb5
    return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c-1 = arith.constant -1 : index
    %c8 = arith.constant 8 : index
    %c1024 = arith.constant 1024 : index
    %c-128 = arith.constant -128 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %c-32 = arith.constant -32 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
    memref.assume_alignment %0, 64 : memref<512x256xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
    memref.assume_alignment %1, 64 : memref<256x1024xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
    memref.assume_alignment %2, 64 : memref<512x1024xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = arith.muli %workgroup_id_y, %c32 : index
    %4 = arith.muli %workgroup_id_y, %c-32 : index
    %5 = arith.addi %4, %c512 : index
    %6 = arith.cmpi slt, %5, %c32 : index
    %7 = arith.select %6, %5, %c32 : index
    %8 = arith.muli %workgroup_id_x, %c128 : index
    %9 = arith.muli %workgroup_id_x, %c-128 : index
    %10 = arith.addi %9, %c1024 : index
    %11 = arith.cmpi slt, %10, %c128 : index
    %12 = arith.select %11, %10, %c128 : index
    %13 = gpu.thread_id  x
    %14 = gpu.thread_id  y
    %15 = arith.cmpi sle, %7, %c0 : index
    %16 = arith.subi %c0, %7 : index
    %17 = arith.subi %7, %c1 : index
    %18 = arith.select %15, %16, %17 : index
    %19 = arith.divsi %18, %c8 : index
    %20 = arith.subi %c0, %19 : index
    %21 = arith.addi %19, %c1 : index
    %22 = arith.select %15, %20, %21 : index
    %23 = arith.muli %14, %22 : index
    %24 = arith.muli %23, %c-1 : index
    %25 = arith.addi %7, %24 : index
    %26 = arith.cmpi slt, %25, %22 : index
    %27 = arith.select %26, %25, %22 : index
    %28 = arith.cmpi slt, %27, %c0 : index
    %29 = arith.select %28, %c0, %27 : index
    %30 = arith.cmpi sle, %12, %c0 : index
    %31 = arith.subi %c0, %12 : index
    %32 = arith.subi %12, %c1 : index
    %33 = arith.select %30, %31, %32 : index
    %34 = arith.divsi %33, %c32 : index
    %35 = arith.subi %c0, %34 : index
    %36 = arith.addi %34, %c1 : index
    %37 = arith.select %30, %35, %36 : index
    %38 = arith.muli %13, %37 : index
    %39 = arith.muli %38, %c-1 : index
    %40 = arith.addi %12, %39 : index
    %41 = arith.cmpi slt, %40, %37 : index
    %42 = arith.select %41, %40, %37 : index
    %43 = arith.cmpi slt, %42, %c0 : index
    %44 = arith.select %43, %c0, %42 : index
    cf.br ^bb1(%c0 : index)
  ^bb1(%45: index):  // 2 preds: ^bb0, ^bb4
    %46 = arith.cmpi slt, %45, %29 : index
    cf.cond_br %46, ^bb2(%c0 : index), ^bb5(%c0 : index)
  ^bb2(%47: index):  // 2 preds: ^bb1, ^bb3
    %48 = arith.cmpi slt, %47, %44 : index
    cf.cond_br %48, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %49 = arith.addi %3, %23 : index
    %50 = arith.addi %49, %45 : index
    %51 = arith.addi %8, %38 : index
    %52 = arith.addi %51, %47 : index
    memref.store %cst, %2[%50, %52] : memref<512x1024xf32>
    %53 = arith.addi %47, %c1 : index
    cf.br ^bb2(%53 : index)
  ^bb4:  // pred: ^bb2
    %54 = arith.addi %45, %c1 : index
    cf.br ^bb1(%54 : index)
  ^bb5(%55: index):  // 2 preds: ^bb1, ^bb12
    %56 = arith.cmpi slt, %55, %c256 : index
    cf.cond_br %56, ^bb6(%c0 : index), ^bb13
  ^bb6(%57: index):  // 2 preds: ^bb5, ^bb11
    %58 = arith.cmpi slt, %57, %29 : index
    cf.cond_br %58, ^bb7(%c0 : index), ^bb12
  ^bb7(%59: index):  // 2 preds: ^bb6, ^bb10
    %60 = arith.cmpi slt, %59, %44 : index
    cf.cond_br %60, ^bb8(%c0 : index), ^bb11
  ^bb8(%61: index):  // 2 preds: ^bb7, ^bb9
    %62 = arith.cmpi slt, %61, %c32 : index
    cf.cond_br %62, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %63 = arith.addi %55, %61 : index
    %64 = arith.addi %3, %23 : index
    %65 = arith.addi %64, %57 : index
    %66 = memref.load %0[%65, %63] : memref<512x256xf32>
    %67 = arith.addi %8, %38 : index
    %68 = arith.addi %67, %59 : index
    %69 = memref.load %1[%63, %68] : memref<256x1024xf32>
    %70 = memref.load %2[%65, %68] : memref<512x1024xf32>
    %71 = arith.mulf %66, %69 : f32
    %72 = arith.addf %70, %71 : f32
    memref.store %72, %2[%65, %68] : memref<512x1024xf32>
    %73 = arith.addi %61, %c1 : index
    cf.br ^bb8(%73 : index)
  ^bb10:  // pred: ^bb8
    %74 = arith.addi %59, %c1 : index
    cf.br ^bb7(%74 : index)
  ^bb11:  // pred: ^bb7
    %75 = arith.addi %57, %c1 : index
    cf.br ^bb6(%75 : index)
  ^bb12:  // pred: ^bb6
    %76 = arith.addi %55, %c32 : index
    cf.br ^bb5(%76 : index)
  ^bb13:  // pred: ^bb5
    return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c-1 = arith.constant -1 : index
    %c8 = arith.constant 8 : index
    %c1024 = arith.constant 1024 : index
    %c-128 = arith.constant -128 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %c-32 = arith.constant -32 : index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
    memref.assume_alignment %0, 64 : memref<512x256xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
    memref.assume_alignment %1, 64 : memref<256x1024xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
    memref.assume_alignment %2, 64 : memref<512x1024xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %3 = arith.muli %workgroup_id_y, %c32 : index
    %4 = arith.muli %workgroup_id_y, %c-32 : index
    %5 = arith.addi %4, %c512 : index
    %6 = arith.cmpi slt, %5, %c32 : index
    %7 = arith.select %6, %5, %c32 : index
    %8 = arith.muli %workgroup_id_x, %c128 : index
    %9 = arith.muli %workgroup_id_x, %c-128 : index
    %10 = arith.addi %9, %c1024 : index
    %11 = arith.cmpi slt, %10, %c128 : index
    %12 = arith.select %11, %10, %c128 : index
    %13 = gpu.thread_id  x
    %14 = gpu.thread_id  y
    %15 = arith.cmpi sle, %7, %c0 : index
    %16 = arith.subi %c0, %7 : index
    %17 = arith.subi %7, %c1 : index
    %18 = arith.select %15, %16, %17 : index
    %19 = arith.divsi %18, %c8 : index
    %20 = arith.subi %c0, %19 : index
    %21 = arith.addi %19, %c1 : index
    %22 = arith.select %15, %20, %21 : index
    %23 = arith.muli %14, %22 : index
    %24 = arith.muli %23, %c-1 : index
    %25 = arith.addi %7, %24 : index
    %26 = arith.cmpi slt, %25, %22 : index
    %27 = arith.select %26, %25, %22 : index
    %28 = arith.cmpi slt, %27, %c0 : index
    %29 = arith.select %28, %c0, %27 : index
    %30 = arith.cmpi sle, %12, %c0 : index
    %31 = arith.subi %c0, %12 : index
    %32 = arith.subi %12, %c1 : index
    %33 = arith.select %30, %31, %32 : index
    %34 = arith.divsi %33, %c32 : index
    %35 = arith.subi %c0, %34 : index
    %36 = arith.addi %34, %c1 : index
    %37 = arith.select %30, %35, %36 : index
    %38 = arith.muli %13, %37 : index
    %39 = arith.muli %38, %c-1 : index
    %40 = arith.addi %12, %39 : index
    %41 = arith.cmpi slt, %40, %37 : index
    %42 = arith.select %41, %40, %37 : index
    %43 = arith.cmpi slt, %42, %c0 : index
    %44 = arith.select %43, %c0, %42 : index
    cf.br ^bb1(%c0 : index)
  ^bb1(%45: index):  // 2 preds: ^bb0, ^bb4
    %46 = arith.cmpi slt, %45, %29 : index
    cf.cond_br %46, ^bb2(%c0 : index), ^bb5(%c0 : index)
  ^bb2(%47: index):  // 2 preds: ^bb1, ^bb3
    %48 = arith.cmpi slt, %47, %44 : index
    cf.cond_br %48, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %49 = arith.addi %3, %23 : index
    %50 = arith.addi %49, %45 : index
    %51 = arith.addi %8, %38 : index
    %52 = arith.addi %51, %47 : index
    memref.store %cst, %2[%50, %52] : memref<512x1024xf32>
    %53 = arith.addi %47, %c1 : index
    cf.br ^bb2(%53 : index)
  ^bb4:  // pred: ^bb2
    %54 = arith.addi %45, %c1 : index
    cf.br ^bb1(%54 : index)
  ^bb5(%55: index):  // 2 preds: ^bb1, ^bb12
    %56 = arith.cmpi slt, %55, %c256 : index
    cf.cond_br %56, ^bb6(%c0 : index), ^bb13
  ^bb6(%57: index):  // 2 preds: ^bb5, ^bb11
    %58 = arith.cmpi slt, %57, %29 : index
    cf.cond_br %58, ^bb7(%c0 : index), ^bb12
  ^bb7(%59: index):  // 2 preds: ^bb6, ^bb10
    %60 = arith.cmpi slt, %59, %44 : index
    cf.cond_br %60, ^bb8(%c0 : index), ^bb11
  ^bb8(%61: index):  // 2 preds: ^bb7, ^bb9
    %62 = arith.cmpi slt, %61, %c32 : index
    cf.cond_br %62, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %63 = arith.addi %55, %61 : index
    %64 = arith.addi %3, %23 : index
    %65 = arith.addi %64, %57 : index
    %66 = memref.load %0[%65, %63] : memref<512x256xf32>
    %67 = arith.addi %8, %38 : index
    %68 = arith.addi %67, %59 : index
    %69 = memref.load %1[%63, %68] : memref<256x1024xf32>
    %70 = memref.load %2[%65, %68] : memref<512x1024xf32>
    %71 = arith.mulf %66, %69 : f32
    %72 = arith.addf %70, %71 : f32
    memref.store %72, %2[%65, %68] : memref<512x1024xf32>
    %73 = arith.addi %61, %c1 : index
    cf.br ^bb8(%73 : index)
  ^bb10:  // pred: ^bb8
    %74 = arith.addi %59, %c1 : index
    cf.br ^bb7(%74 : index)
  ^bb11:  // pred: ^bb7
    %75 = arith.addi %57, %c1 : index
    cf.br ^bb6(%75 : index)
  ^bb12:  // pred: ^bb6
    %76 = arith.addi %55, %c32 : index
    cf.br ^bb5(%76 : index)
  ^bb13:  // pred: ^bb5
    return
  }
}

// -----// IR Dump After ConvertToNVVM (iree-convert-to-nvvm) //----- //
module {
  llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
    %0 = llvm.mlir.constant(-1 : index) : i64
    %1 = llvm.mlir.constant(8 : index) : i64
    %2 = llvm.mlir.constant(1024 : index) : i64
    %3 = llvm.mlir.constant(-128 : index) : i64
    %4 = llvm.mlir.constant(128 : index) : i64
    %5 = llvm.mlir.constant(512 : index) : i64
    %6 = llvm.mlir.constant(-32 : index) : i64
    %7 = llvm.mlir.constant(0 : index) : i64
    %8 = llvm.mlir.constant(1 : index) : i64
    %9 = llvm.mlir.constant(256 : index) : i64
    %10 = llvm.mlir.constant(32 : index) : i64
    %11 = llvm.mlir.constant(0.000000e+00 : f32) : f32
    %12 = llvm.bitcast %arg0 : !llvm.ptr<f32> to !llvm.ptr<i8>
    %13 = llvm.getelementptr %12[%7] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
    %14 = llvm.bitcast %13 : !llvm.ptr<i8> to !llvm.ptr<f32>
    %15 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>
    %16 = llvm.insertvalue %14, %15[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %17 = llvm.insertvalue %14, %16[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %18 = llvm.mlir.constant(0 : index) : i64
    %19 = llvm.insertvalue %18, %17[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %20 = llvm.mlir.constant(512 : index) : i64
    %21 = llvm.insertvalue %20, %19[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %22 = llvm.mlir.constant(256 : index) : i64
    %23 = llvm.insertvalue %22, %21[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %24 = llvm.mlir.constant(256 : index) : i64
    %25 = llvm.insertvalue %24, %23[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %26 = llvm.mlir.constant(1 : index) : i64
    %27 = llvm.insertvalue %26, %25[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %28 = llvm.extractvalue %27[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %29 = llvm.mlir.constant(0 : index) : i64
    %30 = llvm.mlir.constant(63 : index) : i64
    %31 = llvm.ptrtoint %28 : !llvm.ptr<f32> to i64
    %32 = llvm.and %31, %30  : i64
    %33 = llvm.icmp "eq" %32, %29 : i64
    "llvm.intr.assume"(%33) : (i1) -> ()
    %34 = llvm.bitcast %arg1 : !llvm.ptr<f32> to !llvm.ptr<i8>
    %35 = llvm.getelementptr %34[%7] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
    %36 = llvm.bitcast %35 : !llvm.ptr<i8> to !llvm.ptr<f32>
    %37 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>
    %38 = llvm.insertvalue %36, %37[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %39 = llvm.insertvalue %36, %38[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %40 = llvm.mlir.constant(0 : index) : i64
    %41 = llvm.insertvalue %40, %39[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %42 = llvm.mlir.constant(256 : index) : i64
    %43 = llvm.insertvalue %42, %41[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %44 = llvm.mlir.constant(1024 : index) : i64
    %45 = llvm.insertvalue %44, %43[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %46 = llvm.mlir.constant(1024 : index) : i64
    %47 = llvm.insertvalue %46, %45[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %48 = llvm.mlir.constant(1 : index) : i64
    %49 = llvm.insertvalue %48, %47[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %50 = llvm.extractvalue %49[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %51 = llvm.mlir.constant(0 : index) : i64
    %52 = llvm.mlir.constant(63 : index) : i64
    %53 = llvm.ptrtoint %50 : !llvm.ptr<f32> to i64
    %54 = llvm.and %53, %52  : i64
    %55 = llvm.icmp "eq" %54, %51 : i64
    "llvm.intr.assume"(%55) : (i1) -> ()
    %56 = llvm.bitcast %arg2 : !llvm.ptr<f32> to !llvm.ptr<i8>
    %57 = llvm.getelementptr %56[%7] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
    %58 = llvm.bitcast %57 : !llvm.ptr<i8> to !llvm.ptr<f32>
    %59 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>
    %60 = llvm.insertvalue %58, %59[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %61 = llvm.insertvalue %58, %60[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %62 = llvm.mlir.constant(0 : index) : i64
    %63 = llvm.insertvalue %62, %61[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %64 = llvm.mlir.constant(512 : index) : i64
    %65 = llvm.insertvalue %64, %63[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %66 = llvm.mlir.constant(1024 : index) : i64
    %67 = llvm.insertvalue %66, %65[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %68 = llvm.mlir.constant(1024 : index) : i64
    %69 = llvm.insertvalue %68, %67[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %70 = llvm.mlir.constant(1 : index) : i64
    %71 = llvm.insertvalue %70, %69[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %72 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %73 = llvm.mlir.constant(0 : index) : i64
    %74 = llvm.mlir.constant(63 : index) : i64
    %75 = llvm.ptrtoint %72 : !llvm.ptr<f32> to i64
    %76 = llvm.and %75, %74  : i64
    %77 = llvm.icmp "eq" %76, %73 : i64
    "llvm.intr.assume"(%77) : (i1) -> ()
    %78 = nvvm.read.ptx.sreg.ctaid.x : i32
    %79 = llvm.sext %78 : i32 to i64
    %80 = nvvm.read.ptx.sreg.ctaid.y : i32
    %81 = llvm.sext %80 : i32 to i64
    %82 = llvm.mul %81, %10  : i64
    %83 = llvm.mul %81, %6  : i64
    %84 = llvm.add %83, %5  : i64
    %85 = llvm.icmp "slt" %84, %10 : i64
    %86 = llvm.select %85, %84, %10 : i1, i64
    %87 = llvm.mul %79, %4  : i64
    %88 = llvm.mul %79, %3  : i64
    %89 = llvm.add %88, %2  : i64
    %90 = llvm.icmp "slt" %89, %4 : i64
    %91 = llvm.select %90, %89, %4 : i1, i64
    %92 = nvvm.read.ptx.sreg.tid.x : i32
    %93 = llvm.sext %92 : i32 to i64
    %94 = nvvm.read.ptx.sreg.tid.y : i32
    %95 = llvm.sext %94 : i32 to i64
    %96 = llvm.icmp "sle" %86, %7 : i64
    %97 = llvm.sub %7, %86  : i64
    %98 = llvm.sub %86, %8  : i64
    %99 = llvm.select %96, %97, %98 : i1, i64
    %100 = llvm.sdiv %99, %1  : i64
    %101 = llvm.sub %7, %100  : i64
    %102 = llvm.add %100, %8  : i64
    %103 = llvm.select %96, %101, %102 : i1, i64
    %104 = llvm.mul %95, %103  : i64
    %105 = llvm.mul %104, %0  : i64
    %106 = llvm.add %86, %105  : i64
    %107 = llvm.icmp "slt" %106, %103 : i64
    %108 = llvm.select %107, %106, %103 : i1, i64
    %109 = llvm.icmp "slt" %108, %7 : i64
    %110 = llvm.select %109, %7, %108 : i1, i64
    %111 = llvm.icmp "sle" %91, %7 : i64
    %112 = llvm.sub %7, %91  : i64
    %113 = llvm.sub %91, %8  : i64
    %114 = llvm.select %111, %112, %113 : i1, i64
    %115 = llvm.sdiv %114, %10  : i64
    %116 = llvm.sub %7, %115  : i64
    %117 = llvm.add %115, %8  : i64
    %118 = llvm.select %111, %116, %117 : i1, i64
    %119 = llvm.mul %93, %118  : i64
    %120 = llvm.mul %119, %0  : i64
    %121 = llvm.add %91, %120  : i64
    %122 = llvm.icmp "slt" %121, %118 : i64
    %123 = llvm.select %122, %121, %118 : i1, i64
    %124 = llvm.icmp "slt" %123, %7 : i64
    %125 = llvm.select %124, %7, %123 : i1, i64
    llvm.br ^bb1(%7 : i64)
  ^bb1(%126: i64):  // 2 preds: ^bb0, ^bb4
    %127 = llvm.icmp "slt" %126, %110 : i64
    llvm.cond_br %127, ^bb2(%7 : i64), ^bb5(%7 : i64)
  ^bb2(%128: i64):  // 2 preds: ^bb1, ^bb3
    %129 = llvm.icmp "slt" %128, %125 : i64
    llvm.cond_br %129, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %130 = llvm.add %82, %104  : i64
    %131 = llvm.add %130, %126  : i64
    %132 = llvm.add %87, %119  : i64
    %133 = llvm.add %132, %128  : i64
    %134 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %135 = llvm.mlir.constant(1024 : index) : i64
    %136 = llvm.mul %131, %135  : i64
    %137 = llvm.add %136, %133  : i64
    %138 = llvm.getelementptr %134[%137] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
    llvm.store %11, %138 : !llvm.ptr<f32>
    %139 = llvm.add %128, %8  : i64
    llvm.br ^bb2(%139 : i64)
  ^bb4:  // pred: ^bb2
    %140 = llvm.add %126, %8  : i64
    llvm.br ^bb1(%140 : i64)
  ^bb5(%141: i64):  // 2 preds: ^bb1, ^bb12
    %142 = llvm.icmp "slt" %141, %9 : i64
    llvm.cond_br %142, ^bb6(%7 : i64), ^bb13
  ^bb6(%143: i64):  // 2 preds: ^bb5, ^bb11
    %144 = llvm.icmp "slt" %143, %110 : i64
    llvm.cond_br %144, ^bb7(%7 : i64), ^bb12
  ^bb7(%145: i64):  // 2 preds: ^bb6, ^bb10
    %146 = llvm.icmp "slt" %145, %125 : i64
    llvm.cond_br %146, ^bb8(%7 : i64), ^bb11
  ^bb8(%147: i64):  // 2 preds: ^bb7, ^bb9
    %148 = llvm.icmp "slt" %147, %10 : i64
    llvm.cond_br %148, ^bb9, ^bb10
  ^bb9:  // pred: ^bb8
    %149 = llvm.add %141, %147  : i64
    %150 = llvm.add %82, %104  : i64
    %151 = llvm.add %150, %143  : i64
    %152 = llvm.extractvalue %27[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %153 = llvm.mlir.constant(256 : index) : i64
    %154 = llvm.mul %151, %153  : i64
    %155 = llvm.add %154, %149  : i64
    %156 = llvm.getelementptr %152[%155] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
    %157 = llvm.load %156 : !llvm.ptr<f32>
    %158 = llvm.add %87, %119  : i64
    %159 = llvm.add %158, %145  : i64
    %160 = llvm.extractvalue %49[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %161 = llvm.mlir.constant(1024 : index) : i64
    %162 = llvm.mul %149, %161  : i64
    %163 = llvm.add %162, %159  : i64
    %164 = llvm.getelementptr %160[%163] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
    %165 = llvm.load %164 : !llvm.ptr<f32>
    %166 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %167 = llvm.mlir.constant(1024 : index) : i64
    %168 = llvm.mul %151, %167  : i64
    %169 = llvm.add %168, %159  : i64
    %170 = llvm.getelementptr %166[%169] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
    %171 = llvm.load %170 : !llvm.ptr<f32>
    %172 = llvm.fmul %157, %165  : f32
    %173 = llvm.fadd %171, %172  : f32
    %174 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %175 = llvm.mlir.constant(1024 : index) : i64
    %176 = llvm.mul %151, %175  : i64
    %177 = llvm.add %176, %159  : i64
    %178 = llvm.getelementptr %174[%177] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
    llvm.store %173, %178 : !llvm.ptr<f32>
    %179 = llvm.add %147, %8  : i64
    llvm.br ^bb8(%179 : i64)
  ^bb10:  // pred: ^bb8
    %180 = llvm.add %145, %8  : i64
    llvm.br ^bb7(%180 : i64)
  ^bb11:  // pred: ^bb7
    %181 = llvm.add %143, %8  : i64
    llvm.br ^bb6(%181 : i64)
  ^bb12:  // pred: ^bb6
    %182 = llvm.add %141, %10  : i64
    llvm.br ^bb5(%182 : i64)
  ^bb13:  // pred: ^bb5
    llvm.return
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::TranslateTargetExecutableVariantsPass (iree-hal-translate-target-executable-variants) //----- //
hal.executable.variant public @cuda_nvptx_fb, target = <"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}> {
  hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<LLVMGPUMatmulSimt>, workgroup_size = [32 : index, 8 : index, 1 : index]} {
  ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
    %c8 = arith.constant 8 : index
    %c16 = arith.constant 16 : index
    %c1 = arith.constant 1 : index
    hal.return %c8, %c16, %c1 : index, index, index
  }
  builtin.module {
    llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
      %0 = llvm.mlir.constant(-1 : index) : i64
      %1 = llvm.mlir.constant(8 : index) : i64
      %2 = llvm.mlir.constant(1024 : index) : i64
      %3 = llvm.mlir.constant(-128 : index) : i64
      %4 = llvm.mlir.constant(128 : index) : i64
      %5 = llvm.mlir.constant(512 : index) : i64
      %6 = llvm.mlir.constant(-32 : index) : i64
      %7 = llvm.mlir.constant(0 : index) : i64
      %8 = llvm.mlir.constant(1 : index) : i64
      %9 = llvm.mlir.constant(256 : index) : i64
      %10 = llvm.mlir.constant(32 : index) : i64
      %11 = llvm.mlir.constant(0.000000e+00 : f32) : f32
      %12 = llvm.bitcast %arg0 : !llvm.ptr<f32> to !llvm.ptr<i8>
      %13 = llvm.getelementptr %12[%7] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
      %14 = llvm.bitcast %13 : !llvm.ptr<i8> to !llvm.ptr<f32>
      %15 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>
      %16 = llvm.insertvalue %14, %15[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %17 = llvm.insertvalue %14, %16[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %18 = llvm.mlir.constant(0 : index) : i64
      %19 = llvm.insertvalue %18, %17[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %20 = llvm.mlir.constant(512 : index) : i64
      %21 = llvm.insertvalue %20, %19[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %22 = llvm.mlir.constant(256 : index) : i64
      %23 = llvm.insertvalue %22, %21[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %24 = llvm.mlir.constant(256 : index) : i64
      %25 = llvm.insertvalue %24, %23[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %26 = llvm.mlir.constant(1 : index) : i64
      %27 = llvm.insertvalue %26, %25[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %28 = llvm.extractvalue %27[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %29 = llvm.mlir.constant(0 : index) : i64
      %30 = llvm.mlir.constant(63 : index) : i64
      %31 = llvm.ptrtoint %28 : !llvm.ptr<f32> to i64
      %32 = llvm.and %31, %30  : i64
      %33 = llvm.icmp "eq" %32, %29 : i64
      "llvm.intr.assume"(%33) : (i1) -> ()
      %34 = llvm.bitcast %arg1 : !llvm.ptr<f32> to !llvm.ptr<i8>
      %35 = llvm.getelementptr %34[%7] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
      %36 = llvm.bitcast %35 : !llvm.ptr<i8> to !llvm.ptr<f32>
      %37 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>
      %38 = llvm.insertvalue %36, %37[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %39 = llvm.insertvalue %36, %38[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %40 = llvm.mlir.constant(0 : index) : i64
      %41 = llvm.insertvalue %40, %39[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %42 = llvm.mlir.constant(256 : index) : i64
      %43 = llvm.insertvalue %42, %41[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %44 = llvm.mlir.constant(1024 : index) : i64
      %45 = llvm.insertvalue %44, %43[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %46 = llvm.mlir.constant(1024 : index) : i64
      %47 = llvm.insertvalue %46, %45[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %48 = llvm.mlir.constant(1 : index) : i64
      %49 = llvm.insertvalue %48, %47[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %50 = llvm.extractvalue %49[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %51 = llvm.mlir.constant(0 : index) : i64
      %52 = llvm.mlir.constant(63 : index) : i64
      %53 = llvm.ptrtoint %50 : !llvm.ptr<f32> to i64
      %54 = llvm.and %53, %52  : i64
      %55 = llvm.icmp "eq" %54, %51 : i64
      "llvm.intr.assume"(%55) : (i1) -> ()
      %56 = llvm.bitcast %arg2 : !llvm.ptr<f32> to !llvm.ptr<i8>
      %57 = llvm.getelementptr %56[%7] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
      %58 = llvm.bitcast %57 : !llvm.ptr<i8> to !llvm.ptr<f32>
      %59 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>
      %60 = llvm.insertvalue %58, %59[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %61 = llvm.insertvalue %58, %60[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %62 = llvm.mlir.constant(0 : index) : i64
      %63 = llvm.insertvalue %62, %61[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %64 = llvm.mlir.constant(512 : index) : i64
      %65 = llvm.insertvalue %64, %63[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %66 = llvm.mlir.constant(1024 : index) : i64
      %67 = llvm.insertvalue %66, %65[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %68 = llvm.mlir.constant(1024 : index) : i64
      %69 = llvm.insertvalue %68, %67[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %70 = llvm.mlir.constant(1 : index) : i64
      %71 = llvm.insertvalue %70, %69[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %72 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %73 = llvm.mlir.constant(0 : index) : i64
      %74 = llvm.mlir.constant(63 : index) : i64
      %75 = llvm.ptrtoint %72 : !llvm.ptr<f32> to i64
      %76 = llvm.and %75, %74  : i64
      %77 = llvm.icmp "eq" %76, %73 : i64
      "llvm.intr.assume"(%77) : (i1) -> ()
      %78 = nvvm.read.ptx.sreg.ctaid.x : i32
      %79 = llvm.sext %78 : i32 to i64
      %80 = nvvm.read.ptx.sreg.ctaid.y : i32
      %81 = llvm.sext %80 : i32 to i64
      %82 = llvm.mul %81, %10  : i64
      %83 = llvm.mul %81, %6  : i64
      %84 = llvm.add %83, %5  : i64
      %85 = llvm.icmp "slt" %84, %10 : i64
      %86 = llvm.select %85, %84, %10 : i1, i64
      %87 = llvm.mul %79, %4  : i64
      %88 = llvm.mul %79, %3  : i64
      %89 = llvm.add %88, %2  : i64
      %90 = llvm.icmp "slt" %89, %4 : i64
      %91 = llvm.select %90, %89, %4 : i1, i64
      %92 = nvvm.read.ptx.sreg.tid.x : i32
      %93 = llvm.sext %92 : i32 to i64
      %94 = nvvm.read.ptx.sreg.tid.y : i32
      %95 = llvm.sext %94 : i32 to i64
      %96 = llvm.icmp "sle" %86, %7 : i64
      %97 = llvm.sub %7, %86  : i64
      %98 = llvm.sub %86, %8  : i64
      %99 = llvm.select %96, %97, %98 : i1, i64
      %100 = llvm.sdiv %99, %1  : i64
      %101 = llvm.sub %7, %100  : i64
      %102 = llvm.add %100, %8  : i64
      %103 = llvm.select %96, %101, %102 : i1, i64
      %104 = llvm.mul %95, %103  : i64
      %105 = llvm.mul %104, %0  : i64
      %106 = llvm.add %86, %105  : i64
      %107 = llvm.icmp "slt" %106, %103 : i64
      %108 = llvm.select %107, %106, %103 : i1, i64
      %109 = llvm.icmp "slt" %108, %7 : i64
      %110 = llvm.select %109, %7, %108 : i1, i64
      %111 = llvm.icmp "sle" %91, %7 : i64
      %112 = llvm.sub %7, %91  : i64
      %113 = llvm.sub %91, %8  : i64
      %114 = llvm.select %111, %112, %113 : i1, i64
      %115 = llvm.sdiv %114, %10  : i64
      %116 = llvm.sub %7, %115  : i64
      %117 = llvm.add %115, %8  : i64
      %118 = llvm.select %111, %116, %117 : i1, i64
      %119 = llvm.mul %93, %118  : i64
      %120 = llvm.mul %119, %0  : i64
      %121 = llvm.add %91, %120  : i64
      %122 = llvm.icmp "slt" %121, %118 : i64
      %123 = llvm.select %122, %121, %118 : i1, i64
      %124 = llvm.icmp "slt" %123, %7 : i64
      %125 = llvm.select %124, %7, %123 : i1, i64
      llvm.br ^bb1(%7 : i64)
    ^bb1(%126: i64):  // 2 preds: ^bb0, ^bb4
      %127 = llvm.icmp "slt" %126, %110 : i64
      llvm.cond_br %127, ^bb2(%7 : i64), ^bb5(%7 : i64)
    ^bb2(%128: i64):  // 2 preds: ^bb1, ^bb3
      %129 = llvm.icmp "slt" %128, %125 : i64
      llvm.cond_br %129, ^bb3, ^bb4
    ^bb3:  // pred: ^bb2
      %130 = llvm.add %82, %104  : i64
      %131 = llvm.add %130, %126  : i64
      %132 = llvm.add %87, %119  : i64
      %133 = llvm.add %132, %128  : i64
      %134 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %135 = llvm.mlir.constant(1024 : index) : i64
      %136 = llvm.mul %131, %135  : i64
      %137 = llvm.add %136, %133  : i64
      %138 = llvm.getelementptr %134[%137] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
      llvm.store %11, %138 : !llvm.ptr<f32>
      %139 = llvm.add %128, %8  : i64
      llvm.br ^bb2(%139 : i64)
    ^bb4:  // pred: ^bb2
      %140 = llvm.add %126, %8  : i64
      llvm.br ^bb1(%140 : i64)
    ^bb5(%141: i64):  // 2 preds: ^bb1, ^bb12
      %142 = llvm.icmp "slt" %141, %9 : i64
      llvm.cond_br %142, ^bb6(%7 : i64), ^bb13
    ^bb6(%143: i64):  // 2 preds: ^bb5, ^bb11
      %144 = llvm.icmp "slt" %143, %110 : i64
      llvm.cond_br %144, ^bb7(%7 : i64), ^bb12
    ^bb7(%145: i64):  // 2 preds: ^bb6, ^bb10
      %146 = llvm.icmp "slt" %145, %125 : i64
      llvm.cond_br %146, ^bb8(%7 : i64), ^bb11
    ^bb8(%147: i64):  // 2 preds: ^bb7, ^bb9
      %148 = llvm.icmp "slt" %147, %10 : i64
      llvm.cond_br %148, ^bb9, ^bb10
    ^bb9:  // pred: ^bb8
      %149 = llvm.add %141, %147  : i64
      %150 = llvm.add %82, %104  : i64
      %151 = llvm.add %150, %143  : i64
      %152 = llvm.extractvalue %27[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %153 = llvm.mlir.constant(256 : index) : i64
      %154 = llvm.mul %151, %153  : i64
      %155 = llvm.add %154, %149  : i64
      %156 = llvm.getelementptr %152[%155] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
      %157 = llvm.load %156 : !llvm.ptr<f32>
      %158 = llvm.add %87, %119  : i64
      %159 = llvm.add %158, %145  : i64
      %160 = llvm.extractvalue %49[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %161 = llvm.mlir.constant(1024 : index) : i64
      %162 = llvm.mul %149, %161  : i64
      %163 = llvm.add %162, %159  : i64
      %164 = llvm.getelementptr %160[%163] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
      %165 = llvm.load %164 : !llvm.ptr<f32>
      %166 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %167 = llvm.mlir.constant(1024 : index) : i64
      %168 = llvm.mul %151, %167  : i64
      %169 = llvm.add %168, %159  : i64
      %170 = llvm.getelementptr %166[%169] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
      %171 = llvm.load %170 : !llvm.ptr<f32>
      %172 = llvm.fmul %157, %165  : f32
      %173 = llvm.fadd %171, %172  : f32
      %174 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
      %175 = llvm.mlir.constant(1024 : index) : i64
      %176 = llvm.mul %151, %175  : i64
      %177 = llvm.add %176, %159  : i64
      %178 = llvm.getelementptr %174[%177] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
      llvm.store %173, %178 : !llvm.ptr<f32>
      %179 = llvm.add %147, %8  : i64
      llvm.br ^bb8(%179 : i64)
    ^bb10:  // pred: ^bb8
      %180 = llvm.add %145, %8  : i64
      llvm.br ^bb7(%180 : i64)
    ^bb11:  // pred: ^bb7
      %181 = llvm.add %143, %8  : i64
      llvm.br ^bb6(%181 : i64)
    ^bb12:  // pred: ^bb6
      %182 = llvm.add %141, %10  : i64
      llvm.br ^bb5(%182 : i64)
    ^bb13:  // pred: ^bb5
      llvm.return
    }
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::TranslateExecutablesPass (iree-hal-translate-executables) //----- //
hal.executable private @matmul_static_dispatch_0 {
  hal.executable.variant public @cuda_nvptx_fb, target = <"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}> {
    hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<LLVMGPUMatmulSimt>, workgroup_size = [32 : index, 8 : index, 1 : index]} {
    ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
      %c8 = arith.constant 8 : index
      %c16 = arith.constant 16 : index
      %c1 = arith.constant 1 : index
      hal.return %c8, %c16, %c1 : index, index, index
    }
    builtin.module {
      llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
        %0 = llvm.mlir.constant(-1 : index) : i64
        %1 = llvm.mlir.constant(8 : index) : i64
        %2 = llvm.mlir.constant(1024 : index) : i64
        %3 = llvm.mlir.constant(-128 : index) : i64
        %4 = llvm.mlir.constant(128 : index) : i64
        %5 = llvm.mlir.constant(512 : index) : i64
        %6 = llvm.mlir.constant(-32 : index) : i64
        %7 = llvm.mlir.constant(0 : index) : i64
        %8 = llvm.mlir.constant(1 : index) : i64
        %9 = llvm.mlir.constant(256 : index) : i64
        %10 = llvm.mlir.constant(32 : index) : i64
        %11 = llvm.mlir.constant(0.000000e+00 : f32) : f32
        %12 = llvm.bitcast %arg0 : !llvm.ptr<f32> to !llvm.ptr<i8>
        %13 = llvm.getelementptr %12[%7] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
        %14 = llvm.bitcast %13 : !llvm.ptr<i8> to !llvm.ptr<f32>
        %15 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>
        %16 = llvm.insertvalue %14, %15[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %17 = llvm.insertvalue %14, %16[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %18 = llvm.mlir.constant(0 : index) : i64
        %19 = llvm.insertvalue %18, %17[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %20 = llvm.mlir.constant(512 : index) : i64
        %21 = llvm.insertvalue %20, %19[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %22 = llvm.mlir.constant(256 : index) : i64
        %23 = llvm.insertvalue %22, %21[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %24 = llvm.mlir.constant(256 : index) : i64
        %25 = llvm.insertvalue %24, %23[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %26 = llvm.mlir.constant(1 : index) : i64
        %27 = llvm.insertvalue %26, %25[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %28 = llvm.extractvalue %27[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %29 = llvm.mlir.constant(0 : index) : i64
        %30 = llvm.mlir.constant(63 : index) : i64
        %31 = llvm.ptrtoint %28 : !llvm.ptr<f32> to i64
        %32 = llvm.and %31, %30  : i64
        %33 = llvm.icmp "eq" %32, %29 : i64
        "llvm.intr.assume"(%33) : (i1) -> ()
        %34 = llvm.bitcast %arg1 : !llvm.ptr<f32> to !llvm.ptr<i8>
        %35 = llvm.getelementptr %34[%7] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
        %36 = llvm.bitcast %35 : !llvm.ptr<i8> to !llvm.ptr<f32>
        %37 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>
        %38 = llvm.insertvalue %36, %37[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %39 = llvm.insertvalue %36, %38[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %40 = llvm.mlir.constant(0 : index) : i64
        %41 = llvm.insertvalue %40, %39[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %42 = llvm.mlir.constant(256 : index) : i64
        %43 = llvm.insertvalue %42, %41[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %44 = llvm.mlir.constant(1024 : index) : i64
        %45 = llvm.insertvalue %44, %43[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %46 = llvm.mlir.constant(1024 : index) : i64
        %47 = llvm.insertvalue %46, %45[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %48 = llvm.mlir.constant(1 : index) : i64
        %49 = llvm.insertvalue %48, %47[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %50 = llvm.extractvalue %49[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %51 = llvm.mlir.constant(0 : index) : i64
        %52 = llvm.mlir.constant(63 : index) : i64
        %53 = llvm.ptrtoint %50 : !llvm.ptr<f32> to i64
        %54 = llvm.and %53, %52  : i64
        %55 = llvm.icmp "eq" %54, %51 : i64
        "llvm.intr.assume"(%55) : (i1) -> ()
        %56 = llvm.bitcast %arg2 : !llvm.ptr<f32> to !llvm.ptr<i8>
        %57 = llvm.getelementptr %56[%7] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
        %58 = llvm.bitcast %57 : !llvm.ptr<i8> to !llvm.ptr<f32>
        %59 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>
        %60 = llvm.insertvalue %58, %59[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %61 = llvm.insertvalue %58, %60[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %62 = llvm.mlir.constant(0 : index) : i64
        %63 = llvm.insertvalue %62, %61[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %64 = llvm.mlir.constant(512 : index) : i64
        %65 = llvm.insertvalue %64, %63[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %66 = llvm.mlir.constant(1024 : index) : i64
        %67 = llvm.insertvalue %66, %65[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %68 = llvm.mlir.constant(1024 : index) : i64
        %69 = llvm.insertvalue %68, %67[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %70 = llvm.mlir.constant(1 : index) : i64
        %71 = llvm.insertvalue %70, %69[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %72 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %73 = llvm.mlir.constant(0 : index) : i64
        %74 = llvm.mlir.constant(63 : index) : i64
        %75 = llvm.ptrtoint %72 : !llvm.ptr<f32> to i64
        %76 = llvm.and %75, %74  : i64
        %77 = llvm.icmp "eq" %76, %73 : i64
        "llvm.intr.assume"(%77) : (i1) -> ()
        %78 = nvvm.read.ptx.sreg.ctaid.x : i32
        %79 = llvm.sext %78 : i32 to i64
        %80 = nvvm.read.ptx.sreg.ctaid.y : i32
        %81 = llvm.sext %80 : i32 to i64
        %82 = llvm.mul %81, %10  : i64
        %83 = llvm.mul %81, %6  : i64
        %84 = llvm.add %83, %5  : i64
        %85 = llvm.icmp "slt" %84, %10 : i64
        %86 = llvm.select %85, %84, %10 : i1, i64
        %87 = llvm.mul %79, %4  : i64
        %88 = llvm.mul %79, %3  : i64
        %89 = llvm.add %88, %2  : i64
        %90 = llvm.icmp "slt" %89, %4 : i64
        %91 = llvm.select %90, %89, %4 : i1, i64
        %92 = nvvm.read.ptx.sreg.tid.x : i32
        %93 = llvm.sext %92 : i32 to i64
        %94 = nvvm.read.ptx.sreg.tid.y : i32
        %95 = llvm.sext %94 : i32 to i64
        %96 = llvm.icmp "sle" %86, %7 : i64
        %97 = llvm.sub %7, %86  : i64
        %98 = llvm.sub %86, %8  : i64
        %99 = llvm.select %96, %97, %98 : i1, i64
        %100 = llvm.sdiv %99, %1  : i64
        %101 = llvm.sub %7, %100  : i64
        %102 = llvm.add %100, %8  : i64
        %103 = llvm.select %96, %101, %102 : i1, i64
        %104 = llvm.mul %95, %103  : i64
        %105 = llvm.mul %104, %0  : i64
        %106 = llvm.add %86, %105  : i64
        %107 = llvm.icmp "slt" %106, %103 : i64
        %108 = llvm.select %107, %106, %103 : i1, i64
        %109 = llvm.icmp "slt" %108, %7 : i64
        %110 = llvm.select %109, %7, %108 : i1, i64
        %111 = llvm.icmp "sle" %91, %7 : i64
        %112 = llvm.sub %7, %91  : i64
        %113 = llvm.sub %91, %8  : i64
        %114 = llvm.select %111, %112, %113 : i1, i64
        %115 = llvm.sdiv %114, %10  : i64
        %116 = llvm.sub %7, %115  : i64
        %117 = llvm.add %115, %8  : i64
        %118 = llvm.select %111, %116, %117 : i1, i64
        %119 = llvm.mul %93, %118  : i64
        %120 = llvm.mul %119, %0  : i64
        %121 = llvm.add %91, %120  : i64
        %122 = llvm.icmp "slt" %121, %118 : i64
        %123 = llvm.select %122, %121, %118 : i1, i64
        %124 = llvm.icmp "slt" %123, %7 : i64
        %125 = llvm.select %124, %7, %123 : i1, i64
        llvm.br ^bb1(%7 : i64)
      ^bb1(%126: i64):  // 2 preds: ^bb0, ^bb4
        %127 = llvm.icmp "slt" %126, %110 : i64
        llvm.cond_br %127, ^bb2(%7 : i64), ^bb5(%7 : i64)
      ^bb2(%128: i64):  // 2 preds: ^bb1, ^bb3
        %129 = llvm.icmp "slt" %128, %125 : i64
        llvm.cond_br %129, ^bb3, ^bb4
      ^bb3:  // pred: ^bb2
        %130 = llvm.add %82, %104  : i64
        %131 = llvm.add %130, %126  : i64
        %132 = llvm.add %87, %119  : i64
        %133 = llvm.add %132, %128  : i64
        %134 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %135 = llvm.mlir.constant(1024 : index) : i64
        %136 = llvm.mul %131, %135  : i64
        %137 = llvm.add %136, %133  : i64
        %138 = llvm.getelementptr %134[%137] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
        llvm.store %11, %138 : !llvm.ptr<f32>
        %139 = llvm.add %128, %8  : i64
        llvm.br ^bb2(%139 : i64)
      ^bb4:  // pred: ^bb2
        %140 = llvm.add %126, %8  : i64
        llvm.br ^bb1(%140 : i64)
      ^bb5(%141: i64):  // 2 preds: ^bb1, ^bb12
        %142 = llvm.icmp "slt" %141, %9 : i64
        llvm.cond_br %142, ^bb6(%7 : i64), ^bb13
      ^bb6(%143: i64):  // 2 preds: ^bb5, ^bb11
        %144 = llvm.icmp "slt" %143, %110 : i64
        llvm.cond_br %144, ^bb7(%7 : i64), ^bb12
      ^bb7(%145: i64):  // 2 preds: ^bb6, ^bb10
        %146 = llvm.icmp "slt" %145, %125 : i64
        llvm.cond_br %146, ^bb8(%7 : i64), ^bb11
      ^bb8(%147: i64):  // 2 preds: ^bb7, ^bb9
        %148 = llvm.icmp "slt" %147, %10 : i64
        llvm.cond_br %148, ^bb9, ^bb10
      ^bb9:  // pred: ^bb8
        %149 = llvm.add %141, %147  : i64
        %150 = llvm.add %82, %104  : i64
        %151 = llvm.add %150, %143  : i64
        %152 = llvm.extractvalue %27[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %153 = llvm.mlir.constant(256 : index) : i64
        %154 = llvm.mul %151, %153  : i64
        %155 = llvm.add %154, %149  : i64
        %156 = llvm.getelementptr %152[%155] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
        %157 = llvm.load %156 : !llvm.ptr<f32>
        %158 = llvm.add %87, %119  : i64
        %159 = llvm.add %158, %145  : i64
        %160 = llvm.extractvalue %49[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %161 = llvm.mlir.constant(1024 : index) : i64
        %162 = llvm.mul %149, %161  : i64
        %163 = llvm.add %162, %159  : i64
        %164 = llvm.getelementptr %160[%163] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
        %165 = llvm.load %164 : !llvm.ptr<f32>
        %166 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %167 = llvm.mlir.constant(1024 : index) : i64
        %168 = llvm.mul %151, %167  : i64
        %169 = llvm.add %168, %159  : i64
        %170 = llvm.getelementptr %166[%169] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
        %171 = llvm.load %170 : !llvm.ptr<f32>
        %172 = llvm.fmul %157, %165  : f32
        %173 = llvm.fadd %171, %172  : f32
        %174 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
        %175 = llvm.mlir.constant(1024 : index) : i64
        %176 = llvm.mul %151, %175  : i64
        %177 = llvm.add %176, %159  : i64
        %178 = llvm.getelementptr %174[%177] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
        llvm.store %173, %178 : !llvm.ptr<f32>
        %179 = llvm.add %147, %8  : i64
        llvm.br ^bb8(%179 : i64)
      ^bb10:  // pred: ^bb8
        %180 = llvm.add %145, %8  : i64
        llvm.br ^bb7(%180 : i64)
      ^bb11:  // pred: ^bb7
        %181 = llvm.add %143, %8  : i64
        llvm.br ^bb6(%181 : i64)
      ^bb12:  // pred: ^bb6
        %182 = llvm.add %141, %10  : i64
        llvm.br ^bb5(%182 : i64)
      ^bb13:  // pred: ^bb5
        llvm.return
      }
    }
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::(anonymous namespace)::ConvertToHALPass (iree-hal-conversion) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(-1 : index) : i64
          %1 = llvm.mlir.constant(8 : index) : i64
          %2 = llvm.mlir.constant(1024 : index) : i64
          %3 = llvm.mlir.constant(-128 : index) : i64
          %4 = llvm.mlir.constant(128 : index) : i64
          %5 = llvm.mlir.constant(512 : index) : i64
          %6 = llvm.mlir.constant(-32 : index) : i64
          %7 = llvm.mlir.constant(0 : index) : i64
          %8 = llvm.mlir.constant(1 : index) : i64
          %9 = llvm.mlir.constant(256 : index) : i64
          %10 = llvm.mlir.constant(32 : index) : i64
          %11 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %12 = llvm.bitcast %arg0 : !llvm.ptr<f32> to !llvm.ptr<i8>
          %13 = llvm.getelementptr %12[%7] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
          %14 = llvm.bitcast %13 : !llvm.ptr<i8> to !llvm.ptr<f32>
          %15 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>
          %16 = llvm.insertvalue %14, %15[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %17 = llvm.insertvalue %14, %16[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %18 = llvm.mlir.constant(0 : index) : i64
          %19 = llvm.insertvalue %18, %17[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %20 = llvm.mlir.constant(512 : index) : i64
          %21 = llvm.insertvalue %20, %19[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %22 = llvm.mlir.constant(256 : index) : i64
          %23 = llvm.insertvalue %22, %21[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %24 = llvm.mlir.constant(256 : index) : i64
          %25 = llvm.insertvalue %24, %23[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %26 = llvm.mlir.constant(1 : index) : i64
          %27 = llvm.insertvalue %26, %25[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %28 = llvm.extractvalue %27[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %29 = llvm.mlir.constant(0 : index) : i64
          %30 = llvm.mlir.constant(63 : index) : i64
          %31 = llvm.ptrtoint %28 : !llvm.ptr<f32> to i64
          %32 = llvm.and %31, %30  : i64
          %33 = llvm.icmp "eq" %32, %29 : i64
          "llvm.intr.assume"(%33) : (i1) -> ()
          %34 = llvm.bitcast %arg1 : !llvm.ptr<f32> to !llvm.ptr<i8>
          %35 = llvm.getelementptr %34[%7] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
          %36 = llvm.bitcast %35 : !llvm.ptr<i8> to !llvm.ptr<f32>
          %37 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>
          %38 = llvm.insertvalue %36, %37[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %39 = llvm.insertvalue %36, %38[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %40 = llvm.mlir.constant(0 : index) : i64
          %41 = llvm.insertvalue %40, %39[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %42 = llvm.mlir.constant(256 : index) : i64
          %43 = llvm.insertvalue %42, %41[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %44 = llvm.mlir.constant(1024 : index) : i64
          %45 = llvm.insertvalue %44, %43[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %46 = llvm.mlir.constant(1024 : index) : i64
          %47 = llvm.insertvalue %46, %45[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %48 = llvm.mlir.constant(1 : index) : i64
          %49 = llvm.insertvalue %48, %47[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %50 = llvm.extractvalue %49[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %51 = llvm.mlir.constant(0 : index) : i64
          %52 = llvm.mlir.constant(63 : index) : i64
          %53 = llvm.ptrtoint %50 : !llvm.ptr<f32> to i64
          %54 = llvm.and %53, %52  : i64
          %55 = llvm.icmp "eq" %54, %51 : i64
          "llvm.intr.assume"(%55) : (i1) -> ()
          %56 = llvm.bitcast %arg2 : !llvm.ptr<f32> to !llvm.ptr<i8>
          %57 = llvm.getelementptr %56[%7] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
          %58 = llvm.bitcast %57 : !llvm.ptr<i8> to !llvm.ptr<f32>
          %59 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>
          %60 = llvm.insertvalue %58, %59[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %61 = llvm.insertvalue %58, %60[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %62 = llvm.mlir.constant(0 : index) : i64
          %63 = llvm.insertvalue %62, %61[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %64 = llvm.mlir.constant(512 : index) : i64
          %65 = llvm.insertvalue %64, %63[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %66 = llvm.mlir.constant(1024 : index) : i64
          %67 = llvm.insertvalue %66, %65[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %68 = llvm.mlir.constant(1024 : index) : i64
          %69 = llvm.insertvalue %68, %67[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %70 = llvm.mlir.constant(1 : index) : i64
          %71 = llvm.insertvalue %70, %69[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %72 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %73 = llvm.mlir.constant(0 : index) : i64
          %74 = llvm.mlir.constant(63 : index) : i64
          %75 = llvm.ptrtoint %72 : !llvm.ptr<f32> to i64
          %76 = llvm.and %75, %74  : i64
          %77 = llvm.icmp "eq" %76, %73 : i64
          "llvm.intr.assume"(%77) : (i1) -> ()
          %78 = nvvm.read.ptx.sreg.ctaid.x : i32
          %79 = llvm.sext %78 : i32 to i64
          %80 = nvvm.read.ptx.sreg.ctaid.y : i32
          %81 = llvm.sext %80 : i32 to i64
          %82 = llvm.mul %81, %10  : i64
          %83 = llvm.mul %81, %6  : i64
          %84 = llvm.add %83, %5  : i64
          %85 = llvm.icmp "slt" %84, %10 : i64
          %86 = llvm.select %85, %84, %10 : i1, i64
          %87 = llvm.mul %79, %4  : i64
          %88 = llvm.mul %79, %3  : i64
          %89 = llvm.add %88, %2  : i64
          %90 = llvm.icmp "slt" %89, %4 : i64
          %91 = llvm.select %90, %89, %4 : i1, i64
          %92 = nvvm.read.ptx.sreg.tid.x : i32
          %93 = llvm.sext %92 : i32 to i64
          %94 = nvvm.read.ptx.sreg.tid.y : i32
          %95 = llvm.sext %94 : i32 to i64
          %96 = llvm.icmp "sle" %86, %7 : i64
          %97 = llvm.sub %7, %86  : i64
          %98 = llvm.sub %86, %8  : i64
          %99 = llvm.select %96, %97, %98 : i1, i64
          %100 = llvm.sdiv %99, %1  : i64
          %101 = llvm.sub %7, %100  : i64
          %102 = llvm.add %100, %8  : i64
          %103 = llvm.select %96, %101, %102 : i1, i64
          %104 = llvm.mul %95, %103  : i64
          %105 = llvm.mul %104, %0  : i64
          %106 = llvm.add %86, %105  : i64
          %107 = llvm.icmp "slt" %106, %103 : i64
          %108 = llvm.select %107, %106, %103 : i1, i64
          %109 = llvm.icmp "slt" %108, %7 : i64
          %110 = llvm.select %109, %7, %108 : i1, i64
          %111 = llvm.icmp "sle" %91, %7 : i64
          %112 = llvm.sub %7, %91  : i64
          %113 = llvm.sub %91, %8  : i64
          %114 = llvm.select %111, %112, %113 : i1, i64
          %115 = llvm.sdiv %114, %10  : i64
          %116 = llvm.sub %7, %115  : i64
          %117 = llvm.add %115, %8  : i64
          %118 = llvm.select %111, %116, %117 : i1, i64
          %119 = llvm.mul %93, %118  : i64
          %120 = llvm.mul %119, %0  : i64
          %121 = llvm.add %91, %120  : i64
          %122 = llvm.icmp "slt" %121, %118 : i64
          %123 = llvm.select %122, %121, %118 : i1, i64
          %124 = llvm.icmp "slt" %123, %7 : i64
          %125 = llvm.select %124, %7, %123 : i1, i64
          llvm.br ^bb1(%7 : i64)
        ^bb1(%126: i64):  // 2 preds: ^bb0, ^bb4
          %127 = llvm.icmp "slt" %126, %110 : i64
          llvm.cond_br %127, ^bb2(%7 : i64), ^bb5(%7 : i64)
        ^bb2(%128: i64):  // 2 preds: ^bb1, ^bb3
          %129 = llvm.icmp "slt" %128, %125 : i64
          llvm.cond_br %129, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %130 = llvm.add %82, %104  : i64
          %131 = llvm.add %130, %126  : i64
          %132 = llvm.add %87, %119  : i64
          %133 = llvm.add %132, %128  : i64
          %134 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %135 = llvm.mlir.constant(1024 : index) : i64
          %136 = llvm.mul %131, %135  : i64
          %137 = llvm.add %136, %133  : i64
          %138 = llvm.getelementptr %134[%137] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %11, %138 : !llvm.ptr<f32>
          %139 = llvm.add %128, %8  : i64
          llvm.br ^bb2(%139 : i64)
        ^bb4:  // pred: ^bb2
          %140 = llvm.add %126, %8  : i64
          llvm.br ^bb1(%140 : i64)
        ^bb5(%141: i64):  // 2 preds: ^bb1, ^bb12
          %142 = llvm.icmp "slt" %141, %9 : i64
          llvm.cond_br %142, ^bb6(%7 : i64), ^bb13
        ^bb6(%143: i64):  // 2 preds: ^bb5, ^bb11
          %144 = llvm.icmp "slt" %143, %110 : i64
          llvm.cond_br %144, ^bb7(%7 : i64), ^bb12
        ^bb7(%145: i64):  // 2 preds: ^bb6, ^bb10
          %146 = llvm.icmp "slt" %145, %125 : i64
          llvm.cond_br %146, ^bb8(%7 : i64), ^bb11
        ^bb8(%147: i64):  // 2 preds: ^bb7, ^bb9
          %148 = llvm.icmp "slt" %147, %10 : i64
          llvm.cond_br %148, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %149 = llvm.add %141, %147  : i64
          %150 = llvm.add %82, %104  : i64
          %151 = llvm.add %150, %143  : i64
          %152 = llvm.extractvalue %27[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %153 = llvm.mlir.constant(256 : index) : i64
          %154 = llvm.mul %151, %153  : i64
          %155 = llvm.add %154, %149  : i64
          %156 = llvm.getelementptr %152[%155] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %157 = llvm.load %156 : !llvm.ptr<f32>
          %158 = llvm.add %87, %119  : i64
          %159 = llvm.add %158, %145  : i64
          %160 = llvm.extractvalue %49[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %161 = llvm.mlir.constant(1024 : index) : i64
          %162 = llvm.mul %149, %161  : i64
          %163 = llvm.add %162, %159  : i64
          %164 = llvm.getelementptr %160[%163] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %165 = llvm.load %164 : !llvm.ptr<f32>
          %166 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %167 = llvm.mlir.constant(1024 : index) : i64
          %168 = llvm.mul %151, %167  : i64
          %169 = llvm.add %168, %159  : i64
          %170 = llvm.getelementptr %166[%169] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %171 = llvm.load %170 : !llvm.ptr<f32>
          %172 = llvm.fmul %157, %165  : f32
          %173 = llvm.fadd %171, %172  : f32
          %174 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %175 = llvm.mlir.constant(1024 : index) : i64
          %176 = llvm.mul %151, %175  : i64
          %177 = llvm.add %176, %159  : i64
          %178 = llvm.getelementptr %174[%177] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %173, %178 : !llvm.ptr<f32>
          %179 = llvm.add %147, %8  : i64
          llvm.br ^bb8(%179 : i64)
        ^bb10:  // pred: ^bb8
          %180 = llvm.add %145, %8  : i64
          llvm.br ^bb7(%180 : i64)
        ^bb11:  // pred: ^bb7
          %181 = llvm.add %143, %8  : i64
          llvm.br ^bb6(%181 : i64)
        ^bb12:  // pred: ^bb6
          %182 = llvm.add %141, %10  : i64
          llvm.br ^bb5(%182 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    %device_1 = hal.ex.shared_device : !hal.device
    %allocator_2 = hal.device.allocator<%device_1 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator_2 : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %device_3 = hal.ex.shared_device : !hal.device
    %allocator_4 = hal.device.allocator<%device_3 : !hal.device> : !hal.allocator
    %buffer_5 = hal.allocator.allocate<%allocator_4 : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %device_6 = hal.ex.shared_device : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %cmd = hal.command_buffer.create device(%device_6 : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %0 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    hal.device.switch<%0 : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%0 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      %c0_12 = arith.constant 0 : index
      %c1_13 = arith.constant 1 : index
      %c2 = arith.constant 2 : index
      %c0_14 = arith.constant 0 : index
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0_14] bindings([
        %c0_12 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1_13 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
        %c2 = (%buffer_5 : !hal.buffer)[%c0, %c2097152]
      ])
      %c8 = arith.constant 8 : index
      %c16 = arith.constant 16 : index
      %c1_15 = arith.constant 1 : index
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@matmul_static_dispatch_0::@cuda_nvptx_fb::@matmul_static_dispatch_0_matmul_512x1024x256) workgroups([%c8, %c16, %c1_15])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %1 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_6 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_6 : !hal.device> affinity(%c-1_i64) wait(%1) signal(%fence) commands([%cmd])
    %c-1_i32 = arith.constant -1 : i32
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %c512_7 = arith.constant 512 : index
    %c1024_8 = arith.constant 1024 : index
    %c0_9 = arith.constant 0 : index
    %c553648160_i32_10 = arith.constant 553648160 : i32
    %c1_i32_11 = arith.constant 1 : i32
    %view = hal.buffer_view.create buffer(%buffer_5 : !hal.buffer)[%c0_9, %c2097152] shape([%c512_7, %c1024_8]) type(%c553648160_i32_10) encoding(%c1_i32_11) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::FixupLegacySyncPass (iree-hal-fixup-legacy-sync) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(-1 : index) : i64
          %1 = llvm.mlir.constant(8 : index) : i64
          %2 = llvm.mlir.constant(1024 : index) : i64
          %3 = llvm.mlir.constant(-128 : index) : i64
          %4 = llvm.mlir.constant(128 : index) : i64
          %5 = llvm.mlir.constant(512 : index) : i64
          %6 = llvm.mlir.constant(-32 : index) : i64
          %7 = llvm.mlir.constant(0 : index) : i64
          %8 = llvm.mlir.constant(1 : index) : i64
          %9 = llvm.mlir.constant(256 : index) : i64
          %10 = llvm.mlir.constant(32 : index) : i64
          %11 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %12 = llvm.bitcast %arg0 : !llvm.ptr<f32> to !llvm.ptr<i8>
          %13 = llvm.getelementptr %12[%7] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
          %14 = llvm.bitcast %13 : !llvm.ptr<i8> to !llvm.ptr<f32>
          %15 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>
          %16 = llvm.insertvalue %14, %15[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %17 = llvm.insertvalue %14, %16[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %18 = llvm.mlir.constant(0 : index) : i64
          %19 = llvm.insertvalue %18, %17[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %20 = llvm.mlir.constant(512 : index) : i64
          %21 = llvm.insertvalue %20, %19[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %22 = llvm.mlir.constant(256 : index) : i64
          %23 = llvm.insertvalue %22, %21[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %24 = llvm.mlir.constant(256 : index) : i64
          %25 = llvm.insertvalue %24, %23[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %26 = llvm.mlir.constant(1 : index) : i64
          %27 = llvm.insertvalue %26, %25[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %28 = llvm.extractvalue %27[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %29 = llvm.mlir.constant(0 : index) : i64
          %30 = llvm.mlir.constant(63 : index) : i64
          %31 = llvm.ptrtoint %28 : !llvm.ptr<f32> to i64
          %32 = llvm.and %31, %30  : i64
          %33 = llvm.icmp "eq" %32, %29 : i64
          "llvm.intr.assume"(%33) : (i1) -> ()
          %34 = llvm.bitcast %arg1 : !llvm.ptr<f32> to !llvm.ptr<i8>
          %35 = llvm.getelementptr %34[%7] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
          %36 = llvm.bitcast %35 : !llvm.ptr<i8> to !llvm.ptr<f32>
          %37 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>
          %38 = llvm.insertvalue %36, %37[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %39 = llvm.insertvalue %36, %38[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %40 = llvm.mlir.constant(0 : index) : i64
          %41 = llvm.insertvalue %40, %39[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %42 = llvm.mlir.constant(256 : index) : i64
          %43 = llvm.insertvalue %42, %41[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %44 = llvm.mlir.constant(1024 : index) : i64
          %45 = llvm.insertvalue %44, %43[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %46 = llvm.mlir.constant(1024 : index) : i64
          %47 = llvm.insertvalue %46, %45[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %48 = llvm.mlir.constant(1 : index) : i64
          %49 = llvm.insertvalue %48, %47[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %50 = llvm.extractvalue %49[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %51 = llvm.mlir.constant(0 : index) : i64
          %52 = llvm.mlir.constant(63 : index) : i64
          %53 = llvm.ptrtoint %50 : !llvm.ptr<f32> to i64
          %54 = llvm.and %53, %52  : i64
          %55 = llvm.icmp "eq" %54, %51 : i64
          "llvm.intr.assume"(%55) : (i1) -> ()
          %56 = llvm.bitcast %arg2 : !llvm.ptr<f32> to !llvm.ptr<i8>
          %57 = llvm.getelementptr %56[%7] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
          %58 = llvm.bitcast %57 : !llvm.ptr<i8> to !llvm.ptr<f32>
          %59 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>
          %60 = llvm.insertvalue %58, %59[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %61 = llvm.insertvalue %58, %60[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %62 = llvm.mlir.constant(0 : index) : i64
          %63 = llvm.insertvalue %62, %61[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %64 = llvm.mlir.constant(512 : index) : i64
          %65 = llvm.insertvalue %64, %63[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %66 = llvm.mlir.constant(1024 : index) : i64
          %67 = llvm.insertvalue %66, %65[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %68 = llvm.mlir.constant(1024 : index) : i64
          %69 = llvm.insertvalue %68, %67[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %70 = llvm.mlir.constant(1 : index) : i64
          %71 = llvm.insertvalue %70, %69[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %72 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %73 = llvm.mlir.constant(0 : index) : i64
          %74 = llvm.mlir.constant(63 : index) : i64
          %75 = llvm.ptrtoint %72 : !llvm.ptr<f32> to i64
          %76 = llvm.and %75, %74  : i64
          %77 = llvm.icmp "eq" %76, %73 : i64
          "llvm.intr.assume"(%77) : (i1) -> ()
          %78 = nvvm.read.ptx.sreg.ctaid.x : i32
          %79 = llvm.sext %78 : i32 to i64
          %80 = nvvm.read.ptx.sreg.ctaid.y : i32
          %81 = llvm.sext %80 : i32 to i64
          %82 = llvm.mul %81, %10  : i64
          %83 = llvm.mul %81, %6  : i64
          %84 = llvm.add %83, %5  : i64
          %85 = llvm.icmp "slt" %84, %10 : i64
          %86 = llvm.select %85, %84, %10 : i1, i64
          %87 = llvm.mul %79, %4  : i64
          %88 = llvm.mul %79, %3  : i64
          %89 = llvm.add %88, %2  : i64
          %90 = llvm.icmp "slt" %89, %4 : i64
          %91 = llvm.select %90, %89, %4 : i1, i64
          %92 = nvvm.read.ptx.sreg.tid.x : i32
          %93 = llvm.sext %92 : i32 to i64
          %94 = nvvm.read.ptx.sreg.tid.y : i32
          %95 = llvm.sext %94 : i32 to i64
          %96 = llvm.icmp "sle" %86, %7 : i64
          %97 = llvm.sub %7, %86  : i64
          %98 = llvm.sub %86, %8  : i64
          %99 = llvm.select %96, %97, %98 : i1, i64
          %100 = llvm.sdiv %99, %1  : i64
          %101 = llvm.sub %7, %100  : i64
          %102 = llvm.add %100, %8  : i64
          %103 = llvm.select %96, %101, %102 : i1, i64
          %104 = llvm.mul %95, %103  : i64
          %105 = llvm.mul %104, %0  : i64
          %106 = llvm.add %86, %105  : i64
          %107 = llvm.icmp "slt" %106, %103 : i64
          %108 = llvm.select %107, %106, %103 : i1, i64
          %109 = llvm.icmp "slt" %108, %7 : i64
          %110 = llvm.select %109, %7, %108 : i1, i64
          %111 = llvm.icmp "sle" %91, %7 : i64
          %112 = llvm.sub %7, %91  : i64
          %113 = llvm.sub %91, %8  : i64
          %114 = llvm.select %111, %112, %113 : i1, i64
          %115 = llvm.sdiv %114, %10  : i64
          %116 = llvm.sub %7, %115  : i64
          %117 = llvm.add %115, %8  : i64
          %118 = llvm.select %111, %116, %117 : i1, i64
          %119 = llvm.mul %93, %118  : i64
          %120 = llvm.mul %119, %0  : i64
          %121 = llvm.add %91, %120  : i64
          %122 = llvm.icmp "slt" %121, %118 : i64
          %123 = llvm.select %122, %121, %118 : i1, i64
          %124 = llvm.icmp "slt" %123, %7 : i64
          %125 = llvm.select %124, %7, %123 : i1, i64
          llvm.br ^bb1(%7 : i64)
        ^bb1(%126: i64):  // 2 preds: ^bb0, ^bb4
          %127 = llvm.icmp "slt" %126, %110 : i64
          llvm.cond_br %127, ^bb2(%7 : i64), ^bb5(%7 : i64)
        ^bb2(%128: i64):  // 2 preds: ^bb1, ^bb3
          %129 = llvm.icmp "slt" %128, %125 : i64
          llvm.cond_br %129, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %130 = llvm.add %82, %104  : i64
          %131 = llvm.add %130, %126  : i64
          %132 = llvm.add %87, %119  : i64
          %133 = llvm.add %132, %128  : i64
          %134 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %135 = llvm.mlir.constant(1024 : index) : i64
          %136 = llvm.mul %131, %135  : i64
          %137 = llvm.add %136, %133  : i64
          %138 = llvm.getelementptr %134[%137] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %11, %138 : !llvm.ptr<f32>
          %139 = llvm.add %128, %8  : i64
          llvm.br ^bb2(%139 : i64)
        ^bb4:  // pred: ^bb2
          %140 = llvm.add %126, %8  : i64
          llvm.br ^bb1(%140 : i64)
        ^bb5(%141: i64):  // 2 preds: ^bb1, ^bb12
          %142 = llvm.icmp "slt" %141, %9 : i64
          llvm.cond_br %142, ^bb6(%7 : i64), ^bb13
        ^bb6(%143: i64):  // 2 preds: ^bb5, ^bb11
          %144 = llvm.icmp "slt" %143, %110 : i64
          llvm.cond_br %144, ^bb7(%7 : i64), ^bb12
        ^bb7(%145: i64):  // 2 preds: ^bb6, ^bb10
          %146 = llvm.icmp "slt" %145, %125 : i64
          llvm.cond_br %146, ^bb8(%7 : i64), ^bb11
        ^bb8(%147: i64):  // 2 preds: ^bb7, ^bb9
          %148 = llvm.icmp "slt" %147, %10 : i64
          llvm.cond_br %148, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %149 = llvm.add %141, %147  : i64
          %150 = llvm.add %82, %104  : i64
          %151 = llvm.add %150, %143  : i64
          %152 = llvm.extractvalue %27[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %153 = llvm.mlir.constant(256 : index) : i64
          %154 = llvm.mul %151, %153  : i64
          %155 = llvm.add %154, %149  : i64
          %156 = llvm.getelementptr %152[%155] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %157 = llvm.load %156 : !llvm.ptr<f32>
          %158 = llvm.add %87, %119  : i64
          %159 = llvm.add %158, %145  : i64
          %160 = llvm.extractvalue %49[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %161 = llvm.mlir.constant(1024 : index) : i64
          %162 = llvm.mul %149, %161  : i64
          %163 = llvm.add %162, %159  : i64
          %164 = llvm.getelementptr %160[%163] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %165 = llvm.load %164 : !llvm.ptr<f32>
          %166 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %167 = llvm.mlir.constant(1024 : index) : i64
          %168 = llvm.mul %151, %167  : i64
          %169 = llvm.add %168, %159  : i64
          %170 = llvm.getelementptr %166[%169] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %171 = llvm.load %170 : !llvm.ptr<f32>
          %172 = llvm.fmul %157, %165  : f32
          %173 = llvm.fadd %171, %172  : f32
          %174 = llvm.extractvalue %71[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
          %175 = llvm.mlir.constant(1024 : index) : i64
          %176 = llvm.mul %151, %175  : i64
          %177 = llvm.add %176, %159  : i64
          %178 = llvm.getelementptr %174[%177] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %173, %178 : !llvm.ptr<f32>
          %179 = llvm.add %147, %8  : i64
          llvm.br ^bb8(%179 : i64)
        ^bb10:  // pred: ^bb8
          %180 = llvm.add %145, %8  : i64
          llvm.br ^bb7(%180 : i64)
        ^bb11:  // pred: ^bb7
          %181 = llvm.add %143, %8  : i64
          llvm.br ^bb6(%181 : i64)
        ^bb12:  // pred: ^bb6
          %182 = llvm.add %141, %10  : i64
          llvm.br ^bb5(%182 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    %device_1 = hal.ex.shared_device : !hal.device
    %allocator_2 = hal.device.allocator<%device_1 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator_2 : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %device_3 = hal.ex.shared_device : !hal.device
    %allocator_4 = hal.device.allocator<%device_3 : !hal.device> : !hal.allocator
    %buffer_5 = hal.allocator.allocate<%allocator_4 : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %device_6 = hal.ex.shared_device : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %cmd = hal.command_buffer.create device(%device_6 : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %0 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    hal.device.switch<%0 : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%0 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      %c0_14 = arith.constant 0 : index
      %c1_15 = arith.constant 1 : index
      %c2 = arith.constant 2 : index
      %c0_16 = arith.constant 0 : index
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0_16] bindings([
        %c0_14 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1_15 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
        %c2 = (%buffer_5 : !hal.buffer)[%c0, %c2097152]
      ])
      %c8 = arith.constant 8 : index
      %c16 = arith.constant 16 : index
      %c1_17 = arith.constant 1 : index
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@matmul_static_dispatch_0::@cuda_nvptx_fb::@matmul_static_dispatch_0_matmul_512x1024x256) workgroups([%c8, %c16, %c1_17])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %1 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_6 : !hal.device) flags("None") : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %status = hal.fence.await until([%1]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device_6 : !hal.device> affinity(%c-1_i64) wait(%1) signal(%fence) commands([%cmd])
    %c-1_i32_7 = arith.constant -1 : i32
    %status_8 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32_7) : i32
    util.status.check_ok %status_8, "failed to wait on timepoint"
    %c512_9 = arith.constant 512 : index
    %c1024_10 = arith.constant 1024 : index
    %c0_11 = arith.constant 0 : index
    %c553648160_i32_12 = arith.constant 553648160 : i32
    %c1_i32_13 = arith.constant 1 : i32
    %view = hal.buffer_view.create buffer(%buffer_5 : !hal.buffer)[%c0_11, %c2097152] shape([%c512_9, %c1024_10]) type(%c553648160_i32_12) encoding(%c1_i32_13) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(-1 : index) : i64
          %2 = llvm.mlir.constant(8 : index) : i64
          %3 = llvm.mlir.constant(1024 : index) : i64
          %4 = llvm.mlir.constant(-128 : index) : i64
          %5 = llvm.mlir.constant(128 : index) : i64
          %6 = llvm.mlir.constant(512 : index) : i64
          %7 = llvm.mlir.constant(-32 : index) : i64
          %8 = llvm.mlir.constant(0 : index) : i64
          %9 = llvm.mlir.constant(1 : index) : i64
          %10 = llvm.mlir.constant(256 : index) : i64
          %11 = llvm.mlir.constant(32 : index) : i64
          %12 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %13 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %8 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %17 = llvm.and %16, %0  : i64
          %18 = llvm.icmp "eq" %17, %8 : i64
          "llvm.intr.assume"(%18) : (i1) -> ()
          %19 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %20 = llvm.and %19, %0  : i64
          %21 = llvm.icmp "eq" %20, %8 : i64
          "llvm.intr.assume"(%21) : (i1) -> ()
          %22 = nvvm.read.ptx.sreg.ctaid.x : i32
          %23 = llvm.sext %22 : i32 to i64
          %24 = nvvm.read.ptx.sreg.ctaid.y : i32
          %25 = llvm.sext %24 : i32 to i64
          %26 = llvm.mul %25, %11  : i64
          %27 = llvm.mul %25, %7  : i64
          %28 = llvm.add %27, %6  : i64
          %29 = llvm.icmp "slt" %28, %11 : i64
          %30 = llvm.select %29, %28, %11 : i1, i64
          %31 = llvm.mul %23, %5  : i64
          %32 = llvm.mul %23, %4  : i64
          %33 = llvm.add %32, %3  : i64
          %34 = llvm.icmp "slt" %33, %5 : i64
          %35 = llvm.select %34, %33, %5 : i1, i64
          %36 = nvvm.read.ptx.sreg.tid.x : i32
          %37 = llvm.sext %36 : i32 to i64
          %38 = nvvm.read.ptx.sreg.tid.y : i32
          %39 = llvm.sext %38 : i32 to i64
          %40 = llvm.icmp "sle" %30, %8 : i64
          %41 = llvm.sub %8, %30  : i64
          %42 = llvm.sub %30, %9  : i64
          %43 = llvm.select %40, %41, %42 : i1, i64
          %44 = llvm.sdiv %43, %2  : i64
          %45 = llvm.sub %8, %44  : i64
          %46 = llvm.add %44, %9  : i64
          %47 = llvm.select %40, %45, %46 : i1, i64
          %48 = llvm.mul %39, %47  : i64
          %49 = llvm.mul %48, %1  : i64
          %50 = llvm.add %30, %49  : i64
          %51 = llvm.icmp "slt" %50, %47 : i64
          %52 = llvm.select %51, %50, %47 : i1, i64
          %53 = llvm.icmp "slt" %52, %8 : i64
          %54 = llvm.select %53, %8, %52 : i1, i64
          %55 = llvm.icmp "sle" %35, %8 : i64
          %56 = llvm.sub %8, %35  : i64
          %57 = llvm.sub %35, %9  : i64
          %58 = llvm.select %55, %56, %57 : i1, i64
          %59 = llvm.sdiv %58, %11  : i64
          %60 = llvm.sub %8, %59  : i64
          %61 = llvm.add %59, %9  : i64
          %62 = llvm.select %55, %60, %61 : i1, i64
          %63 = llvm.mul %37, %62  : i64
          %64 = llvm.mul %63, %1  : i64
          %65 = llvm.add %35, %64  : i64
          %66 = llvm.icmp "slt" %65, %62 : i64
          %67 = llvm.select %66, %65, %62 : i1, i64
          %68 = llvm.icmp "slt" %67, %8 : i64
          %69 = llvm.select %68, %8, %67 : i1, i64
          llvm.br ^bb1(%8 : i64)
        ^bb1(%70: i64):  // 2 preds: ^bb0, ^bb4
          %71 = llvm.icmp "slt" %70, %54 : i64
          llvm.cond_br %71, ^bb2(%8 : i64), ^bb5(%8 : i64)
        ^bb2(%72: i64):  // 2 preds: ^bb1, ^bb3
          %73 = llvm.icmp "slt" %72, %69 : i64
          llvm.cond_br %73, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %74 = llvm.add %26, %48  : i64
          %75 = llvm.add %74, %70  : i64
          %76 = llvm.add %31, %63  : i64
          %77 = llvm.add %76, %72  : i64
          %78 = llvm.mul %75, %3  : i64
          %79 = llvm.add %78, %77  : i64
          %80 = llvm.getelementptr %arg2[%79] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %12, %80 : !llvm.ptr<f32>
          %81 = llvm.add %72, %9  : i64
          llvm.br ^bb2(%81 : i64)
        ^bb4:  // pred: ^bb2
          %82 = llvm.add %70, %9  : i64
          llvm.br ^bb1(%82 : i64)
        ^bb5(%83: i64):  // 2 preds: ^bb1, ^bb12
          %84 = llvm.icmp "slt" %83, %10 : i64
          llvm.cond_br %84, ^bb6(%8 : i64), ^bb13
        ^bb6(%85: i64):  // 2 preds: ^bb5, ^bb11
          %86 = llvm.icmp "slt" %85, %54 : i64
          llvm.cond_br %86, ^bb7(%8 : i64), ^bb12
        ^bb7(%87: i64):  // 2 preds: ^bb6, ^bb10
          %88 = llvm.icmp "slt" %87, %69 : i64
          llvm.cond_br %88, ^bb8(%8 : i64), ^bb11
        ^bb8(%89: i64):  // 2 preds: ^bb7, ^bb9
          %90 = llvm.icmp "slt" %89, %11 : i64
          llvm.cond_br %90, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %91 = llvm.add %83, %89  : i64
          %92 = llvm.add %26, %48  : i64
          %93 = llvm.add %92, %85  : i64
          %94 = llvm.mul %93, %10  : i64
          %95 = llvm.add %94, %91  : i64
          %96 = llvm.getelementptr %arg0[%95] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %97 = llvm.load %96 : !llvm.ptr<f32>
          %98 = llvm.add %31, %63  : i64
          %99 = llvm.add %98, %87  : i64
          %100 = llvm.mul %91, %3  : i64
          %101 = llvm.add %100, %99  : i64
          %102 = llvm.getelementptr %arg1[%101] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %103 = llvm.load %102 : !llvm.ptr<f32>
          %104 = llvm.mul %93, %3  : i64
          %105 = llvm.add %104, %99  : i64
          %106 = llvm.getelementptr %arg2[%105] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %107 = llvm.load %106 : !llvm.ptr<f32>
          %108 = llvm.fmul %97, %103  : f32
          %109 = llvm.fadd %107, %108  : f32
          %110 = llvm.mul %93, %3  : i64
          %111 = llvm.add %110, %99  : i64
          %112 = llvm.getelementptr %arg2[%111] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %109, %112 : !llvm.ptr<f32>
          %113 = llvm.add %89, %9  : i64
          llvm.br ^bb8(%113 : i64)
        ^bb10:  // pred: ^bb8
          %114 = llvm.add %87, %9  : i64
          llvm.br ^bb7(%114 : i64)
        ^bb11:  // pred: ^bb7
          %115 = llvm.add %85, %9  : i64
          llvm.br ^bb6(%115 : i64)
        ^bb12:  // pred: ^bb6
          %116 = llvm.add %83, %11  : i64
          llvm.br ^bb5(%116 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    %device_1 = hal.ex.shared_device : !hal.device
    %allocator_2 = hal.device.allocator<%device_1 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator_2 : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %device_3 = hal.ex.shared_device : !hal.device
    %allocator_4 = hal.device.allocator<%device_3 : !hal.device> : !hal.allocator
    %buffer_5 = hal.allocator.allocate<%allocator_4 : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %device_6 = hal.ex.shared_device : !hal.device
    %cmd = hal.command_buffer.create device(%device_6 : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device_6 : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device_6 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
        %c2 = (%buffer_5 : !hal.buffer)[%c0, %c2097152]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@matmul_static_dispatch_0::@cuda_nvptx_fb::@matmul_static_dispatch_0_matmul_512x1024x256) workgroups([%c8, %c16, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_6 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_6 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_5 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(-1 : index) : i64
          %2 = llvm.mlir.constant(8 : index) : i64
          %3 = llvm.mlir.constant(1024 : index) : i64
          %4 = llvm.mlir.constant(-128 : index) : i64
          %5 = llvm.mlir.constant(128 : index) : i64
          %6 = llvm.mlir.constant(512 : index) : i64
          %7 = llvm.mlir.constant(-32 : index) : i64
          %8 = llvm.mlir.constant(0 : index) : i64
          %9 = llvm.mlir.constant(1 : index) : i64
          %10 = llvm.mlir.constant(256 : index) : i64
          %11 = llvm.mlir.constant(32 : index) : i64
          %12 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %13 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %8 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %17 = llvm.and %16, %0  : i64
          %18 = llvm.icmp "eq" %17, %8 : i64
          "llvm.intr.assume"(%18) : (i1) -> ()
          %19 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %20 = llvm.and %19, %0  : i64
          %21 = llvm.icmp "eq" %20, %8 : i64
          "llvm.intr.assume"(%21) : (i1) -> ()
          %22 = nvvm.read.ptx.sreg.ctaid.x : i32
          %23 = llvm.sext %22 : i32 to i64
          %24 = nvvm.read.ptx.sreg.ctaid.y : i32
          %25 = llvm.sext %24 : i32 to i64
          %26 = llvm.mul %25, %11  : i64
          %27 = llvm.mul %25, %7  : i64
          %28 = llvm.add %27, %6  : i64
          %29 = llvm.icmp "slt" %28, %11 : i64
          %30 = llvm.select %29, %28, %11 : i1, i64
          %31 = llvm.mul %23, %5  : i64
          %32 = llvm.mul %23, %4  : i64
          %33 = llvm.add %32, %3  : i64
          %34 = llvm.icmp "slt" %33, %5 : i64
          %35 = llvm.select %34, %33, %5 : i1, i64
          %36 = nvvm.read.ptx.sreg.tid.x : i32
          %37 = llvm.sext %36 : i32 to i64
          %38 = nvvm.read.ptx.sreg.tid.y : i32
          %39 = llvm.sext %38 : i32 to i64
          %40 = llvm.icmp "sle" %30, %8 : i64
          %41 = llvm.sub %8, %30  : i64
          %42 = llvm.sub %30, %9  : i64
          %43 = llvm.select %40, %41, %42 : i1, i64
          %44 = llvm.sdiv %43, %2  : i64
          %45 = llvm.sub %8, %44  : i64
          %46 = llvm.add %44, %9  : i64
          %47 = llvm.select %40, %45, %46 : i1, i64
          %48 = llvm.mul %39, %47  : i64
          %49 = llvm.mul %48, %1  : i64
          %50 = llvm.add %30, %49  : i64
          %51 = llvm.icmp "slt" %50, %47 : i64
          %52 = llvm.select %51, %50, %47 : i1, i64
          %53 = llvm.icmp "slt" %52, %8 : i64
          %54 = llvm.select %53, %8, %52 : i1, i64
          %55 = llvm.icmp "sle" %35, %8 : i64
          %56 = llvm.sub %8, %35  : i64
          %57 = llvm.sub %35, %9  : i64
          %58 = llvm.select %55, %56, %57 : i1, i64
          %59 = llvm.sdiv %58, %11  : i64
          %60 = llvm.sub %8, %59  : i64
          %61 = llvm.add %59, %9  : i64
          %62 = llvm.select %55, %60, %61 : i1, i64
          %63 = llvm.mul %37, %62  : i64
          %64 = llvm.mul %63, %1  : i64
          %65 = llvm.add %35, %64  : i64
          %66 = llvm.icmp "slt" %65, %62 : i64
          %67 = llvm.select %66, %65, %62 : i1, i64
          %68 = llvm.icmp "slt" %67, %8 : i64
          %69 = llvm.select %68, %8, %67 : i1, i64
          llvm.br ^bb1(%8 : i64)
        ^bb1(%70: i64):  // 2 preds: ^bb0, ^bb4
          %71 = llvm.icmp "slt" %70, %54 : i64
          llvm.cond_br %71, ^bb2(%8 : i64), ^bb5(%8 : i64)
        ^bb2(%72: i64):  // 2 preds: ^bb1, ^bb3
          %73 = llvm.icmp "slt" %72, %69 : i64
          llvm.cond_br %73, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %74 = llvm.add %26, %48  : i64
          %75 = llvm.add %74, %70  : i64
          %76 = llvm.add %31, %63  : i64
          %77 = llvm.add %76, %72  : i64
          %78 = llvm.mul %75, %3  : i64
          %79 = llvm.add %78, %77  : i64
          %80 = llvm.getelementptr %arg2[%79] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %12, %80 : !llvm.ptr<f32>
          %81 = llvm.add %72, %9  : i64
          llvm.br ^bb2(%81 : i64)
        ^bb4:  // pred: ^bb2
          %82 = llvm.add %70, %9  : i64
          llvm.br ^bb1(%82 : i64)
        ^bb5(%83: i64):  // 2 preds: ^bb1, ^bb12
          %84 = llvm.icmp "slt" %83, %10 : i64
          llvm.cond_br %84, ^bb6(%8 : i64), ^bb13
        ^bb6(%85: i64):  // 2 preds: ^bb5, ^bb11
          %86 = llvm.icmp "slt" %85, %54 : i64
          llvm.cond_br %86, ^bb7(%8 : i64), ^bb12
        ^bb7(%87: i64):  // 2 preds: ^bb6, ^bb10
          %88 = llvm.icmp "slt" %87, %69 : i64
          llvm.cond_br %88, ^bb8(%8 : i64), ^bb11
        ^bb8(%89: i64):  // 2 preds: ^bb7, ^bb9
          %90 = llvm.icmp "slt" %89, %11 : i64
          llvm.cond_br %90, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %91 = llvm.add %83, %89  : i64
          %92 = llvm.add %26, %48  : i64
          %93 = llvm.add %92, %85  : i64
          %94 = llvm.mul %93, %10  : i64
          %95 = llvm.add %94, %91  : i64
          %96 = llvm.getelementptr %arg0[%95] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %97 = llvm.load %96 : !llvm.ptr<f32>
          %98 = llvm.add %31, %63  : i64
          %99 = llvm.add %98, %87  : i64
          %100 = llvm.mul %91, %3  : i64
          %101 = llvm.add %100, %99  : i64
          %102 = llvm.getelementptr %arg1[%101] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %103 = llvm.load %102 : !llvm.ptr<f32>
          %104 = llvm.mul %93, %3  : i64
          %105 = llvm.add %104, %99  : i64
          %106 = llvm.getelementptr %arg2[%105] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %107 = llvm.load %106 : !llvm.ptr<f32>
          %108 = llvm.fmul %97, %103  : f32
          %109 = llvm.fadd %107, %108  : f32
          llvm.store %109, %106 : !llvm.ptr<f32>
          %110 = llvm.add %89, %9  : i64
          llvm.br ^bb8(%110 : i64)
        ^bb10:  // pred: ^bb8
          %111 = llvm.add %87, %9  : i64
          llvm.br ^bb7(%111 : i64)
        ^bb11:  // pred: ^bb7
          %112 = llvm.add %85, %9  : i64
          llvm.br ^bb6(%112 : i64)
        ^bb12:  // pred: ^bb6
          %113 = llvm.add %83, %11  : i64
          llvm.br ^bb5(%113 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
        %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@matmul_static_dispatch_0::@cuda_nvptx_fb::@matmul_static_dispatch_0_matmul_512x1024x256) workgroups([%c8, %c16, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c2 = arith.constant 2 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
    %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@matmul_static_dispatch_0::@cuda_nvptx_fb::@matmul_static_dispatch_0_matmul_512x1024x256) workgroups([%c8, %c16, %c1])
    hal.return
  }
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(-1 : index) : i64
          %2 = llvm.mlir.constant(8 : index) : i64
          %3 = llvm.mlir.constant(1024 : index) : i64
          %4 = llvm.mlir.constant(-128 : index) : i64
          %5 = llvm.mlir.constant(128 : index) : i64
          %6 = llvm.mlir.constant(512 : index) : i64
          %7 = llvm.mlir.constant(-32 : index) : i64
          %8 = llvm.mlir.constant(0 : index) : i64
          %9 = llvm.mlir.constant(1 : index) : i64
          %10 = llvm.mlir.constant(256 : index) : i64
          %11 = llvm.mlir.constant(32 : index) : i64
          %12 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %13 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %8 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %17 = llvm.and %16, %0  : i64
          %18 = llvm.icmp "eq" %17, %8 : i64
          "llvm.intr.assume"(%18) : (i1) -> ()
          %19 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %20 = llvm.and %19, %0  : i64
          %21 = llvm.icmp "eq" %20, %8 : i64
          "llvm.intr.assume"(%21) : (i1) -> ()
          %22 = nvvm.read.ptx.sreg.ctaid.x : i32
          %23 = llvm.sext %22 : i32 to i64
          %24 = nvvm.read.ptx.sreg.ctaid.y : i32
          %25 = llvm.sext %24 : i32 to i64
          %26 = llvm.mul %25, %11  : i64
          %27 = llvm.mul %25, %7  : i64
          %28 = llvm.add %27, %6  : i64
          %29 = llvm.icmp "slt" %28, %11 : i64
          %30 = llvm.select %29, %28, %11 : i1, i64
          %31 = llvm.mul %23, %5  : i64
          %32 = llvm.mul %23, %4  : i64
          %33 = llvm.add %32, %3  : i64
          %34 = llvm.icmp "slt" %33, %5 : i64
          %35 = llvm.select %34, %33, %5 : i1, i64
          %36 = nvvm.read.ptx.sreg.tid.x : i32
          %37 = llvm.sext %36 : i32 to i64
          %38 = nvvm.read.ptx.sreg.tid.y : i32
          %39 = llvm.sext %38 : i32 to i64
          %40 = llvm.icmp "sle" %30, %8 : i64
          %41 = llvm.sub %8, %30  : i64
          %42 = llvm.sub %30, %9  : i64
          %43 = llvm.select %40, %41, %42 : i1, i64
          %44 = llvm.sdiv %43, %2  : i64
          %45 = llvm.sub %8, %44  : i64
          %46 = llvm.add %44, %9  : i64
          %47 = llvm.select %40, %45, %46 : i1, i64
          %48 = llvm.mul %39, %47  : i64
          %49 = llvm.mul %48, %1  : i64
          %50 = llvm.add %30, %49  : i64
          %51 = llvm.icmp "slt" %50, %47 : i64
          %52 = llvm.select %51, %50, %47 : i1, i64
          %53 = llvm.icmp "slt" %52, %8 : i64
          %54 = llvm.select %53, %8, %52 : i1, i64
          %55 = llvm.icmp "sle" %35, %8 : i64
          %56 = llvm.sub %8, %35  : i64
          %57 = llvm.sub %35, %9  : i64
          %58 = llvm.select %55, %56, %57 : i1, i64
          %59 = llvm.sdiv %58, %11  : i64
          %60 = llvm.sub %8, %59  : i64
          %61 = llvm.add %59, %9  : i64
          %62 = llvm.select %55, %60, %61 : i1, i64
          %63 = llvm.mul %37, %62  : i64
          %64 = llvm.mul %63, %1  : i64
          %65 = llvm.add %35, %64  : i64
          %66 = llvm.icmp "slt" %65, %62 : i64
          %67 = llvm.select %66, %65, %62 : i1, i64
          %68 = llvm.icmp "slt" %67, %8 : i64
          %69 = llvm.select %68, %8, %67 : i1, i64
          llvm.br ^bb1(%8 : i64)
        ^bb1(%70: i64):  // 2 preds: ^bb0, ^bb4
          %71 = llvm.icmp "slt" %70, %54 : i64
          llvm.cond_br %71, ^bb2(%8 : i64), ^bb5(%8 : i64)
        ^bb2(%72: i64):  // 2 preds: ^bb1, ^bb3
          %73 = llvm.icmp "slt" %72, %69 : i64
          llvm.cond_br %73, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %74 = llvm.add %26, %48  : i64
          %75 = llvm.add %74, %70  : i64
          %76 = llvm.add %31, %63  : i64
          %77 = llvm.add %76, %72  : i64
          %78 = llvm.mul %75, %3  : i64
          %79 = llvm.add %78, %77  : i64
          %80 = llvm.getelementptr %arg2[%79] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %12, %80 : !llvm.ptr<f32>
          %81 = llvm.add %72, %9  : i64
          llvm.br ^bb2(%81 : i64)
        ^bb4:  // pred: ^bb2
          %82 = llvm.add %70, %9  : i64
          llvm.br ^bb1(%82 : i64)
        ^bb5(%83: i64):  // 2 preds: ^bb1, ^bb12
          %84 = llvm.icmp "slt" %83, %10 : i64
          llvm.cond_br %84, ^bb6(%8 : i64), ^bb13
        ^bb6(%85: i64):  // 2 preds: ^bb5, ^bb11
          %86 = llvm.icmp "slt" %85, %54 : i64
          llvm.cond_br %86, ^bb7(%8 : i64), ^bb12
        ^bb7(%87: i64):  // 2 preds: ^bb6, ^bb10
          %88 = llvm.icmp "slt" %87, %69 : i64
          llvm.cond_br %88, ^bb8(%8 : i64), ^bb11
        ^bb8(%89: i64):  // 2 preds: ^bb7, ^bb9
          %90 = llvm.icmp "slt" %89, %11 : i64
          llvm.cond_br %90, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %91 = llvm.add %83, %89  : i64
          %92 = llvm.add %26, %48  : i64
          %93 = llvm.add %92, %85  : i64
          %94 = llvm.mul %93, %10  : i64
          %95 = llvm.add %94, %91  : i64
          %96 = llvm.getelementptr %arg0[%95] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %97 = llvm.load %96 : !llvm.ptr<f32>
          %98 = llvm.add %31, %63  : i64
          %99 = llvm.add %98, %87  : i64
          %100 = llvm.mul %91, %3  : i64
          %101 = llvm.add %100, %99  : i64
          %102 = llvm.getelementptr %arg1[%101] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %103 = llvm.load %102 : !llvm.ptr<f32>
          %104 = llvm.mul %93, %3  : i64
          %105 = llvm.add %104, %99  : i64
          %106 = llvm.getelementptr %arg2[%105] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %107 = llvm.load %106 : !llvm.ptr<f32>
          %108 = llvm.fmul %97, %103  : f32
          %109 = llvm.fadd %107, %108  : f32
          llvm.store %109, %106 : !llvm.ptr<f32>
          %110 = llvm.add %89, %9  : i64
          llvm.br ^bb8(%110 : i64)
        ^bb10:  // pred: ^bb8
          %111 = llvm.add %87, %9  : i64
          llvm.br ^bb7(%111 : i64)
        ^bb11:  // pred: ^bb7
          %112 = llvm.add %85, %9  : i64
          llvm.br ^bb6(%112 : i64)
        ^bb12:  // pred: ^bb6
          %113 = llvm.add %83, %11  : i64
          llvm.br ^bb5(%113 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
        %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@matmul_static_dispatch_0::@cuda_nvptx_fb::@matmul_static_dispatch_0_matmul_512x1024x256) workgroups([%c8, %c16, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(-1 : index) : i64
          %2 = llvm.mlir.constant(8 : index) : i64
          %3 = llvm.mlir.constant(1024 : index) : i64
          %4 = llvm.mlir.constant(-128 : index) : i64
          %5 = llvm.mlir.constant(128 : index) : i64
          %6 = llvm.mlir.constant(512 : index) : i64
          %7 = llvm.mlir.constant(-32 : index) : i64
          %8 = llvm.mlir.constant(0 : index) : i64
          %9 = llvm.mlir.constant(1 : index) : i64
          %10 = llvm.mlir.constant(256 : index) : i64
          %11 = llvm.mlir.constant(32 : index) : i64
          %12 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %13 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %8 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %17 = llvm.and %16, %0  : i64
          %18 = llvm.icmp "eq" %17, %8 : i64
          "llvm.intr.assume"(%18) : (i1) -> ()
          %19 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %20 = llvm.and %19, %0  : i64
          %21 = llvm.icmp "eq" %20, %8 : i64
          "llvm.intr.assume"(%21) : (i1) -> ()
          %22 = nvvm.read.ptx.sreg.ctaid.x : i32
          %23 = llvm.sext %22 : i32 to i64
          %24 = nvvm.read.ptx.sreg.ctaid.y : i32
          %25 = llvm.sext %24 : i32 to i64
          %26 = llvm.mul %25, %11  : i64
          %27 = llvm.mul %25, %7  : i64
          %28 = llvm.add %27, %6  : i64
          %29 = llvm.icmp "slt" %28, %11 : i64
          %30 = llvm.select %29, %28, %11 : i1, i64
          %31 = llvm.mul %23, %5  : i64
          %32 = llvm.mul %23, %4  : i64
          %33 = llvm.add %32, %3  : i64
          %34 = llvm.icmp "slt" %33, %5 : i64
          %35 = llvm.select %34, %33, %5 : i1, i64
          %36 = nvvm.read.ptx.sreg.tid.x : i32
          %37 = llvm.sext %36 : i32 to i64
          %38 = nvvm.read.ptx.sreg.tid.y : i32
          %39 = llvm.sext %38 : i32 to i64
          %40 = llvm.icmp "sle" %30, %8 : i64
          %41 = llvm.sub %8, %30  : i64
          %42 = llvm.sub %30, %9  : i64
          %43 = llvm.select %40, %41, %42 : i1, i64
          %44 = llvm.sdiv %43, %2  : i64
          %45 = llvm.sub %8, %44  : i64
          %46 = llvm.add %44, %9  : i64
          %47 = llvm.select %40, %45, %46 : i1, i64
          %48 = llvm.mul %39, %47  : i64
          %49 = llvm.mul %48, %1  : i64
          %50 = llvm.add %30, %49  : i64
          %51 = llvm.icmp "slt" %50, %47 : i64
          %52 = llvm.select %51, %50, %47 : i1, i64
          %53 = llvm.icmp "slt" %52, %8 : i64
          %54 = llvm.select %53, %8, %52 : i1, i64
          %55 = llvm.icmp "sle" %35, %8 : i64
          %56 = llvm.sub %8, %35  : i64
          %57 = llvm.sub %35, %9  : i64
          %58 = llvm.select %55, %56, %57 : i1, i64
          %59 = llvm.sdiv %58, %11  : i64
          %60 = llvm.sub %8, %59  : i64
          %61 = llvm.add %59, %9  : i64
          %62 = llvm.select %55, %60, %61 : i1, i64
          %63 = llvm.mul %37, %62  : i64
          %64 = llvm.mul %63, %1  : i64
          %65 = llvm.add %35, %64  : i64
          %66 = llvm.icmp "slt" %65, %62 : i64
          %67 = llvm.select %66, %65, %62 : i1, i64
          %68 = llvm.icmp "slt" %67, %8 : i64
          %69 = llvm.select %68, %8, %67 : i1, i64
          llvm.br ^bb1(%8 : i64)
        ^bb1(%70: i64):  // 2 preds: ^bb0, ^bb4
          %71 = llvm.icmp "slt" %70, %54 : i64
          llvm.cond_br %71, ^bb2(%8 : i64), ^bb5(%8 : i64)
        ^bb2(%72: i64):  // 2 preds: ^bb1, ^bb3
          %73 = llvm.icmp "slt" %72, %69 : i64
          llvm.cond_br %73, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %74 = llvm.add %26, %48  : i64
          %75 = llvm.add %74, %70  : i64
          %76 = llvm.add %31, %63  : i64
          %77 = llvm.add %76, %72  : i64
          %78 = llvm.mul %75, %3  : i64
          %79 = llvm.add %78, %77  : i64
          %80 = llvm.getelementptr %arg2[%79] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %12, %80 : !llvm.ptr<f32>
          %81 = llvm.add %72, %9  : i64
          llvm.br ^bb2(%81 : i64)
        ^bb4:  // pred: ^bb2
          %82 = llvm.add %70, %9  : i64
          llvm.br ^bb1(%82 : i64)
        ^bb5(%83: i64):  // 2 preds: ^bb1, ^bb12
          %84 = llvm.icmp "slt" %83, %10 : i64
          llvm.cond_br %84, ^bb6(%8 : i64), ^bb13
        ^bb6(%85: i64):  // 2 preds: ^bb5, ^bb11
          %86 = llvm.icmp "slt" %85, %54 : i64
          llvm.cond_br %86, ^bb7(%8 : i64), ^bb12
        ^bb7(%87: i64):  // 2 preds: ^bb6, ^bb10
          %88 = llvm.icmp "slt" %87, %69 : i64
          llvm.cond_br %88, ^bb8(%8 : i64), ^bb11
        ^bb8(%89: i64):  // 2 preds: ^bb7, ^bb9
          %90 = llvm.icmp "slt" %89, %11 : i64
          llvm.cond_br %90, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %91 = llvm.add %83, %89  : i64
          %92 = llvm.add %26, %48  : i64
          %93 = llvm.add %92, %85  : i64
          %94 = llvm.mul %93, %10  : i64
          %95 = llvm.add %94, %91  : i64
          %96 = llvm.getelementptr %arg0[%95] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %97 = llvm.load %96 : !llvm.ptr<f32>
          %98 = llvm.add %31, %63  : i64
          %99 = llvm.add %98, %87  : i64
          %100 = llvm.mul %91, %3  : i64
          %101 = llvm.add %100, %99  : i64
          %102 = llvm.getelementptr %arg1[%101] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %103 = llvm.load %102 : !llvm.ptr<f32>
          %104 = llvm.mul %93, %3  : i64
          %105 = llvm.add %104, %99  : i64
          %106 = llvm.getelementptr %arg2[%105] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %107 = llvm.load %106 : !llvm.ptr<f32>
          %108 = llvm.fmul %97, %103  : f32
          %109 = llvm.fadd %107, %108  : f32
          llvm.store %109, %106 : !llvm.ptr<f32>
          %110 = llvm.add %89, %9  : i64
          llvm.br ^bb8(%110 : i64)
        ^bb10:  // pred: ^bb8
          %111 = llvm.add %87, %9  : i64
          llvm.br ^bb7(%111 : i64)
        ^bb11:  // pred: ^bb7
          %112 = llvm.add %85, %9  : i64
          llvm.br ^bb6(%112 : i64)
        ^bb12:  // pred: ^bb6
          %113 = llvm.add %83, %11  : i64
          llvm.br ^bb5(%113 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
        %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@matmul_static_dispatch_0::@cuda_nvptx_fb::@matmul_static_dispatch_0_matmul_512x1024x256) workgroups([%c8, %c16, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(-1 : index) : i64
          %2 = llvm.mlir.constant(8 : index) : i64
          %3 = llvm.mlir.constant(1024 : index) : i64
          %4 = llvm.mlir.constant(-128 : index) : i64
          %5 = llvm.mlir.constant(128 : index) : i64
          %6 = llvm.mlir.constant(512 : index) : i64
          %7 = llvm.mlir.constant(-32 : index) : i64
          %8 = llvm.mlir.constant(0 : index) : i64
          %9 = llvm.mlir.constant(1 : index) : i64
          %10 = llvm.mlir.constant(256 : index) : i64
          %11 = llvm.mlir.constant(32 : index) : i64
          %12 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %13 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %8 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %17 = llvm.and %16, %0  : i64
          %18 = llvm.icmp "eq" %17, %8 : i64
          "llvm.intr.assume"(%18) : (i1) -> ()
          %19 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %20 = llvm.and %19, %0  : i64
          %21 = llvm.icmp "eq" %20, %8 : i64
          "llvm.intr.assume"(%21) : (i1) -> ()
          %22 = nvvm.read.ptx.sreg.ctaid.x : i32
          %23 = llvm.sext %22 : i32 to i64
          %24 = nvvm.read.ptx.sreg.ctaid.y : i32
          %25 = llvm.sext %24 : i32 to i64
          %26 = llvm.mul %25, %11  : i64
          %27 = llvm.mul %25, %7  : i64
          %28 = llvm.add %27, %6  : i64
          %29 = llvm.icmp "slt" %28, %11 : i64
          %30 = llvm.select %29, %28, %11 : i1, i64
          %31 = llvm.mul %23, %5  : i64
          %32 = llvm.mul %23, %4  : i64
          %33 = llvm.add %32, %3  : i64
          %34 = llvm.icmp "slt" %33, %5 : i64
          %35 = llvm.select %34, %33, %5 : i1, i64
          %36 = nvvm.read.ptx.sreg.tid.x : i32
          %37 = llvm.sext %36 : i32 to i64
          %38 = nvvm.read.ptx.sreg.tid.y : i32
          %39 = llvm.sext %38 : i32 to i64
          %40 = llvm.icmp "sle" %30, %8 : i64
          %41 = llvm.sub %8, %30  : i64
          %42 = llvm.sub %30, %9  : i64
          %43 = llvm.select %40, %41, %42 : i1, i64
          %44 = llvm.sdiv %43, %2  : i64
          %45 = llvm.sub %8, %44  : i64
          %46 = llvm.add %44, %9  : i64
          %47 = llvm.select %40, %45, %46 : i1, i64
          %48 = llvm.mul %39, %47  : i64
          %49 = llvm.mul %48, %1  : i64
          %50 = llvm.add %30, %49  : i64
          %51 = llvm.icmp "slt" %50, %47 : i64
          %52 = llvm.select %51, %50, %47 : i1, i64
          %53 = llvm.icmp "slt" %52, %8 : i64
          %54 = llvm.select %53, %8, %52 : i1, i64
          %55 = llvm.icmp "sle" %35, %8 : i64
          %56 = llvm.sub %8, %35  : i64
          %57 = llvm.sub %35, %9  : i64
          %58 = llvm.select %55, %56, %57 : i1, i64
          %59 = llvm.sdiv %58, %11  : i64
          %60 = llvm.sub %8, %59  : i64
          %61 = llvm.add %59, %9  : i64
          %62 = llvm.select %55, %60, %61 : i1, i64
          %63 = llvm.mul %37, %62  : i64
          %64 = llvm.mul %63, %1  : i64
          %65 = llvm.add %35, %64  : i64
          %66 = llvm.icmp "slt" %65, %62 : i64
          %67 = llvm.select %66, %65, %62 : i1, i64
          %68 = llvm.icmp "slt" %67, %8 : i64
          %69 = llvm.select %68, %8, %67 : i1, i64
          llvm.br ^bb1(%8 : i64)
        ^bb1(%70: i64):  // 2 preds: ^bb0, ^bb4
          %71 = llvm.icmp "slt" %70, %54 : i64
          llvm.cond_br %71, ^bb2(%8 : i64), ^bb5(%8 : i64)
        ^bb2(%72: i64):  // 2 preds: ^bb1, ^bb3
          %73 = llvm.icmp "slt" %72, %69 : i64
          llvm.cond_br %73, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %74 = llvm.add %26, %48  : i64
          %75 = llvm.add %74, %70  : i64
          %76 = llvm.add %31, %63  : i64
          %77 = llvm.add %76, %72  : i64
          %78 = llvm.mul %75, %3  : i64
          %79 = llvm.add %78, %77  : i64
          %80 = llvm.getelementptr %arg2[%79] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %12, %80 : !llvm.ptr<f32>
          %81 = llvm.add %72, %9  : i64
          llvm.br ^bb2(%81 : i64)
        ^bb4:  // pred: ^bb2
          %82 = llvm.add %70, %9  : i64
          llvm.br ^bb1(%82 : i64)
        ^bb5(%83: i64):  // 2 preds: ^bb1, ^bb12
          %84 = llvm.icmp "slt" %83, %10 : i64
          llvm.cond_br %84, ^bb6(%8 : i64), ^bb13
        ^bb6(%85: i64):  // 2 preds: ^bb5, ^bb11
          %86 = llvm.icmp "slt" %85, %54 : i64
          llvm.cond_br %86, ^bb7(%8 : i64), ^bb12
        ^bb7(%87: i64):  // 2 preds: ^bb6, ^bb10
          %88 = llvm.icmp "slt" %87, %69 : i64
          llvm.cond_br %88, ^bb8(%8 : i64), ^bb11
        ^bb8(%89: i64):  // 2 preds: ^bb7, ^bb9
          %90 = llvm.icmp "slt" %89, %11 : i64
          llvm.cond_br %90, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %91 = llvm.add %83, %89  : i64
          %92 = llvm.add %26, %48  : i64
          %93 = llvm.add %92, %85  : i64
          %94 = llvm.mul %93, %10  : i64
          %95 = llvm.add %94, %91  : i64
          %96 = llvm.getelementptr %arg0[%95] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %97 = llvm.load %96 : !llvm.ptr<f32>
          %98 = llvm.add %31, %63  : i64
          %99 = llvm.add %98, %87  : i64
          %100 = llvm.mul %91, %3  : i64
          %101 = llvm.add %100, %99  : i64
          %102 = llvm.getelementptr %arg1[%101] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %103 = llvm.load %102 : !llvm.ptr<f32>
          %104 = llvm.mul %93, %3  : i64
          %105 = llvm.add %104, %99  : i64
          %106 = llvm.getelementptr %arg2[%105] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %107 = llvm.load %106 : !llvm.ptr<f32>
          %108 = llvm.fmul %97, %103  : f32
          %109 = llvm.fadd %107, %108  : f32
          llvm.store %109, %106 : !llvm.ptr<f32>
          %110 = llvm.add %89, %9  : i64
          llvm.br ^bb8(%110 : i64)
        ^bb10:  // pred: ^bb8
          %111 = llvm.add %87, %9  : i64
          llvm.br ^bb7(%111 : i64)
        ^bb11:  // pred: ^bb7
          %112 = llvm.add %85, %9  : i64
          llvm.br ^bb6(%112 : i64)
        ^bb12:  // pred: ^bb6
          %113 = llvm.add %83, %11  : i64
          llvm.br ^bb5(%113 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
        %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@matmul_static_dispatch_0::@cuda_nvptx_fb::@matmul_static_dispatch_0_matmul_512x1024x256) workgroups([%c8, %c16, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::LinkTargetExecutablesPass (iree-hal-link-target-executables) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(-1 : index) : i64
          %2 = llvm.mlir.constant(8 : index) : i64
          %3 = llvm.mlir.constant(1024 : index) : i64
          %4 = llvm.mlir.constant(-128 : index) : i64
          %5 = llvm.mlir.constant(128 : index) : i64
          %6 = llvm.mlir.constant(512 : index) : i64
          %7 = llvm.mlir.constant(-32 : index) : i64
          %8 = llvm.mlir.constant(0 : index) : i64
          %9 = llvm.mlir.constant(1 : index) : i64
          %10 = llvm.mlir.constant(256 : index) : i64
          %11 = llvm.mlir.constant(32 : index) : i64
          %12 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %13 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %8 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %17 = llvm.and %16, %0  : i64
          %18 = llvm.icmp "eq" %17, %8 : i64
          "llvm.intr.assume"(%18) : (i1) -> ()
          %19 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %20 = llvm.and %19, %0  : i64
          %21 = llvm.icmp "eq" %20, %8 : i64
          "llvm.intr.assume"(%21) : (i1) -> ()
          %22 = nvvm.read.ptx.sreg.ctaid.x : i32
          %23 = llvm.sext %22 : i32 to i64
          %24 = nvvm.read.ptx.sreg.ctaid.y : i32
          %25 = llvm.sext %24 : i32 to i64
          %26 = llvm.mul %25, %11  : i64
          %27 = llvm.mul %25, %7  : i64
          %28 = llvm.add %27, %6  : i64
          %29 = llvm.icmp "slt" %28, %11 : i64
          %30 = llvm.select %29, %28, %11 : i1, i64
          %31 = llvm.mul %23, %5  : i64
          %32 = llvm.mul %23, %4  : i64
          %33 = llvm.add %32, %3  : i64
          %34 = llvm.icmp "slt" %33, %5 : i64
          %35 = llvm.select %34, %33, %5 : i1, i64
          %36 = nvvm.read.ptx.sreg.tid.x : i32
          %37 = llvm.sext %36 : i32 to i64
          %38 = nvvm.read.ptx.sreg.tid.y : i32
          %39 = llvm.sext %38 : i32 to i64
          %40 = llvm.icmp "sle" %30, %8 : i64
          %41 = llvm.sub %8, %30  : i64
          %42 = llvm.sub %30, %9  : i64
          %43 = llvm.select %40, %41, %42 : i1, i64
          %44 = llvm.sdiv %43, %2  : i64
          %45 = llvm.sub %8, %44  : i64
          %46 = llvm.add %44, %9  : i64
          %47 = llvm.select %40, %45, %46 : i1, i64
          %48 = llvm.mul %39, %47  : i64
          %49 = llvm.mul %48, %1  : i64
          %50 = llvm.add %30, %49  : i64
          %51 = llvm.icmp "slt" %50, %47 : i64
          %52 = llvm.select %51, %50, %47 : i1, i64
          %53 = llvm.icmp "slt" %52, %8 : i64
          %54 = llvm.select %53, %8, %52 : i1, i64
          %55 = llvm.icmp "sle" %35, %8 : i64
          %56 = llvm.sub %8, %35  : i64
          %57 = llvm.sub %35, %9  : i64
          %58 = llvm.select %55, %56, %57 : i1, i64
          %59 = llvm.sdiv %58, %11  : i64
          %60 = llvm.sub %8, %59  : i64
          %61 = llvm.add %59, %9  : i64
          %62 = llvm.select %55, %60, %61 : i1, i64
          %63 = llvm.mul %37, %62  : i64
          %64 = llvm.mul %63, %1  : i64
          %65 = llvm.add %35, %64  : i64
          %66 = llvm.icmp "slt" %65, %62 : i64
          %67 = llvm.select %66, %65, %62 : i1, i64
          %68 = llvm.icmp "slt" %67, %8 : i64
          %69 = llvm.select %68, %8, %67 : i1, i64
          llvm.br ^bb1(%8 : i64)
        ^bb1(%70: i64):  // 2 preds: ^bb0, ^bb4
          %71 = llvm.icmp "slt" %70, %54 : i64
          llvm.cond_br %71, ^bb2(%8 : i64), ^bb5(%8 : i64)
        ^bb2(%72: i64):  // 2 preds: ^bb1, ^bb3
          %73 = llvm.icmp "slt" %72, %69 : i64
          llvm.cond_br %73, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %74 = llvm.add %26, %48  : i64
          %75 = llvm.add %74, %70  : i64
          %76 = llvm.add %31, %63  : i64
          %77 = llvm.add %76, %72  : i64
          %78 = llvm.mul %75, %3  : i64
          %79 = llvm.add %78, %77  : i64
          %80 = llvm.getelementptr %arg2[%79] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %12, %80 : !llvm.ptr<f32>
          %81 = llvm.add %72, %9  : i64
          llvm.br ^bb2(%81 : i64)
        ^bb4:  // pred: ^bb2
          %82 = llvm.add %70, %9  : i64
          llvm.br ^bb1(%82 : i64)
        ^bb5(%83: i64):  // 2 preds: ^bb1, ^bb12
          %84 = llvm.icmp "slt" %83, %10 : i64
          llvm.cond_br %84, ^bb6(%8 : i64), ^bb13
        ^bb6(%85: i64):  // 2 preds: ^bb5, ^bb11
          %86 = llvm.icmp "slt" %85, %54 : i64
          llvm.cond_br %86, ^bb7(%8 : i64), ^bb12
        ^bb7(%87: i64):  // 2 preds: ^bb6, ^bb10
          %88 = llvm.icmp "slt" %87, %69 : i64
          llvm.cond_br %88, ^bb8(%8 : i64), ^bb11
        ^bb8(%89: i64):  // 2 preds: ^bb7, ^bb9
          %90 = llvm.icmp "slt" %89, %11 : i64
          llvm.cond_br %90, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %91 = llvm.add %83, %89  : i64
          %92 = llvm.add %26, %48  : i64
          %93 = llvm.add %92, %85  : i64
          %94 = llvm.mul %93, %10  : i64
          %95 = llvm.add %94, %91  : i64
          %96 = llvm.getelementptr %arg0[%95] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %97 = llvm.load %96 : !llvm.ptr<f32>
          %98 = llvm.add %31, %63  : i64
          %99 = llvm.add %98, %87  : i64
          %100 = llvm.mul %91, %3  : i64
          %101 = llvm.add %100, %99  : i64
          %102 = llvm.getelementptr %arg1[%101] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %103 = llvm.load %102 : !llvm.ptr<f32>
          %104 = llvm.mul %93, %3  : i64
          %105 = llvm.add %104, %99  : i64
          %106 = llvm.getelementptr %arg2[%105] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %107 = llvm.load %106 : !llvm.ptr<f32>
          %108 = llvm.fmul %97, %103  : f32
          %109 = llvm.fadd %107, %108  : f32
          llvm.store %109, %106 : !llvm.ptr<f32>
          %110 = llvm.add %89, %9  : i64
          llvm.br ^bb8(%110 : i64)
        ^bb10:  // pred: ^bb8
          %111 = llvm.add %87, %9  : i64
          llvm.br ^bb7(%111 : i64)
        ^bb11:  // pred: ^bb7
          %112 = llvm.add %85, %9  : i64
          llvm.br ^bb6(%112 : i64)
        ^bb12:  // pred: ^bb6
          %113 = llvm.add %83, %11  : i64
          llvm.br ^bb5(%113 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
        %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@matmul_static_dispatch_0::@cuda_nvptx_fb::@matmul_static_dispatch_0_matmul_512x1024x256) workgroups([%c8, %c16, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(-1 : index) : i64
          %2 = llvm.mlir.constant(8 : index) : i64
          %3 = llvm.mlir.constant(1024 : index) : i64
          %4 = llvm.mlir.constant(-128 : index) : i64
          %5 = llvm.mlir.constant(128 : index) : i64
          %6 = llvm.mlir.constant(512 : index) : i64
          %7 = llvm.mlir.constant(-32 : index) : i64
          %8 = llvm.mlir.constant(0 : index) : i64
          %9 = llvm.mlir.constant(1 : index) : i64
          %10 = llvm.mlir.constant(256 : index) : i64
          %11 = llvm.mlir.constant(32 : index) : i64
          %12 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %13 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %8 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %17 = llvm.and %16, %0  : i64
          %18 = llvm.icmp "eq" %17, %8 : i64
          "llvm.intr.assume"(%18) : (i1) -> ()
          %19 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %20 = llvm.and %19, %0  : i64
          %21 = llvm.icmp "eq" %20, %8 : i64
          "llvm.intr.assume"(%21) : (i1) -> ()
          %22 = nvvm.read.ptx.sreg.ctaid.x : i32
          %23 = llvm.sext %22 : i32 to i64
          %24 = nvvm.read.ptx.sreg.ctaid.y : i32
          %25 = llvm.sext %24 : i32 to i64
          %26 = llvm.mul %25, %11  : i64
          %27 = llvm.mul %25, %7  : i64
          %28 = llvm.add %27, %6  : i64
          %29 = llvm.icmp "slt" %28, %11 : i64
          %30 = llvm.select %29, %28, %11 : i1, i64
          %31 = llvm.mul %23, %5  : i64
          %32 = llvm.mul %23, %4  : i64
          %33 = llvm.add %32, %3  : i64
          %34 = llvm.icmp "slt" %33, %5 : i64
          %35 = llvm.select %34, %33, %5 : i1, i64
          %36 = nvvm.read.ptx.sreg.tid.x : i32
          %37 = llvm.sext %36 : i32 to i64
          %38 = nvvm.read.ptx.sreg.tid.y : i32
          %39 = llvm.sext %38 : i32 to i64
          %40 = llvm.icmp "sle" %30, %8 : i64
          %41 = llvm.sub %8, %30  : i64
          %42 = llvm.sub %30, %9  : i64
          %43 = llvm.select %40, %41, %42 : i1, i64
          %44 = llvm.sdiv %43, %2  : i64
          %45 = llvm.sub %8, %44  : i64
          %46 = llvm.add %44, %9  : i64
          %47 = llvm.select %40, %45, %46 : i1, i64
          %48 = llvm.mul %39, %47  : i64
          %49 = llvm.mul %48, %1  : i64
          %50 = llvm.add %30, %49  : i64
          %51 = llvm.icmp "slt" %50, %47 : i64
          %52 = llvm.select %51, %50, %47 : i1, i64
          %53 = llvm.icmp "slt" %52, %8 : i64
          %54 = llvm.select %53, %8, %52 : i1, i64
          %55 = llvm.icmp "sle" %35, %8 : i64
          %56 = llvm.sub %8, %35  : i64
          %57 = llvm.sub %35, %9  : i64
          %58 = llvm.select %55, %56, %57 : i1, i64
          %59 = llvm.sdiv %58, %11  : i64
          %60 = llvm.sub %8, %59  : i64
          %61 = llvm.add %59, %9  : i64
          %62 = llvm.select %55, %60, %61 : i1, i64
          %63 = llvm.mul %37, %62  : i64
          %64 = llvm.mul %63, %1  : i64
          %65 = llvm.add %35, %64  : i64
          %66 = llvm.icmp "slt" %65, %62 : i64
          %67 = llvm.select %66, %65, %62 : i1, i64
          %68 = llvm.icmp "slt" %67, %8 : i64
          %69 = llvm.select %68, %8, %67 : i1, i64
          llvm.br ^bb1(%8 : i64)
        ^bb1(%70: i64):  // 2 preds: ^bb0, ^bb4
          %71 = llvm.icmp "slt" %70, %54 : i64
          llvm.cond_br %71, ^bb2(%8 : i64), ^bb5(%8 : i64)
        ^bb2(%72: i64):  // 2 preds: ^bb1, ^bb3
          %73 = llvm.icmp "slt" %72, %69 : i64
          llvm.cond_br %73, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %74 = llvm.add %26, %48  : i64
          %75 = llvm.add %74, %70  : i64
          %76 = llvm.add %31, %63  : i64
          %77 = llvm.add %76, %72  : i64
          %78 = llvm.mul %75, %3  : i64
          %79 = llvm.add %78, %77  : i64
          %80 = llvm.getelementptr %arg2[%79] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %12, %80 : !llvm.ptr<f32>
          %81 = llvm.add %72, %9  : i64
          llvm.br ^bb2(%81 : i64)
        ^bb4:  // pred: ^bb2
          %82 = llvm.add %70, %9  : i64
          llvm.br ^bb1(%82 : i64)
        ^bb5(%83: i64):  // 2 preds: ^bb1, ^bb12
          %84 = llvm.icmp "slt" %83, %10 : i64
          llvm.cond_br %84, ^bb6(%8 : i64), ^bb13
        ^bb6(%85: i64):  // 2 preds: ^bb5, ^bb11
          %86 = llvm.icmp "slt" %85, %54 : i64
          llvm.cond_br %86, ^bb7(%8 : i64), ^bb12
        ^bb7(%87: i64):  // 2 preds: ^bb6, ^bb10
          %88 = llvm.icmp "slt" %87, %69 : i64
          llvm.cond_br %88, ^bb8(%8 : i64), ^bb11
        ^bb8(%89: i64):  // 2 preds: ^bb7, ^bb9
          %90 = llvm.icmp "slt" %89, %11 : i64
          llvm.cond_br %90, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %91 = llvm.add %83, %89  : i64
          %92 = llvm.add %26, %48  : i64
          %93 = llvm.add %92, %85  : i64
          %94 = llvm.mul %93, %10  : i64
          %95 = llvm.add %94, %91  : i64
          %96 = llvm.getelementptr %arg0[%95] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %97 = llvm.load %96 : !llvm.ptr<f32>
          %98 = llvm.add %31, %63  : i64
          %99 = llvm.add %98, %87  : i64
          %100 = llvm.mul %91, %3  : i64
          %101 = llvm.add %100, %99  : i64
          %102 = llvm.getelementptr %arg1[%101] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %103 = llvm.load %102 : !llvm.ptr<f32>
          %104 = llvm.mul %93, %3  : i64
          %105 = llvm.add %104, %99  : i64
          %106 = llvm.getelementptr %arg2[%105] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %107 = llvm.load %106 : !llvm.ptr<f32>
          %108 = llvm.fmul %97, %103  : f32
          %109 = llvm.fadd %107, %108  : f32
          llvm.store %109, %106 : !llvm.ptr<f32>
          %110 = llvm.add %89, %9  : i64
          llvm.br ^bb8(%110 : i64)
        ^bb10:  // pred: ^bb8
          %111 = llvm.add %87, %9  : i64
          llvm.br ^bb7(%111 : i64)
        ^bb11:  // pred: ^bb7
          %112 = llvm.add %85, %9  : i64
          llvm.br ^bb6(%112 : i64)
        ^bb12:  // pred: ^bb6
          %113 = llvm.add %83, %11  : i64
          llvm.br ^bb5(%113 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
        %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@matmul_static_dispatch_0::@cuda_nvptx_fb::@matmul_static_dispatch_0_matmul_512x1024x256) workgroups([%c8, %c16, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::LinkExecutablesPass (iree-hal-link-executables) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(-1 : index) : i64
          %2 = llvm.mlir.constant(8 : index) : i64
          %3 = llvm.mlir.constant(1024 : index) : i64
          %4 = llvm.mlir.constant(-128 : index) : i64
          %5 = llvm.mlir.constant(128 : index) : i64
          %6 = llvm.mlir.constant(512 : index) : i64
          %7 = llvm.mlir.constant(-32 : index) : i64
          %8 = llvm.mlir.constant(0 : index) : i64
          %9 = llvm.mlir.constant(1 : index) : i64
          %10 = llvm.mlir.constant(256 : index) : i64
          %11 = llvm.mlir.constant(32 : index) : i64
          %12 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %13 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %8 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %17 = llvm.and %16, %0  : i64
          %18 = llvm.icmp "eq" %17, %8 : i64
          "llvm.intr.assume"(%18) : (i1) -> ()
          %19 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %20 = llvm.and %19, %0  : i64
          %21 = llvm.icmp "eq" %20, %8 : i64
          "llvm.intr.assume"(%21) : (i1) -> ()
          %22 = nvvm.read.ptx.sreg.ctaid.x : i32
          %23 = llvm.sext %22 : i32 to i64
          %24 = nvvm.read.ptx.sreg.ctaid.y : i32
          %25 = llvm.sext %24 : i32 to i64
          %26 = llvm.mul %25, %11  : i64
          %27 = llvm.mul %25, %7  : i64
          %28 = llvm.add %27, %6  : i64
          %29 = llvm.icmp "slt" %28, %11 : i64
          %30 = llvm.select %29, %28, %11 : i1, i64
          %31 = llvm.mul %23, %5  : i64
          %32 = llvm.mul %23, %4  : i64
          %33 = llvm.add %32, %3  : i64
          %34 = llvm.icmp "slt" %33, %5 : i64
          %35 = llvm.select %34, %33, %5 : i1, i64
          %36 = nvvm.read.ptx.sreg.tid.x : i32
          %37 = llvm.sext %36 : i32 to i64
          %38 = nvvm.read.ptx.sreg.tid.y : i32
          %39 = llvm.sext %38 : i32 to i64
          %40 = llvm.icmp "sle" %30, %8 : i64
          %41 = llvm.sub %8, %30  : i64
          %42 = llvm.sub %30, %9  : i64
          %43 = llvm.select %40, %41, %42 : i1, i64
          %44 = llvm.sdiv %43, %2  : i64
          %45 = llvm.sub %8, %44  : i64
          %46 = llvm.add %44, %9  : i64
          %47 = llvm.select %40, %45, %46 : i1, i64
          %48 = llvm.mul %39, %47  : i64
          %49 = llvm.mul %48, %1  : i64
          %50 = llvm.add %30, %49  : i64
          %51 = llvm.icmp "slt" %50, %47 : i64
          %52 = llvm.select %51, %50, %47 : i1, i64
          %53 = llvm.icmp "slt" %52, %8 : i64
          %54 = llvm.select %53, %8, %52 : i1, i64
          %55 = llvm.icmp "sle" %35, %8 : i64
          %56 = llvm.sub %8, %35  : i64
          %57 = llvm.sub %35, %9  : i64
          %58 = llvm.select %55, %56, %57 : i1, i64
          %59 = llvm.sdiv %58, %11  : i64
          %60 = llvm.sub %8, %59  : i64
          %61 = llvm.add %59, %9  : i64
          %62 = llvm.select %55, %60, %61 : i1, i64
          %63 = llvm.mul %37, %62  : i64
          %64 = llvm.mul %63, %1  : i64
          %65 = llvm.add %35, %64  : i64
          %66 = llvm.icmp "slt" %65, %62 : i64
          %67 = llvm.select %66, %65, %62 : i1, i64
          %68 = llvm.icmp "slt" %67, %8 : i64
          %69 = llvm.select %68, %8, %67 : i1, i64
          llvm.br ^bb1(%8 : i64)
        ^bb1(%70: i64):  // 2 preds: ^bb0, ^bb4
          %71 = llvm.icmp "slt" %70, %54 : i64
          llvm.cond_br %71, ^bb2(%8 : i64), ^bb5(%8 : i64)
        ^bb2(%72: i64):  // 2 preds: ^bb1, ^bb3
          %73 = llvm.icmp "slt" %72, %69 : i64
          llvm.cond_br %73, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %74 = llvm.add %26, %48  : i64
          %75 = llvm.add %74, %70  : i64
          %76 = llvm.add %31, %63  : i64
          %77 = llvm.add %76, %72  : i64
          %78 = llvm.mul %75, %3  : i64
          %79 = llvm.add %78, %77  : i64
          %80 = llvm.getelementptr %arg2[%79] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %12, %80 : !llvm.ptr<f32>
          %81 = llvm.add %72, %9  : i64
          llvm.br ^bb2(%81 : i64)
        ^bb4:  // pred: ^bb2
          %82 = llvm.add %70, %9  : i64
          llvm.br ^bb1(%82 : i64)
        ^bb5(%83: i64):  // 2 preds: ^bb1, ^bb12
          %84 = llvm.icmp "slt" %83, %10 : i64
          llvm.cond_br %84, ^bb6(%8 : i64), ^bb13
        ^bb6(%85: i64):  // 2 preds: ^bb5, ^bb11
          %86 = llvm.icmp "slt" %85, %54 : i64
          llvm.cond_br %86, ^bb7(%8 : i64), ^bb12
        ^bb7(%87: i64):  // 2 preds: ^bb6, ^bb10
          %88 = llvm.icmp "slt" %87, %69 : i64
          llvm.cond_br %88, ^bb8(%8 : i64), ^bb11
        ^bb8(%89: i64):  // 2 preds: ^bb7, ^bb9
          %90 = llvm.icmp "slt" %89, %11 : i64
          llvm.cond_br %90, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %91 = llvm.add %83, %89  : i64
          %92 = llvm.add %26, %48  : i64
          %93 = llvm.add %92, %85  : i64
          %94 = llvm.mul %93, %10  : i64
          %95 = llvm.add %94, %91  : i64
          %96 = llvm.getelementptr %arg0[%95] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %97 = llvm.load %96 : !llvm.ptr<f32>
          %98 = llvm.add %31, %63  : i64
          %99 = llvm.add %98, %87  : i64
          %100 = llvm.mul %91, %3  : i64
          %101 = llvm.add %100, %99  : i64
          %102 = llvm.getelementptr %arg1[%101] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %103 = llvm.load %102 : !llvm.ptr<f32>
          %104 = llvm.mul %93, %3  : i64
          %105 = llvm.add %104, %99  : i64
          %106 = llvm.getelementptr %arg2[%105] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %107 = llvm.load %106 : !llvm.ptr<f32>
          %108 = llvm.fmul %97, %103  : f32
          %109 = llvm.fadd %107, %108  : f32
          llvm.store %109, %106 : !llvm.ptr<f32>
          %110 = llvm.add %89, %9  : i64
          llvm.br ^bb8(%110 : i64)
        ^bb10:  // pred: ^bb8
          %111 = llvm.add %87, %9  : i64
          llvm.br ^bb7(%111 : i64)
        ^bb11:  // pred: ^bb7
          %112 = llvm.add %85, %9  : i64
          llvm.br ^bb6(%112 : i64)
        ^bb12:  // pred: ^bb6
          %113 = llvm.add %83, %11  : i64
          llvm.br ^bb5(%113 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
        %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@matmul_static_dispatch_0::@cuda_nvptx_fb::@matmul_static_dispatch_0_matmul_512x1024x256) workgroups([%c8, %c16, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ResolveExportOrdinalsPass (iree-hal-resolve-export-ordinals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(-1 : index) : i64
          %2 = llvm.mlir.constant(8 : index) : i64
          %3 = llvm.mlir.constant(1024 : index) : i64
          %4 = llvm.mlir.constant(-128 : index) : i64
          %5 = llvm.mlir.constant(128 : index) : i64
          %6 = llvm.mlir.constant(512 : index) : i64
          %7 = llvm.mlir.constant(-32 : index) : i64
          %8 = llvm.mlir.constant(0 : index) : i64
          %9 = llvm.mlir.constant(1 : index) : i64
          %10 = llvm.mlir.constant(256 : index) : i64
          %11 = llvm.mlir.constant(32 : index) : i64
          %12 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %13 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %8 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %17 = llvm.and %16, %0  : i64
          %18 = llvm.icmp "eq" %17, %8 : i64
          "llvm.intr.assume"(%18) : (i1) -> ()
          %19 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %20 = llvm.and %19, %0  : i64
          %21 = llvm.icmp "eq" %20, %8 : i64
          "llvm.intr.assume"(%21) : (i1) -> ()
          %22 = nvvm.read.ptx.sreg.ctaid.x : i32
          %23 = llvm.sext %22 : i32 to i64
          %24 = nvvm.read.ptx.sreg.ctaid.y : i32
          %25 = llvm.sext %24 : i32 to i64
          %26 = llvm.mul %25, %11  : i64
          %27 = llvm.mul %25, %7  : i64
          %28 = llvm.add %27, %6  : i64
          %29 = llvm.icmp "slt" %28, %11 : i64
          %30 = llvm.select %29, %28, %11 : i1, i64
          %31 = llvm.mul %23, %5  : i64
          %32 = llvm.mul %23, %4  : i64
          %33 = llvm.add %32, %3  : i64
          %34 = llvm.icmp "slt" %33, %5 : i64
          %35 = llvm.select %34, %33, %5 : i1, i64
          %36 = nvvm.read.ptx.sreg.tid.x : i32
          %37 = llvm.sext %36 : i32 to i64
          %38 = nvvm.read.ptx.sreg.tid.y : i32
          %39 = llvm.sext %38 : i32 to i64
          %40 = llvm.icmp "sle" %30, %8 : i64
          %41 = llvm.sub %8, %30  : i64
          %42 = llvm.sub %30, %9  : i64
          %43 = llvm.select %40, %41, %42 : i1, i64
          %44 = llvm.sdiv %43, %2  : i64
          %45 = llvm.sub %8, %44  : i64
          %46 = llvm.add %44, %9  : i64
          %47 = llvm.select %40, %45, %46 : i1, i64
          %48 = llvm.mul %39, %47  : i64
          %49 = llvm.mul %48, %1  : i64
          %50 = llvm.add %30, %49  : i64
          %51 = llvm.icmp "slt" %50, %47 : i64
          %52 = llvm.select %51, %50, %47 : i1, i64
          %53 = llvm.icmp "slt" %52, %8 : i64
          %54 = llvm.select %53, %8, %52 : i1, i64
          %55 = llvm.icmp "sle" %35, %8 : i64
          %56 = llvm.sub %8, %35  : i64
          %57 = llvm.sub %35, %9  : i64
          %58 = llvm.select %55, %56, %57 : i1, i64
          %59 = llvm.sdiv %58, %11  : i64
          %60 = llvm.sub %8, %59  : i64
          %61 = llvm.add %59, %9  : i64
          %62 = llvm.select %55, %60, %61 : i1, i64
          %63 = llvm.mul %37, %62  : i64
          %64 = llvm.mul %63, %1  : i64
          %65 = llvm.add %35, %64  : i64
          %66 = llvm.icmp "slt" %65, %62 : i64
          %67 = llvm.select %66, %65, %62 : i1, i64
          %68 = llvm.icmp "slt" %67, %8 : i64
          %69 = llvm.select %68, %8, %67 : i1, i64
          llvm.br ^bb1(%8 : i64)
        ^bb1(%70: i64):  // 2 preds: ^bb0, ^bb4
          %71 = llvm.icmp "slt" %70, %54 : i64
          llvm.cond_br %71, ^bb2(%8 : i64), ^bb5(%8 : i64)
        ^bb2(%72: i64):  // 2 preds: ^bb1, ^bb3
          %73 = llvm.icmp "slt" %72, %69 : i64
          llvm.cond_br %73, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %74 = llvm.add %26, %48  : i64
          %75 = llvm.add %74, %70  : i64
          %76 = llvm.add %31, %63  : i64
          %77 = llvm.add %76, %72  : i64
          %78 = llvm.mul %75, %3  : i64
          %79 = llvm.add %78, %77  : i64
          %80 = llvm.getelementptr %arg2[%79] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %12, %80 : !llvm.ptr<f32>
          %81 = llvm.add %72, %9  : i64
          llvm.br ^bb2(%81 : i64)
        ^bb4:  // pred: ^bb2
          %82 = llvm.add %70, %9  : i64
          llvm.br ^bb1(%82 : i64)
        ^bb5(%83: i64):  // 2 preds: ^bb1, ^bb12
          %84 = llvm.icmp "slt" %83, %10 : i64
          llvm.cond_br %84, ^bb6(%8 : i64), ^bb13
        ^bb6(%85: i64):  // 2 preds: ^bb5, ^bb11
          %86 = llvm.icmp "slt" %85, %54 : i64
          llvm.cond_br %86, ^bb7(%8 : i64), ^bb12
        ^bb7(%87: i64):  // 2 preds: ^bb6, ^bb10
          %88 = llvm.icmp "slt" %87, %69 : i64
          llvm.cond_br %88, ^bb8(%8 : i64), ^bb11
        ^bb8(%89: i64):  // 2 preds: ^bb7, ^bb9
          %90 = llvm.icmp "slt" %89, %11 : i64
          llvm.cond_br %90, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %91 = llvm.add %83, %89  : i64
          %92 = llvm.add %26, %48  : i64
          %93 = llvm.add %92, %85  : i64
          %94 = llvm.mul %93, %10  : i64
          %95 = llvm.add %94, %91  : i64
          %96 = llvm.getelementptr %arg0[%95] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %97 = llvm.load %96 : !llvm.ptr<f32>
          %98 = llvm.add %31, %63  : i64
          %99 = llvm.add %98, %87  : i64
          %100 = llvm.mul %91, %3  : i64
          %101 = llvm.add %100, %99  : i64
          %102 = llvm.getelementptr %arg1[%101] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %103 = llvm.load %102 : !llvm.ptr<f32>
          %104 = llvm.mul %93, %3  : i64
          %105 = llvm.add %104, %99  : i64
          %106 = llvm.getelementptr %arg2[%105] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %107 = llvm.load %106 : !llvm.ptr<f32>
          %108 = llvm.fmul %97, %103  : f32
          %109 = llvm.fadd %107, %108  : f32
          llvm.store %109, %106 : !llvm.ptr<f32>
          %110 = llvm.add %89, %9  : i64
          llvm.br ^bb8(%110 : i64)
        ^bb10:  // pred: ^bb8
          %111 = llvm.add %87, %9  : i64
          llvm.br ^bb7(%111 : i64)
        ^bb11:  // pred: ^bb7
          %112 = llvm.add %85, %9  : i64
          llvm.br ^bb6(%112 : i64)
        ^bb12:  // pred: ^bb6
          %113 = llvm.add %83, %11  : i64
          llvm.br ^bb5(%113 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
        %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
      ])
      %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
      %exe = hal.executable.lookup device(%1 : !hal.device) executable(@matmul_static_dispatch_0) : !hal.executable
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[0] workgroups([%c8, %c16, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::MaterializeResourceCachesPass (iree-hal-materialize-resource-caches) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %0 = hal.device.switch<%device : !hal.device> -> !hal.executable
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
      hal.return %exe : !hal.executable
    },
    #hal.match.always {
      %1 = util.null : !hal.executable
      hal.return %1 : !hal.executable
    }
    util.global.store %0, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(-1 : index) : i64
          %2 = llvm.mlir.constant(8 : index) : i64
          %3 = llvm.mlir.constant(1024 : index) : i64
          %4 = llvm.mlir.constant(-128 : index) : i64
          %5 = llvm.mlir.constant(128 : index) : i64
          %6 = llvm.mlir.constant(512 : index) : i64
          %7 = llvm.mlir.constant(-32 : index) : i64
          %8 = llvm.mlir.constant(0 : index) : i64
          %9 = llvm.mlir.constant(1 : index) : i64
          %10 = llvm.mlir.constant(256 : index) : i64
          %11 = llvm.mlir.constant(32 : index) : i64
          %12 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %13 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %8 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %17 = llvm.and %16, %0  : i64
          %18 = llvm.icmp "eq" %17, %8 : i64
          "llvm.intr.assume"(%18) : (i1) -> ()
          %19 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %20 = llvm.and %19, %0  : i64
          %21 = llvm.icmp "eq" %20, %8 : i64
          "llvm.intr.assume"(%21) : (i1) -> ()
          %22 = nvvm.read.ptx.sreg.ctaid.x : i32
          %23 = llvm.sext %22 : i32 to i64
          %24 = nvvm.read.ptx.sreg.ctaid.y : i32
          %25 = llvm.sext %24 : i32 to i64
          %26 = llvm.mul %25, %11  : i64
          %27 = llvm.mul %25, %7  : i64
          %28 = llvm.add %27, %6  : i64
          %29 = llvm.icmp "slt" %28, %11 : i64
          %30 = llvm.select %29, %28, %11 : i1, i64
          %31 = llvm.mul %23, %5  : i64
          %32 = llvm.mul %23, %4  : i64
          %33 = llvm.add %32, %3  : i64
          %34 = llvm.icmp "slt" %33, %5 : i64
          %35 = llvm.select %34, %33, %5 : i1, i64
          %36 = nvvm.read.ptx.sreg.tid.x : i32
          %37 = llvm.sext %36 : i32 to i64
          %38 = nvvm.read.ptx.sreg.tid.y : i32
          %39 = llvm.sext %38 : i32 to i64
          %40 = llvm.icmp "sle" %30, %8 : i64
          %41 = llvm.sub %8, %30  : i64
          %42 = llvm.sub %30, %9  : i64
          %43 = llvm.select %40, %41, %42 : i1, i64
          %44 = llvm.sdiv %43, %2  : i64
          %45 = llvm.sub %8, %44  : i64
          %46 = llvm.add %44, %9  : i64
          %47 = llvm.select %40, %45, %46 : i1, i64
          %48 = llvm.mul %39, %47  : i64
          %49 = llvm.mul %48, %1  : i64
          %50 = llvm.add %30, %49  : i64
          %51 = llvm.icmp "slt" %50, %47 : i64
          %52 = llvm.select %51, %50, %47 : i1, i64
          %53 = llvm.icmp "slt" %52, %8 : i64
          %54 = llvm.select %53, %8, %52 : i1, i64
          %55 = llvm.icmp "sle" %35, %8 : i64
          %56 = llvm.sub %8, %35  : i64
          %57 = llvm.sub %35, %9  : i64
          %58 = llvm.select %55, %56, %57 : i1, i64
          %59 = llvm.sdiv %58, %11  : i64
          %60 = llvm.sub %8, %59  : i64
          %61 = llvm.add %59, %9  : i64
          %62 = llvm.select %55, %60, %61 : i1, i64
          %63 = llvm.mul %37, %62  : i64
          %64 = llvm.mul %63, %1  : i64
          %65 = llvm.add %35, %64  : i64
          %66 = llvm.icmp "slt" %65, %62 : i64
          %67 = llvm.select %66, %65, %62 : i1, i64
          %68 = llvm.icmp "slt" %67, %8 : i64
          %69 = llvm.select %68, %8, %67 : i1, i64
          llvm.br ^bb1(%8 : i64)
        ^bb1(%70: i64):  // 2 preds: ^bb0, ^bb4
          %71 = llvm.icmp "slt" %70, %54 : i64
          llvm.cond_br %71, ^bb2(%8 : i64), ^bb5(%8 : i64)
        ^bb2(%72: i64):  // 2 preds: ^bb1, ^bb3
          %73 = llvm.icmp "slt" %72, %69 : i64
          llvm.cond_br %73, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %74 = llvm.add %26, %48  : i64
          %75 = llvm.add %74, %70  : i64
          %76 = llvm.add %31, %63  : i64
          %77 = llvm.add %76, %72  : i64
          %78 = llvm.mul %75, %3  : i64
          %79 = llvm.add %78, %77  : i64
          %80 = llvm.getelementptr %arg2[%79] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %12, %80 : !llvm.ptr<f32>
          %81 = llvm.add %72, %9  : i64
          llvm.br ^bb2(%81 : i64)
        ^bb4:  // pred: ^bb2
          %82 = llvm.add %70, %9  : i64
          llvm.br ^bb1(%82 : i64)
        ^bb5(%83: i64):  // 2 preds: ^bb1, ^bb12
          %84 = llvm.icmp "slt" %83, %10 : i64
          llvm.cond_br %84, ^bb6(%8 : i64), ^bb13
        ^bb6(%85: i64):  // 2 preds: ^bb5, ^bb11
          %86 = llvm.icmp "slt" %85, %54 : i64
          llvm.cond_br %86, ^bb7(%8 : i64), ^bb12
        ^bb7(%87: i64):  // 2 preds: ^bb6, ^bb10
          %88 = llvm.icmp "slt" %87, %69 : i64
          llvm.cond_br %88, ^bb8(%8 : i64), ^bb11
        ^bb8(%89: i64):  // 2 preds: ^bb7, ^bb9
          %90 = llvm.icmp "slt" %89, %11 : i64
          llvm.cond_br %90, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %91 = llvm.add %83, %89  : i64
          %92 = llvm.add %26, %48  : i64
          %93 = llvm.add %92, %85  : i64
          %94 = llvm.mul %93, %10  : i64
          %95 = llvm.add %94, %91  : i64
          %96 = llvm.getelementptr %arg0[%95] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %97 = llvm.load %96 : !llvm.ptr<f32>
          %98 = llvm.add %31, %63  : i64
          %99 = llvm.add %98, %87  : i64
          %100 = llvm.mul %91, %3  : i64
          %101 = llvm.add %100, %99  : i64
          %102 = llvm.getelementptr %arg1[%101] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %103 = llvm.load %102 : !llvm.ptr<f32>
          %104 = llvm.mul %93, %3  : i64
          %105 = llvm.add %104, %99  : i64
          %106 = llvm.getelementptr %arg2[%105] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %107 = llvm.load %106 : !llvm.ptr<f32>
          %108 = llvm.fmul %97, %103  : f32
          %109 = llvm.fadd %107, %108  : f32
          llvm.store %109, %106 : !llvm.ptr<f32>
          %110 = llvm.add %89, %9  : i64
          llvm.br ^bb8(%110 : i64)
        ^bb10:  // pred: ^bb8
          %111 = llvm.add %87, %9  : i64
          llvm.br ^bb7(%111 : i64)
        ^bb11:  // pred: ^bb7
          %112 = llvm.add %85, %9  : i64
          llvm.br ^bb6(%112 : i64)
        ^bb12:  // pred: ^bb6
          %113 = llvm.add %83, %11  : i64
          llvm.br ^bb5(%113 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
        %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
      ])
      %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
      %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::InlineDeviceSwitchesPass (iree-hal-inline-device-switches) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::InlineDeviceSwitchesPass (iree-hal-inline-device-switches) //----- //
util.initializer {
  %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  %device = hal.ex.shared_device : !hal.device
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::InlineDeviceSwitchesPass (iree-hal-inline-device-switches) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb5(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %true = arith.constant true
  cf.cond_br %true, ^bb3, ^bb4
^bb3:  // pred: ^bb2
  %0 = util.null : !hal.executable
  cf.br ^bb5(%0 : !hal.executable)
^bb4:  // pred: ^bb2
  util.unreachable "device not supported in the compiled configuration"
^bb5(%1: !hal.executable):  // 2 preds: ^bb1, ^bb3
  util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::InlineDeviceSwitchesPass (iree-hal-inline-device-switches) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c2 = arith.constant 2 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
  ])
  %0 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
  %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
  cf.br ^bb3
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
^bb3:  // pred: ^bb1
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %1 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%1) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::MemoizeDeviceQueriesPass (iree-hal-memoize-device-queries) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %ok, @_device_query_0_ok : i1
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %_device_query_0_ok = util.global.load @_device_query_0_ok : i1
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb5(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %true = arith.constant true
    cf.cond_br %true, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %0 = util.null : !hal.executable
    cf.br ^bb5(%0 : !hal.executable)
  ^bb4:  // pred: ^bb2
    util.unreachable "device not supported in the compiled configuration"
  ^bb5(%1: !hal.executable):  // 2 preds: ^bb1, ^bb3
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(-1 : index) : i64
          %2 = llvm.mlir.constant(8 : index) : i64
          %3 = llvm.mlir.constant(1024 : index) : i64
          %4 = llvm.mlir.constant(-128 : index) : i64
          %5 = llvm.mlir.constant(128 : index) : i64
          %6 = llvm.mlir.constant(512 : index) : i64
          %7 = llvm.mlir.constant(-32 : index) : i64
          %8 = llvm.mlir.constant(0 : index) : i64
          %9 = llvm.mlir.constant(1 : index) : i64
          %10 = llvm.mlir.constant(256 : index) : i64
          %11 = llvm.mlir.constant(32 : index) : i64
          %12 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %13 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %8 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %17 = llvm.and %16, %0  : i64
          %18 = llvm.icmp "eq" %17, %8 : i64
          "llvm.intr.assume"(%18) : (i1) -> ()
          %19 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %20 = llvm.and %19, %0  : i64
          %21 = llvm.icmp "eq" %20, %8 : i64
          "llvm.intr.assume"(%21) : (i1) -> ()
          %22 = nvvm.read.ptx.sreg.ctaid.x : i32
          %23 = llvm.sext %22 : i32 to i64
          %24 = nvvm.read.ptx.sreg.ctaid.y : i32
          %25 = llvm.sext %24 : i32 to i64
          %26 = llvm.mul %25, %11  : i64
          %27 = llvm.mul %25, %7  : i64
          %28 = llvm.add %27, %6  : i64
          %29 = llvm.icmp "slt" %28, %11 : i64
          %30 = llvm.select %29, %28, %11 : i1, i64
          %31 = llvm.mul %23, %5  : i64
          %32 = llvm.mul %23, %4  : i64
          %33 = llvm.add %32, %3  : i64
          %34 = llvm.icmp "slt" %33, %5 : i64
          %35 = llvm.select %34, %33, %5 : i1, i64
          %36 = nvvm.read.ptx.sreg.tid.x : i32
          %37 = llvm.sext %36 : i32 to i64
          %38 = nvvm.read.ptx.sreg.tid.y : i32
          %39 = llvm.sext %38 : i32 to i64
          %40 = llvm.icmp "sle" %30, %8 : i64
          %41 = llvm.sub %8, %30  : i64
          %42 = llvm.sub %30, %9  : i64
          %43 = llvm.select %40, %41, %42 : i1, i64
          %44 = llvm.sdiv %43, %2  : i64
          %45 = llvm.sub %8, %44  : i64
          %46 = llvm.add %44, %9  : i64
          %47 = llvm.select %40, %45, %46 : i1, i64
          %48 = llvm.mul %39, %47  : i64
          %49 = llvm.mul %48, %1  : i64
          %50 = llvm.add %30, %49  : i64
          %51 = llvm.icmp "slt" %50, %47 : i64
          %52 = llvm.select %51, %50, %47 : i1, i64
          %53 = llvm.icmp "slt" %52, %8 : i64
          %54 = llvm.select %53, %8, %52 : i1, i64
          %55 = llvm.icmp "sle" %35, %8 : i64
          %56 = llvm.sub %8, %35  : i64
          %57 = llvm.sub %35, %9  : i64
          %58 = llvm.select %55, %56, %57 : i1, i64
          %59 = llvm.sdiv %58, %11  : i64
          %60 = llvm.sub %8, %59  : i64
          %61 = llvm.add %59, %9  : i64
          %62 = llvm.select %55, %60, %61 : i1, i64
          %63 = llvm.mul %37, %62  : i64
          %64 = llvm.mul %63, %1  : i64
          %65 = llvm.add %35, %64  : i64
          %66 = llvm.icmp "slt" %65, %62 : i64
          %67 = llvm.select %66, %65, %62 : i1, i64
          %68 = llvm.icmp "slt" %67, %8 : i64
          %69 = llvm.select %68, %8, %67 : i1, i64
          llvm.br ^bb1(%8 : i64)
        ^bb1(%70: i64):  // 2 preds: ^bb0, ^bb4
          %71 = llvm.icmp "slt" %70, %54 : i64
          llvm.cond_br %71, ^bb2(%8 : i64), ^bb5(%8 : i64)
        ^bb2(%72: i64):  // 2 preds: ^bb1, ^bb3
          %73 = llvm.icmp "slt" %72, %69 : i64
          llvm.cond_br %73, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %74 = llvm.add %26, %48  : i64
          %75 = llvm.add %74, %70  : i64
          %76 = llvm.add %31, %63  : i64
          %77 = llvm.add %76, %72  : i64
          %78 = llvm.mul %75, %3  : i64
          %79 = llvm.add %78, %77  : i64
          %80 = llvm.getelementptr %arg2[%79] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %12, %80 : !llvm.ptr<f32>
          %81 = llvm.add %72, %9  : i64
          llvm.br ^bb2(%81 : i64)
        ^bb4:  // pred: ^bb2
          %82 = llvm.add %70, %9  : i64
          llvm.br ^bb1(%82 : i64)
        ^bb5(%83: i64):  // 2 preds: ^bb1, ^bb12
          %84 = llvm.icmp "slt" %83, %10 : i64
          llvm.cond_br %84, ^bb6(%8 : i64), ^bb13
        ^bb6(%85: i64):  // 2 preds: ^bb5, ^bb11
          %86 = llvm.icmp "slt" %85, %54 : i64
          llvm.cond_br %86, ^bb7(%8 : i64), ^bb12
        ^bb7(%87: i64):  // 2 preds: ^bb6, ^bb10
          %88 = llvm.icmp "slt" %87, %69 : i64
          llvm.cond_br %88, ^bb8(%8 : i64), ^bb11
        ^bb8(%89: i64):  // 2 preds: ^bb7, ^bb9
          %90 = llvm.icmp "slt" %89, %11 : i64
          llvm.cond_br %90, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %91 = llvm.add %83, %89  : i64
          %92 = llvm.add %26, %48  : i64
          %93 = llvm.add %92, %85  : i64
          %94 = llvm.mul %93, %10  : i64
          %95 = llvm.add %94, %91  : i64
          %96 = llvm.getelementptr %arg0[%95] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %97 = llvm.load %96 : !llvm.ptr<f32>
          %98 = llvm.add %31, %63  : i64
          %99 = llvm.add %98, %87  : i64
          %100 = llvm.mul %91, %3  : i64
          %101 = llvm.add %100, %99  : i64
          %102 = llvm.getelementptr %arg1[%101] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %103 = llvm.load %102 : !llvm.ptr<f32>
          %104 = llvm.mul %93, %3  : i64
          %105 = llvm.add %104, %99  : i64
          %106 = llvm.getelementptr %arg2[%105] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %107 = llvm.load %106 : !llvm.ptr<f32>
          %108 = llvm.fmul %97, %103  : f32
          %109 = llvm.fadd %107, %108  : f32
          llvm.store %109, %106 : !llvm.ptr<f32>
          %110 = llvm.add %89, %9  : i64
          llvm.br ^bb8(%110 : i64)
        ^bb10:  // pred: ^bb8
          %111 = llvm.add %87, %9  : i64
          llvm.br ^bb7(%111 : i64)
        ^bb11:  // pred: ^bb7
          %112 = llvm.add %85, %9  : i64
          llvm.br ^bb6(%112 : i64)
        ^bb12:  // pred: ^bb6
          %113 = llvm.add %83, %11  : i64
          llvm.br ^bb5(%113 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %_device_query_0_ok = util.global.load @_device_query_0_ok : i1
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    %0 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    cf.br ^bb3
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  ^bb3:  // pred: ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %1 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%1) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %ok, @_device_query_0_ok : i1
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(-1 : index) : i64
          %2 = llvm.mlir.constant(8 : index) : i64
          %3 = llvm.mlir.constant(1024 : index) : i64
          %4 = llvm.mlir.constant(-128 : index) : i64
          %5 = llvm.mlir.constant(128 : index) : i64
          %6 = llvm.mlir.constant(512 : index) : i64
          %7 = llvm.mlir.constant(-32 : index) : i64
          %8 = llvm.mlir.constant(0 : index) : i64
          %9 = llvm.mlir.constant(1 : index) : i64
          %10 = llvm.mlir.constant(256 : index) : i64
          %11 = llvm.mlir.constant(32 : index) : i64
          %12 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %13 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %8 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %17 = llvm.and %16, %0  : i64
          %18 = llvm.icmp "eq" %17, %8 : i64
          "llvm.intr.assume"(%18) : (i1) -> ()
          %19 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %20 = llvm.and %19, %0  : i64
          %21 = llvm.icmp "eq" %20, %8 : i64
          "llvm.intr.assume"(%21) : (i1) -> ()
          %22 = nvvm.read.ptx.sreg.ctaid.x : i32
          %23 = llvm.sext %22 : i32 to i64
          %24 = nvvm.read.ptx.sreg.ctaid.y : i32
          %25 = llvm.sext %24 : i32 to i64
          %26 = llvm.mul %25, %11  : i64
          %27 = llvm.mul %25, %7  : i64
          %28 = llvm.add %27, %6  : i64
          %29 = llvm.icmp "slt" %28, %11 : i64
          %30 = llvm.select %29, %28, %11 : i1, i64
          %31 = llvm.mul %23, %5  : i64
          %32 = llvm.mul %23, %4  : i64
          %33 = llvm.add %32, %3  : i64
          %34 = llvm.icmp "slt" %33, %5 : i64
          %35 = llvm.select %34, %33, %5 : i1, i64
          %36 = nvvm.read.ptx.sreg.tid.x : i32
          %37 = llvm.sext %36 : i32 to i64
          %38 = nvvm.read.ptx.sreg.tid.y : i32
          %39 = llvm.sext %38 : i32 to i64
          %40 = llvm.icmp "sle" %30, %8 : i64
          %41 = llvm.sub %8, %30  : i64
          %42 = llvm.sub %30, %9  : i64
          %43 = llvm.select %40, %41, %42 : i1, i64
          %44 = llvm.sdiv %43, %2  : i64
          %45 = llvm.sub %8, %44  : i64
          %46 = llvm.add %44, %9  : i64
          %47 = llvm.select %40, %45, %46 : i1, i64
          %48 = llvm.mul %39, %47  : i64
          %49 = llvm.mul %48, %1  : i64
          %50 = llvm.add %30, %49  : i64
          %51 = llvm.icmp "slt" %50, %47 : i64
          %52 = llvm.select %51, %50, %47 : i1, i64
          %53 = llvm.icmp "slt" %52, %8 : i64
          %54 = llvm.select %53, %8, %52 : i1, i64
          %55 = llvm.icmp "sle" %35, %8 : i64
          %56 = llvm.sub %8, %35  : i64
          %57 = llvm.sub %35, %9  : i64
          %58 = llvm.select %55, %56, %57 : i1, i64
          %59 = llvm.sdiv %58, %11  : i64
          %60 = llvm.sub %8, %59  : i64
          %61 = llvm.add %59, %9  : i64
          %62 = llvm.select %55, %60, %61 : i1, i64
          %63 = llvm.mul %37, %62  : i64
          %64 = llvm.mul %63, %1  : i64
          %65 = llvm.add %35, %64  : i64
          %66 = llvm.icmp "slt" %65, %62 : i64
          %67 = llvm.select %66, %65, %62 : i1, i64
          %68 = llvm.icmp "slt" %67, %8 : i64
          %69 = llvm.select %68, %8, %67 : i1, i64
          llvm.br ^bb1(%8 : i64)
        ^bb1(%70: i64):  // 2 preds: ^bb0, ^bb4
          %71 = llvm.icmp "slt" %70, %54 : i64
          llvm.cond_br %71, ^bb2(%8 : i64), ^bb5(%8 : i64)
        ^bb2(%72: i64):  // 2 preds: ^bb1, ^bb3
          %73 = llvm.icmp "slt" %72, %69 : i64
          llvm.cond_br %73, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %74 = llvm.add %26, %48  : i64
          %75 = llvm.add %74, %70  : i64
          %76 = llvm.add %31, %63  : i64
          %77 = llvm.add %76, %72  : i64
          %78 = llvm.mul %75, %3  : i64
          %79 = llvm.add %78, %77  : i64
          %80 = llvm.getelementptr %arg2[%79] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %12, %80 : !llvm.ptr<f32>
          %81 = llvm.add %72, %9  : i64
          llvm.br ^bb2(%81 : i64)
        ^bb4:  // pred: ^bb2
          %82 = llvm.add %70, %9  : i64
          llvm.br ^bb1(%82 : i64)
        ^bb5(%83: i64):  // 2 preds: ^bb1, ^bb12
          %84 = llvm.icmp "slt" %83, %10 : i64
          llvm.cond_br %84, ^bb6(%8 : i64), ^bb13
        ^bb6(%85: i64):  // 2 preds: ^bb5, ^bb11
          %86 = llvm.icmp "slt" %85, %54 : i64
          llvm.cond_br %86, ^bb7(%8 : i64), ^bb12
        ^bb7(%87: i64):  // 2 preds: ^bb6, ^bb10
          %88 = llvm.icmp "slt" %87, %69 : i64
          llvm.cond_br %88, ^bb8(%8 : i64), ^bb11
        ^bb8(%89: i64):  // 2 preds: ^bb7, ^bb9
          %90 = llvm.icmp "slt" %89, %11 : i64
          llvm.cond_br %90, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %91 = llvm.add %83, %89  : i64
          %92 = llvm.add %26, %48  : i64
          %93 = llvm.add %92, %85  : i64
          %94 = llvm.mul %93, %10  : i64
          %95 = llvm.add %94, %91  : i64
          %96 = llvm.getelementptr %arg0[%95] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %97 = llvm.load %96 : !llvm.ptr<f32>
          %98 = llvm.add %31, %63  : i64
          %99 = llvm.add %98, %87  : i64
          %100 = llvm.mul %91, %3  : i64
          %101 = llvm.add %100, %99  : i64
          %102 = llvm.getelementptr %arg1[%101] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %103 = llvm.load %102 : !llvm.ptr<f32>
          %104 = llvm.mul %93, %3  : i64
          %105 = llvm.add %104, %99  : i64
          %106 = llvm.getelementptr %arg2[%105] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %107 = llvm.load %106 : !llvm.ptr<f32>
          %108 = llvm.fmul %97, %103  : f32
          %109 = llvm.fadd %107, %108  : f32
          llvm.store %109, %106 : !llvm.ptr<f32>
          %110 = llvm.add %89, %9  : i64
          llvm.br ^bb8(%110 : i64)
        ^bb10:  // pred: ^bb8
          %111 = llvm.add %87, %9  : i64
          llvm.br ^bb7(%111 : i64)
        ^bb11:  // pred: ^bb7
          %112 = llvm.add %85, %9  : i64
          llvm.br ^bb6(%112 : i64)
        ^bb12:  // pred: ^bb6
          %113 = llvm.add %83, %11  : i64
          llvm.br ^bb5(%113 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %ok, @_device_query_0_ok : i1
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(-1 : index) : i64
          %2 = llvm.mlir.constant(8 : index) : i64
          %3 = llvm.mlir.constant(1024 : index) : i64
          %4 = llvm.mlir.constant(-128 : index) : i64
          %5 = llvm.mlir.constant(128 : index) : i64
          %6 = llvm.mlir.constant(512 : index) : i64
          %7 = llvm.mlir.constant(-32 : index) : i64
          %8 = llvm.mlir.constant(0 : index) : i64
          %9 = llvm.mlir.constant(1 : index) : i64
          %10 = llvm.mlir.constant(256 : index) : i64
          %11 = llvm.mlir.constant(32 : index) : i64
          %12 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %13 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %8 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %17 = llvm.and %16, %0  : i64
          %18 = llvm.icmp "eq" %17, %8 : i64
          "llvm.intr.assume"(%18) : (i1) -> ()
          %19 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %20 = llvm.and %19, %0  : i64
          %21 = llvm.icmp "eq" %20, %8 : i64
          "llvm.intr.assume"(%21) : (i1) -> ()
          %22 = nvvm.read.ptx.sreg.ctaid.x : i32
          %23 = llvm.sext %22 : i32 to i64
          %24 = nvvm.read.ptx.sreg.ctaid.y : i32
          %25 = llvm.sext %24 : i32 to i64
          %26 = llvm.mul %25, %11  : i64
          %27 = llvm.mul %25, %7  : i64
          %28 = llvm.add %27, %6  : i64
          %29 = llvm.icmp "slt" %28, %11 : i64
          %30 = llvm.select %29, %28, %11 : i1, i64
          %31 = llvm.mul %23, %5  : i64
          %32 = llvm.mul %23, %4  : i64
          %33 = llvm.add %32, %3  : i64
          %34 = llvm.icmp "slt" %33, %5 : i64
          %35 = llvm.select %34, %33, %5 : i1, i64
          %36 = nvvm.read.ptx.sreg.tid.x : i32
          %37 = llvm.sext %36 : i32 to i64
          %38 = nvvm.read.ptx.sreg.tid.y : i32
          %39 = llvm.sext %38 : i32 to i64
          %40 = llvm.icmp "sle" %30, %8 : i64
          %41 = llvm.sub %8, %30  : i64
          %42 = llvm.sub %30, %9  : i64
          %43 = llvm.select %40, %41, %42 : i1, i64
          %44 = llvm.sdiv %43, %2  : i64
          %45 = llvm.sub %8, %44  : i64
          %46 = llvm.add %44, %9  : i64
          %47 = llvm.select %40, %45, %46 : i1, i64
          %48 = llvm.mul %39, %47  : i64
          %49 = llvm.mul %48, %1  : i64
          %50 = llvm.add %30, %49  : i64
          %51 = llvm.icmp "slt" %50, %47 : i64
          %52 = llvm.select %51, %50, %47 : i1, i64
          %53 = llvm.icmp "slt" %52, %8 : i64
          %54 = llvm.select %53, %8, %52 : i1, i64
          %55 = llvm.icmp "sle" %35, %8 : i64
          %56 = llvm.sub %8, %35  : i64
          %57 = llvm.sub %35, %9  : i64
          %58 = llvm.select %55, %56, %57 : i1, i64
          %59 = llvm.sdiv %58, %11  : i64
          %60 = llvm.sub %8, %59  : i64
          %61 = llvm.add %59, %9  : i64
          %62 = llvm.select %55, %60, %61 : i1, i64
          %63 = llvm.mul %37, %62  : i64
          %64 = llvm.mul %63, %1  : i64
          %65 = llvm.add %35, %64  : i64
          %66 = llvm.icmp "slt" %65, %62 : i64
          %67 = llvm.select %66, %65, %62 : i1, i64
          %68 = llvm.icmp "slt" %67, %8 : i64
          %69 = llvm.select %68, %8, %67 : i1, i64
          llvm.br ^bb1(%8 : i64)
        ^bb1(%70: i64):  // 2 preds: ^bb0, ^bb4
          %71 = llvm.icmp "slt" %70, %54 : i64
          llvm.cond_br %71, ^bb2(%8 : i64), ^bb5(%8 : i64)
        ^bb2(%72: i64):  // 2 preds: ^bb1, ^bb3
          %73 = llvm.icmp "slt" %72, %69 : i64
          llvm.cond_br %73, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %74 = llvm.add %26, %48  : i64
          %75 = llvm.add %74, %70  : i64
          %76 = llvm.add %31, %63  : i64
          %77 = llvm.add %76, %72  : i64
          %78 = llvm.mul %75, %3  : i64
          %79 = llvm.add %78, %77  : i64
          %80 = llvm.getelementptr %arg2[%79] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %12, %80 : !llvm.ptr<f32>
          %81 = llvm.add %72, %9  : i64
          llvm.br ^bb2(%81 : i64)
        ^bb4:  // pred: ^bb2
          %82 = llvm.add %70, %9  : i64
          llvm.br ^bb1(%82 : i64)
        ^bb5(%83: i64):  // 2 preds: ^bb1, ^bb12
          %84 = llvm.icmp "slt" %83, %10 : i64
          llvm.cond_br %84, ^bb6(%8 : i64), ^bb13
        ^bb6(%85: i64):  // 2 preds: ^bb5, ^bb11
          %86 = llvm.icmp "slt" %85, %54 : i64
          llvm.cond_br %86, ^bb7(%8 : i64), ^bb12
        ^bb7(%87: i64):  // 2 preds: ^bb6, ^bb10
          %88 = llvm.icmp "slt" %87, %69 : i64
          llvm.cond_br %88, ^bb8(%8 : i64), ^bb11
        ^bb8(%89: i64):  // 2 preds: ^bb7, ^bb9
          %90 = llvm.icmp "slt" %89, %11 : i64
          llvm.cond_br %90, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %91 = llvm.add %83, %89  : i64
          %92 = llvm.add %26, %48  : i64
          %93 = llvm.add %92, %85  : i64
          %94 = llvm.mul %93, %10  : i64
          %95 = llvm.add %94, %91  : i64
          %96 = llvm.getelementptr %arg0[%95] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %97 = llvm.load %96 : !llvm.ptr<f32>
          %98 = llvm.add %31, %63  : i64
          %99 = llvm.add %98, %87  : i64
          %100 = llvm.mul %91, %3  : i64
          %101 = llvm.add %100, %99  : i64
          %102 = llvm.getelementptr %arg1[%101] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %103 = llvm.load %102 : !llvm.ptr<f32>
          %104 = llvm.mul %93, %3  : i64
          %105 = llvm.add %104, %99  : i64
          %106 = llvm.getelementptr %arg2[%105] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %107 = llvm.load %106 : !llvm.ptr<f32>
          %108 = llvm.fmul %97, %103  : f32
          %109 = llvm.fadd %107, %108  : f32
          llvm.store %109, %106 : !llvm.ptr<f32>
          %110 = llvm.add %89, %9  : i64
          llvm.br ^bb8(%110 : i64)
        ^bb10:  // pred: ^bb8
          %111 = llvm.add %87, %9  : i64
          llvm.br ^bb7(%111 : i64)
        ^bb11:  // pred: ^bb7
          %112 = llvm.add %85, %9  : i64
          llvm.br ^bb6(%112 : i64)
        ^bb12:  // pred: ^bb6
          %113 = llvm.add %83, %11  : i64
          llvm.br ^bb5(%113 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  %device = hal.ex.shared_device : !hal.device
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
  %c-1_i32 = arith.constant -1 : i32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c2 = arith.constant 2 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %device = hal.ex.shared_device : !hal.device
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @_device_query_0 : i1
  util.global.store %ok, @_device_query_0_ok : i1
  util.initializer.return
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.global.store %ok, @_device_query_0_ok : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(-1 : index) : i64
          %2 = llvm.mlir.constant(8 : index) : i64
          %3 = llvm.mlir.constant(1024 : index) : i64
          %4 = llvm.mlir.constant(-128 : index) : i64
          %5 = llvm.mlir.constant(128 : index) : i64
          %6 = llvm.mlir.constant(512 : index) : i64
          %7 = llvm.mlir.constant(-32 : index) : i64
          %8 = llvm.mlir.constant(0 : index) : i64
          %9 = llvm.mlir.constant(1 : index) : i64
          %10 = llvm.mlir.constant(256 : index) : i64
          %11 = llvm.mlir.constant(32 : index) : i64
          %12 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %13 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %8 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %17 = llvm.and %16, %0  : i64
          %18 = llvm.icmp "eq" %17, %8 : i64
          "llvm.intr.assume"(%18) : (i1) -> ()
          %19 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %20 = llvm.and %19, %0  : i64
          %21 = llvm.icmp "eq" %20, %8 : i64
          "llvm.intr.assume"(%21) : (i1) -> ()
          %22 = nvvm.read.ptx.sreg.ctaid.x : i32
          %23 = llvm.sext %22 : i32 to i64
          %24 = nvvm.read.ptx.sreg.ctaid.y : i32
          %25 = llvm.sext %24 : i32 to i64
          %26 = llvm.mul %25, %11  : i64
          %27 = llvm.mul %25, %7  : i64
          %28 = llvm.add %27, %6  : i64
          %29 = llvm.icmp "slt" %28, %11 : i64
          %30 = llvm.select %29, %28, %11 : i1, i64
          %31 = llvm.mul %23, %5  : i64
          %32 = llvm.mul %23, %4  : i64
          %33 = llvm.add %32, %3  : i64
          %34 = llvm.icmp "slt" %33, %5 : i64
          %35 = llvm.select %34, %33, %5 : i1, i64
          %36 = nvvm.read.ptx.sreg.tid.x : i32
          %37 = llvm.sext %36 : i32 to i64
          %38 = nvvm.read.ptx.sreg.tid.y : i32
          %39 = llvm.sext %38 : i32 to i64
          %40 = llvm.icmp "sle" %30, %8 : i64
          %41 = llvm.sub %8, %30  : i64
          %42 = llvm.sub %30, %9  : i64
          %43 = llvm.select %40, %41, %42 : i1, i64
          %44 = llvm.sdiv %43, %2  : i64
          %45 = llvm.sub %8, %44  : i64
          %46 = llvm.add %44, %9  : i64
          %47 = llvm.select %40, %45, %46 : i1, i64
          %48 = llvm.mul %39, %47  : i64
          %49 = llvm.mul %48, %1  : i64
          %50 = llvm.add %30, %49  : i64
          %51 = llvm.icmp "slt" %50, %47 : i64
          %52 = llvm.select %51, %50, %47 : i1, i64
          %53 = llvm.icmp "slt" %52, %8 : i64
          %54 = llvm.select %53, %8, %52 : i1, i64
          %55 = llvm.icmp "sle" %35, %8 : i64
          %56 = llvm.sub %8, %35  : i64
          %57 = llvm.sub %35, %9  : i64
          %58 = llvm.select %55, %56, %57 : i1, i64
          %59 = llvm.sdiv %58, %11  : i64
          %60 = llvm.sub %8, %59  : i64
          %61 = llvm.add %59, %9  : i64
          %62 = llvm.select %55, %60, %61 : i1, i64
          %63 = llvm.mul %37, %62  : i64
          %64 = llvm.mul %63, %1  : i64
          %65 = llvm.add %35, %64  : i64
          %66 = llvm.icmp "slt" %65, %62 : i64
          %67 = llvm.select %66, %65, %62 : i1, i64
          %68 = llvm.icmp "slt" %67, %8 : i64
          %69 = llvm.select %68, %8, %67 : i1, i64
          llvm.br ^bb1(%8 : i64)
        ^bb1(%70: i64):  // 2 preds: ^bb0, ^bb4
          %71 = llvm.icmp "slt" %70, %54 : i64
          llvm.cond_br %71, ^bb2(%8 : i64), ^bb5(%8 : i64)
        ^bb2(%72: i64):  // 2 preds: ^bb1, ^bb3
          %73 = llvm.icmp "slt" %72, %69 : i64
          llvm.cond_br %73, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %74 = llvm.add %26, %48  : i64
          %75 = llvm.add %74, %70  : i64
          %76 = llvm.add %31, %63  : i64
          %77 = llvm.add %76, %72  : i64
          %78 = llvm.mul %75, %3  : i64
          %79 = llvm.add %78, %77  : i64
          %80 = llvm.getelementptr %arg2[%79] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %12, %80 : !llvm.ptr<f32>
          %81 = llvm.add %72, %9  : i64
          llvm.br ^bb2(%81 : i64)
        ^bb4:  // pred: ^bb2
          %82 = llvm.add %70, %9  : i64
          llvm.br ^bb1(%82 : i64)
        ^bb5(%83: i64):  // 2 preds: ^bb1, ^bb12
          %84 = llvm.icmp "slt" %83, %10 : i64
          llvm.cond_br %84, ^bb6(%8 : i64), ^bb13
        ^bb6(%85: i64):  // 2 preds: ^bb5, ^bb11
          %86 = llvm.icmp "slt" %85, %54 : i64
          llvm.cond_br %86, ^bb7(%8 : i64), ^bb12
        ^bb7(%87: i64):  // 2 preds: ^bb6, ^bb10
          %88 = llvm.icmp "slt" %87, %69 : i64
          llvm.cond_br %88, ^bb8(%8 : i64), ^bb11
        ^bb8(%89: i64):  // 2 preds: ^bb7, ^bb9
          %90 = llvm.icmp "slt" %89, %11 : i64
          llvm.cond_br %90, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %91 = llvm.add %83, %89  : i64
          %92 = llvm.add %26, %48  : i64
          %93 = llvm.add %92, %85  : i64
          %94 = llvm.mul %93, %10  : i64
          %95 = llvm.add %94, %91  : i64
          %96 = llvm.getelementptr %arg0[%95] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %97 = llvm.load %96 : !llvm.ptr<f32>
          %98 = llvm.add %31, %63  : i64
          %99 = llvm.add %98, %87  : i64
          %100 = llvm.mul %91, %3  : i64
          %101 = llvm.add %100, %99  : i64
          %102 = llvm.getelementptr %arg1[%101] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %103 = llvm.load %102 : !llvm.ptr<f32>
          %104 = llvm.mul %93, %3  : i64
          %105 = llvm.add %104, %99  : i64
          %106 = llvm.getelementptr %arg2[%105] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %107 = llvm.load %106 : !llvm.ptr<f32>
          %108 = llvm.fmul %97, %103  : f32
          %109 = llvm.fadd %107, %108  : f32
          llvm.store %109, %106 : !llvm.ptr<f32>
          %110 = llvm.add %89, %9  : i64
          llvm.br ^bb8(%110 : i64)
        ^bb10:  // pred: ^bb8
          %111 = llvm.add %87, %9  : i64
          llvm.br ^bb7(%111 : i64)
        ^bb11:  // pred: ^bb7
          %112 = llvm.add %85, %9  : i64
          llvm.br ^bb6(%112 : i64)
        ^bb12:  // pred: ^bb6
          %113 = llvm.add %83, %11  : i64
          llvm.br ^bb5(%113 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1 = arith.constant 1 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c2097152 = arith.constant 2097152 : index
    %c1048576 = arith.constant 1048576 : index
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c16 = arith.constant 16 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(-1 : index) : i64
          %2 = llvm.mlir.constant(8 : index) : i64
          %3 = llvm.mlir.constant(1024 : index) : i64
          %4 = llvm.mlir.constant(-128 : index) : i64
          %5 = llvm.mlir.constant(128 : index) : i64
          %6 = llvm.mlir.constant(512 : index) : i64
          %7 = llvm.mlir.constant(-32 : index) : i64
          %8 = llvm.mlir.constant(0 : index) : i64
          %9 = llvm.mlir.constant(1 : index) : i64
          %10 = llvm.mlir.constant(256 : index) : i64
          %11 = llvm.mlir.constant(32 : index) : i64
          %12 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %13 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %8 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %17 = llvm.and %16, %0  : i64
          %18 = llvm.icmp "eq" %17, %8 : i64
          "llvm.intr.assume"(%18) : (i1) -> ()
          %19 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %20 = llvm.and %19, %0  : i64
          %21 = llvm.icmp "eq" %20, %8 : i64
          "llvm.intr.assume"(%21) : (i1) -> ()
          %22 = nvvm.read.ptx.sreg.ctaid.x : i32
          %23 = llvm.sext %22 : i32 to i64
          %24 = nvvm.read.ptx.sreg.ctaid.y : i32
          %25 = llvm.sext %24 : i32 to i64
          %26 = llvm.mul %25, %11  : i64
          %27 = llvm.mul %25, %7  : i64
          %28 = llvm.add %27, %6  : i64
          %29 = llvm.icmp "slt" %28, %11 : i64
          %30 = llvm.select %29, %28, %11 : i1, i64
          %31 = llvm.mul %23, %5  : i64
          %32 = llvm.mul %23, %4  : i64
          %33 = llvm.add %32, %3  : i64
          %34 = llvm.icmp "slt" %33, %5 : i64
          %35 = llvm.select %34, %33, %5 : i1, i64
          %36 = nvvm.read.ptx.sreg.tid.x : i32
          %37 = llvm.sext %36 : i32 to i64
          %38 = nvvm.read.ptx.sreg.tid.y : i32
          %39 = llvm.sext %38 : i32 to i64
          %40 = llvm.icmp "sle" %30, %8 : i64
          %41 = llvm.sub %8, %30  : i64
          %42 = llvm.sub %30, %9  : i64
          %43 = llvm.select %40, %41, %42 : i1, i64
          %44 = llvm.sdiv %43, %2  : i64
          %45 = llvm.sub %8, %44  : i64
          %46 = llvm.add %44, %9  : i64
          %47 = llvm.select %40, %45, %46 : i1, i64
          %48 = llvm.mul %39, %47  : i64
          %49 = llvm.mul %48, %1  : i64
          %50 = llvm.add %30, %49  : i64
          %51 = llvm.icmp "slt" %50, %47 : i64
          %52 = llvm.select %51, %50, %47 : i1, i64
          %53 = llvm.icmp "slt" %52, %8 : i64
          %54 = llvm.select %53, %8, %52 : i1, i64
          %55 = llvm.icmp "sle" %35, %8 : i64
          %56 = llvm.sub %8, %35  : i64
          %57 = llvm.sub %35, %9  : i64
          %58 = llvm.select %55, %56, %57 : i1, i64
          %59 = llvm.sdiv %58, %11  : i64
          %60 = llvm.sub %8, %59  : i64
          %61 = llvm.add %59, %9  : i64
          %62 = llvm.select %55, %60, %61 : i1, i64
          %63 = llvm.mul %37, %62  : i64
          %64 = llvm.mul %63, %1  : i64
          %65 = llvm.add %35, %64  : i64
          %66 = llvm.icmp "slt" %65, %62 : i64
          %67 = llvm.select %66, %65, %62 : i1, i64
          %68 = llvm.icmp "slt" %67, %8 : i64
          %69 = llvm.select %68, %8, %67 : i1, i64
          llvm.br ^bb1(%8 : i64)
        ^bb1(%70: i64):  // 2 preds: ^bb0, ^bb4
          %71 = llvm.icmp "slt" %70, %54 : i64
          llvm.cond_br %71, ^bb2(%8 : i64), ^bb5(%8 : i64)
        ^bb2(%72: i64):  // 2 preds: ^bb1, ^bb3
          %73 = llvm.icmp "slt" %72, %69 : i64
          llvm.cond_br %73, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %74 = llvm.add %26, %48  : i64
          %75 = llvm.add %74, %70  : i64
          %76 = llvm.add %31, %63  : i64
          %77 = llvm.add %76, %72  : i64
          %78 = llvm.mul %75, %3  : i64
          %79 = llvm.add %78, %77  : i64
          %80 = llvm.getelementptr %arg2[%79] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %12, %80 : !llvm.ptr<f32>
          %81 = llvm.add %72, %9  : i64
          llvm.br ^bb2(%81 : i64)
        ^bb4:  // pred: ^bb2
          %82 = llvm.add %70, %9  : i64
          llvm.br ^bb1(%82 : i64)
        ^bb5(%83: i64):  // 2 preds: ^bb1, ^bb12
          %84 = llvm.icmp "slt" %83, %10 : i64
          llvm.cond_br %84, ^bb6(%8 : i64), ^bb13
        ^bb6(%85: i64):  // 2 preds: ^bb5, ^bb11
          %86 = llvm.icmp "slt" %85, %54 : i64
          llvm.cond_br %86, ^bb7(%8 : i64), ^bb12
        ^bb7(%87: i64):  // 2 preds: ^bb6, ^bb10
          %88 = llvm.icmp "slt" %87, %69 : i64
          llvm.cond_br %88, ^bb8(%8 : i64), ^bb11
        ^bb8(%89: i64):  // 2 preds: ^bb7, ^bb9
          %90 = llvm.icmp "slt" %89, %11 : i64
          llvm.cond_br %90, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %91 = llvm.add %83, %89  : i64
          %92 = llvm.add %26, %48  : i64
          %93 = llvm.add %92, %85  : i64
          %94 = llvm.mul %93, %10  : i64
          %95 = llvm.add %94, %91  : i64
          %96 = llvm.getelementptr %arg0[%95] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %97 = llvm.load %96 : !llvm.ptr<f32>
          %98 = llvm.add %31, %63  : i64
          %99 = llvm.add %98, %87  : i64
          %100 = llvm.mul %91, %3  : i64
          %101 = llvm.add %100, %99  : i64
          %102 = llvm.getelementptr %arg1[%101] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %103 = llvm.load %102 : !llvm.ptr<f32>
          %104 = llvm.mul %93, %3  : i64
          %105 = llvm.add %104, %99  : i64
          %106 = llvm.getelementptr %arg2[%105] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %107 = llvm.load %106 : !llvm.ptr<f32>
          %108 = llvm.fmul %97, %103  : f32
          %109 = llvm.fadd %107, %108  : f32
          llvm.store %109, %106 : !llvm.ptr<f32>
          %110 = llvm.add %89, %9  : i64
          llvm.br ^bb8(%110 : i64)
        ^bb10:  // pred: ^bb8
          %111 = llvm.add %87, %9  : i64
          llvm.br ^bb7(%111 : i64)
        ^bb11:  // pred: ^bb7
          %112 = llvm.add %85, %9  : i64
          llvm.br ^bb6(%112 : i64)
        ^bb12:  // pred: ^bb6
          %113 = llvm.add %83, %11  : i64
          llvm.br ^bb5(%113 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1 = arith.constant 1 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c2097152 = arith.constant 2097152 : index
    %c1048576 = arith.constant 1048576 : index
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c16 = arith.constant 16 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(-1 : index) : i64
          %2 = llvm.mlir.constant(8 : index) : i64
          %3 = llvm.mlir.constant(1024 : index) : i64
          %4 = llvm.mlir.constant(-128 : index) : i64
          %5 = llvm.mlir.constant(128 : index) : i64
          %6 = llvm.mlir.constant(512 : index) : i64
          %7 = llvm.mlir.constant(-32 : index) : i64
          %8 = llvm.mlir.constant(0 : index) : i64
          %9 = llvm.mlir.constant(1 : index) : i64
          %10 = llvm.mlir.constant(256 : index) : i64
          %11 = llvm.mlir.constant(32 : index) : i64
          %12 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %13 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %8 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %17 = llvm.and %16, %0  : i64
          %18 = llvm.icmp "eq" %17, %8 : i64
          "llvm.intr.assume"(%18) : (i1) -> ()
          %19 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %20 = llvm.and %19, %0  : i64
          %21 = llvm.icmp "eq" %20, %8 : i64
          "llvm.intr.assume"(%21) : (i1) -> ()
          %22 = nvvm.read.ptx.sreg.ctaid.x : i32
          %23 = llvm.sext %22 : i32 to i64
          %24 = nvvm.read.ptx.sreg.ctaid.y : i32
          %25 = llvm.sext %24 : i32 to i64
          %26 = llvm.mul %25, %11  : i64
          %27 = llvm.mul %25, %7  : i64
          %28 = llvm.add %27, %6  : i64
          %29 = llvm.icmp "slt" %28, %11 : i64
          %30 = llvm.select %29, %28, %11 : i1, i64
          %31 = llvm.mul %23, %5  : i64
          %32 = llvm.mul %23, %4  : i64
          %33 = llvm.add %32, %3  : i64
          %34 = llvm.icmp "slt" %33, %5 : i64
          %35 = llvm.select %34, %33, %5 : i1, i64
          %36 = nvvm.read.ptx.sreg.tid.x : i32
          %37 = llvm.sext %36 : i32 to i64
          %38 = nvvm.read.ptx.sreg.tid.y : i32
          %39 = llvm.sext %38 : i32 to i64
          %40 = llvm.icmp "sle" %30, %8 : i64
          %41 = llvm.sub %8, %30  : i64
          %42 = llvm.sub %30, %9  : i64
          %43 = llvm.select %40, %41, %42 : i1, i64
          %44 = llvm.sdiv %43, %2  : i64
          %45 = llvm.sub %8, %44  : i64
          %46 = llvm.add %44, %9  : i64
          %47 = llvm.select %40, %45, %46 : i1, i64
          %48 = llvm.mul %39, %47  : i64
          %49 = llvm.mul %48, %1  : i64
          %50 = llvm.add %30, %49  : i64
          %51 = llvm.icmp "slt" %50, %47 : i64
          %52 = llvm.select %51, %50, %47 : i1, i64
          %53 = llvm.icmp "slt" %52, %8 : i64
          %54 = llvm.select %53, %8, %52 : i1, i64
          %55 = llvm.icmp "sle" %35, %8 : i64
          %56 = llvm.sub %8, %35  : i64
          %57 = llvm.sub %35, %9  : i64
          %58 = llvm.select %55, %56, %57 : i1, i64
          %59 = llvm.sdiv %58, %11  : i64
          %60 = llvm.sub %8, %59  : i64
          %61 = llvm.add %59, %9  : i64
          %62 = llvm.select %55, %60, %61 : i1, i64
          %63 = llvm.mul %37, %62  : i64
          %64 = llvm.mul %63, %1  : i64
          %65 = llvm.add %35, %64  : i64
          %66 = llvm.icmp "slt" %65, %62 : i64
          %67 = llvm.select %66, %65, %62 : i1, i64
          %68 = llvm.icmp "slt" %67, %8 : i64
          %69 = llvm.select %68, %8, %67 : i1, i64
          llvm.br ^bb1(%8 : i64)
        ^bb1(%70: i64):  // 2 preds: ^bb0, ^bb4
          %71 = llvm.icmp "slt" %70, %54 : i64
          llvm.cond_br %71, ^bb2(%8 : i64), ^bb5(%8 : i64)
        ^bb2(%72: i64):  // 2 preds: ^bb1, ^bb3
          %73 = llvm.icmp "slt" %72, %69 : i64
          llvm.cond_br %73, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %74 = llvm.add %26, %48  : i64
          %75 = llvm.add %74, %70  : i64
          %76 = llvm.add %31, %63  : i64
          %77 = llvm.add %76, %72  : i64
          %78 = llvm.mul %75, %3  : i64
          %79 = llvm.add %78, %77  : i64
          %80 = llvm.getelementptr %arg2[%79] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %12, %80 : !llvm.ptr<f32>
          %81 = llvm.add %72, %9  : i64
          llvm.br ^bb2(%81 : i64)
        ^bb4:  // pred: ^bb2
          %82 = llvm.add %70, %9  : i64
          llvm.br ^bb1(%82 : i64)
        ^bb5(%83: i64):  // 2 preds: ^bb1, ^bb12
          %84 = llvm.icmp "slt" %83, %10 : i64
          llvm.cond_br %84, ^bb6(%8 : i64), ^bb13
        ^bb6(%85: i64):  // 2 preds: ^bb5, ^bb11
          %86 = llvm.icmp "slt" %85, %54 : i64
          llvm.cond_br %86, ^bb7(%8 : i64), ^bb12
        ^bb7(%87: i64):  // 2 preds: ^bb6, ^bb10
          %88 = llvm.icmp "slt" %87, %69 : i64
          llvm.cond_br %88, ^bb8(%8 : i64), ^bb11
        ^bb8(%89: i64):  // 2 preds: ^bb7, ^bb9
          %90 = llvm.icmp "slt" %89, %11 : i64
          llvm.cond_br %90, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %91 = llvm.add %83, %89  : i64
          %92 = llvm.add %26, %48  : i64
          %93 = llvm.add %92, %85  : i64
          %94 = llvm.mul %93, %10  : i64
          %95 = llvm.add %94, %91  : i64
          %96 = llvm.getelementptr %arg0[%95] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %97 = llvm.load %96 : !llvm.ptr<f32>
          %98 = llvm.add %31, %63  : i64
          %99 = llvm.add %98, %87  : i64
          %100 = llvm.mul %91, %3  : i64
          %101 = llvm.add %100, %99  : i64
          %102 = llvm.getelementptr %arg1[%101] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %103 = llvm.load %102 : !llvm.ptr<f32>
          %104 = llvm.mul %93, %3  : i64
          %105 = llvm.add %104, %99  : i64
          %106 = llvm.getelementptr %arg2[%105] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %107 = llvm.load %106 : !llvm.ptr<f32>
          %108 = llvm.fmul %97, %103  : f32
          %109 = llvm.fadd %107, %108  : f32
          llvm.store %109, %106 : !llvm.ptr<f32>
          %110 = llvm.add %89, %9  : i64
          llvm.br ^bb8(%110 : i64)
        ^bb10:  // pred: ^bb8
          %111 = llvm.add %87, %9  : i64
          llvm.br ^bb7(%111 : i64)
        ^bb11:  // pred: ^bb7
          %112 = llvm.add %85, %9  : i64
          llvm.br ^bb6(%112 : i64)
        ^bb12:  // pred: ^bb6
          %113 = llvm.add %83, %11  : i64
          llvm.br ^bb5(%113 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1 = arith.constant 1 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c2097152 = arith.constant 2097152 : index
    %c1048576 = arith.constant 1048576 : index
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c16 = arith.constant 16 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  %device = hal.ex.shared_device : !hal.device
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @_device_query_0 : i1
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %device = hal.ex.shared_device : !hal.device
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c256 = arith.constant 256 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1 = arith.constant 1 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c2097152 = arith.constant 2097152 : index
  %c1048576 = arith.constant 1048576 : index
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c-1_i64 = arith.constant -1 : i64
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c16 = arith.constant 16 : index
  %c-1_i32 = arith.constant -1 : i32
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(-1 : index) : i64
          %2 = llvm.mlir.constant(8 : index) : i64
          %3 = llvm.mlir.constant(1024 : index) : i64
          %4 = llvm.mlir.constant(-128 : index) : i64
          %5 = llvm.mlir.constant(128 : index) : i64
          %6 = llvm.mlir.constant(512 : index) : i64
          %7 = llvm.mlir.constant(-32 : index) : i64
          %8 = llvm.mlir.constant(0 : index) : i64
          %9 = llvm.mlir.constant(1 : index) : i64
          %10 = llvm.mlir.constant(256 : index) : i64
          %11 = llvm.mlir.constant(32 : index) : i64
          %12 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %13 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %8 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %17 = llvm.and %16, %0  : i64
          %18 = llvm.icmp "eq" %17, %8 : i64
          "llvm.intr.assume"(%18) : (i1) -> ()
          %19 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %20 = llvm.and %19, %0  : i64
          %21 = llvm.icmp "eq" %20, %8 : i64
          "llvm.intr.assume"(%21) : (i1) -> ()
          %22 = nvvm.read.ptx.sreg.ctaid.x : i32
          %23 = llvm.sext %22 : i32 to i64
          %24 = nvvm.read.ptx.sreg.ctaid.y : i32
          %25 = llvm.sext %24 : i32 to i64
          %26 = llvm.mul %25, %11  : i64
          %27 = llvm.mul %25, %7  : i64
          %28 = llvm.add %27, %6  : i64
          %29 = llvm.icmp "slt" %28, %11 : i64
          %30 = llvm.select %29, %28, %11 : i1, i64
          %31 = llvm.mul %23, %5  : i64
          %32 = llvm.mul %23, %4  : i64
          %33 = llvm.add %32, %3  : i64
          %34 = llvm.icmp "slt" %33, %5 : i64
          %35 = llvm.select %34, %33, %5 : i1, i64
          %36 = nvvm.read.ptx.sreg.tid.x : i32
          %37 = llvm.sext %36 : i32 to i64
          %38 = nvvm.read.ptx.sreg.tid.y : i32
          %39 = llvm.sext %38 : i32 to i64
          %40 = llvm.icmp "sle" %30, %8 : i64
          %41 = llvm.sub %8, %30  : i64
          %42 = llvm.sub %30, %9  : i64
          %43 = llvm.select %40, %41, %42 : i1, i64
          %44 = llvm.sdiv %43, %2  : i64
          %45 = llvm.sub %8, %44  : i64
          %46 = llvm.add %44, %9  : i64
          %47 = llvm.select %40, %45, %46 : i1, i64
          %48 = llvm.mul %39, %47  : i64
          %49 = llvm.mul %48, %1  : i64
          %50 = llvm.add %30, %49  : i64
          %51 = llvm.icmp "slt" %50, %47 : i64
          %52 = llvm.select %51, %50, %47 : i1, i64
          %53 = llvm.icmp "slt" %52, %8 : i64
          %54 = llvm.select %53, %8, %52 : i1, i64
          %55 = llvm.icmp "sle" %35, %8 : i64
          %56 = llvm.sub %8, %35  : i64
          %57 = llvm.sub %35, %9  : i64
          %58 = llvm.select %55, %56, %57 : i1, i64
          %59 = llvm.sdiv %58, %11  : i64
          %60 = llvm.sub %8, %59  : i64
          %61 = llvm.add %59, %9  : i64
          %62 = llvm.select %55, %60, %61 : i1, i64
          %63 = llvm.mul %37, %62  : i64
          %64 = llvm.mul %63, %1  : i64
          %65 = llvm.add %35, %64  : i64
          %66 = llvm.icmp "slt" %65, %62 : i64
          %67 = llvm.select %66, %65, %62 : i1, i64
          %68 = llvm.icmp "slt" %67, %8 : i64
          %69 = llvm.select %68, %8, %67 : i1, i64
          llvm.br ^bb1(%8 : i64)
        ^bb1(%70: i64):  // 2 preds: ^bb0, ^bb4
          %71 = llvm.icmp "slt" %70, %54 : i64
          llvm.cond_br %71, ^bb2(%8 : i64), ^bb5(%8 : i64)
        ^bb2(%72: i64):  // 2 preds: ^bb1, ^bb3
          %73 = llvm.icmp "slt" %72, %69 : i64
          llvm.cond_br %73, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %74 = llvm.add %26, %48  : i64
          %75 = llvm.add %74, %70  : i64
          %76 = llvm.add %31, %63  : i64
          %77 = llvm.add %76, %72  : i64
          %78 = llvm.mul %75, %3  : i64
          %79 = llvm.add %78, %77  : i64
          %80 = llvm.getelementptr %arg2[%79] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %12, %80 : !llvm.ptr<f32>
          %81 = llvm.add %72, %9  : i64
          llvm.br ^bb2(%81 : i64)
        ^bb4:  // pred: ^bb2
          %82 = llvm.add %70, %9  : i64
          llvm.br ^bb1(%82 : i64)
        ^bb5(%83: i64):  // 2 preds: ^bb1, ^bb12
          %84 = llvm.icmp "slt" %83, %10 : i64
          llvm.cond_br %84, ^bb6(%8 : i64), ^bb13
        ^bb6(%85: i64):  // 2 preds: ^bb5, ^bb11
          %86 = llvm.icmp "slt" %85, %54 : i64
          llvm.cond_br %86, ^bb7(%8 : i64), ^bb12
        ^bb7(%87: i64):  // 2 preds: ^bb6, ^bb10
          %88 = llvm.icmp "slt" %87, %69 : i64
          llvm.cond_br %88, ^bb8(%8 : i64), ^bb11
        ^bb8(%89: i64):  // 2 preds: ^bb7, ^bb9
          %90 = llvm.icmp "slt" %89, %11 : i64
          llvm.cond_br %90, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %91 = llvm.add %83, %89  : i64
          %92 = llvm.add %26, %48  : i64
          %93 = llvm.add %92, %85  : i64
          %94 = llvm.mul %93, %10  : i64
          %95 = llvm.add %94, %91  : i64
          %96 = llvm.getelementptr %arg0[%95] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %97 = llvm.load %96 : !llvm.ptr<f32>
          %98 = llvm.add %31, %63  : i64
          %99 = llvm.add %98, %87  : i64
          %100 = llvm.mul %91, %3  : i64
          %101 = llvm.add %100, %99  : i64
          %102 = llvm.getelementptr %arg1[%101] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %103 = llvm.load %102 : !llvm.ptr<f32>
          %104 = llvm.mul %93, %3  : i64
          %105 = llvm.add %104, %99  : i64
          %106 = llvm.getelementptr %arg2[%105] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %107 = llvm.load %106 : !llvm.ptr<f32>
          %108 = llvm.fmul %97, %103  : f32
          %109 = llvm.fadd %107, %108  : f32
          llvm.store %109, %106 : !llvm.ptr<f32>
          %110 = llvm.add %89, %9  : i64
          llvm.br ^bb8(%110 : i64)
        ^bb10:  // pred: ^bb8
          %111 = llvm.add %87, %9  : i64
          llvm.br ^bb7(%111 : i64)
        ^bb11:  // pred: ^bb7
          %112 = llvm.add %85, %9  : i64
          llvm.br ^bb6(%112 : i64)
        ^bb12:  // pred: ^bb6
          %113 = llvm.add %83, %11  : i64
          llvm.br ^bb5(%113 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1 = arith.constant 1 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c2097152 = arith.constant 2097152 : index
    %c1048576 = arith.constant 1048576 : index
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c16 = arith.constant 16 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @_device_query_0 : i1
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  %device = hal.ex.shared_device : !hal.device
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %device = hal.ex.shared_device : !hal.device
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c256 = arith.constant 256 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1 = arith.constant 1 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c2097152 = arith.constant 2097152 : index
  %c1048576 = arith.constant 1048576 : index
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c-1_i64 = arith.constant -1 : i64
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c16 = arith.constant 16 : index
  %c-1_i32 = arith.constant -1 : i32
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After CombineInitializers (iree-util-combine-initializers) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUMatmulSimt>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %device_0 = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_1 = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_1 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device_2 = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_2 : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    cf.br ^bb4
  ^bb4:  // pred: ^bb3
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [32 : index, 8 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c16 = arith.constant 16 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c16, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @matmul_static_dispatch_0_matmul_512x1024x256(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(-1 : index) : i64
          %2 = llvm.mlir.constant(8 : index) : i64
          %3 = llvm.mlir.constant(1024 : index) : i64
          %4 = llvm.mlir.constant(-128 : index) : i64
          %5 = llvm.mlir.constant(128 : index) : i64
          %6 = llvm.mlir.constant(512 : index) : i64
          %7 = llvm.mlir.constant(-32 : index) : i64
          %8 = llvm.mlir.constant(0 : index) : i64
          %9 = llvm.mlir.constant(1 : index) : i64
          %10 = llvm.mlir.constant(256 : index) : i64
          %11 = llvm.mlir.constant(32 : index) : i64
          %12 = llvm.mlir.constant(0.000000e+00 : f32) : f32
          %13 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %8 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %17 = llvm.and %16, %0  : i64
          %18 = llvm.icmp "eq" %17, %8 : i64
          "llvm.intr.assume"(%18) : (i1) -> ()
          %19 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %20 = llvm.and %19, %0  : i64
          %21 = llvm.icmp "eq" %20, %8 : i64
          "llvm.intr.assume"(%21) : (i1) -> ()
          %22 = nvvm.read.ptx.sreg.ctaid.x : i32
          %23 = llvm.sext %22 : i32 to i64
          %24 = nvvm.read.ptx.sreg.ctaid.y : i32
          %25 = llvm.sext %24 : i32 to i64
          %26 = llvm.mul %25, %11  : i64
          %27 = llvm.mul %25, %7  : i64
          %28 = llvm.add %27, %6  : i64
          %29 = llvm.icmp "slt" %28, %11 : i64
          %30 = llvm.select %29, %28, %11 : i1, i64
          %31 = llvm.mul %23, %5  : i64
          %32 = llvm.mul %23, %4  : i64
          %33 = llvm.add %32, %3  : i64
          %34 = llvm.icmp "slt" %33, %5 : i64
          %35 = llvm.select %34, %33, %5 : i1, i64
          %36 = nvvm.read.ptx.sreg.tid.x : i32
          %37 = llvm.sext %36 : i32 to i64
          %38 = nvvm.read.ptx.sreg.tid.y : i32
          %39 = llvm.sext %38 : i32 to i64
          %40 = llvm.icmp "sle" %30, %8 : i64
          %41 = llvm.sub %8, %30  : i64
          %42 = llvm.sub %30, %9  : i64
          %43 = llvm.select %40, %41, %42 : i1, i64
          %44 = llvm.sdiv %43, %2  : i64
          %45 = llvm.sub %8, %44  : i64
          %46 = llvm.add %44, %9  : i64
          %47 = llvm.select %40, %45, %46 : i1, i64
          %48 = llvm.mul %39, %47  : i64
          %49 = llvm.mul %48, %1  : i64
          %50 = llvm.add %30, %49  : i64
          %51 = llvm.icmp "slt" %50, %47 : i64
          %52 = llvm.select %51, %50, %47 : i1, i64
          %53 = llvm.icmp "slt" %52, %8 : i64
          %54 = llvm.select %53, %8, %52 : i1, i64
          %55 = llvm.icmp "sle" %35, %8 : i64
          %56 = llvm.sub %8, %35  : i64
          %57 = llvm.sub %35, %9  : i64
          %58 = llvm.select %55, %56, %57 : i1, i64
          %59 = llvm.sdiv %58, %11  : i64
          %60 = llvm.sub %8, %59  : i64
          %61 = llvm.add %59, %9  : i64
          %62 = llvm.select %55, %60, %61 : i1, i64
          %63 = llvm.mul %37, %62  : i64
          %64 = llvm.mul %63, %1  : i64
          %65 = llvm.add %35, %64  : i64
          %66 = llvm.icmp "slt" %65, %62 : i64
          %67 = llvm.select %66, %65, %62 : i1, i64
          %68 = llvm.icmp "slt" %67, %8 : i64
          %69 = llvm.select %68, %8, %67 : i1, i64
          llvm.br ^bb1(%8 : i64)
        ^bb1(%70: i64):  // 2 preds: ^bb0, ^bb4
          %71 = llvm.icmp "slt" %70, %54 : i64
          llvm.cond_br %71, ^bb2(%8 : i64), ^bb5(%8 : i64)
        ^bb2(%72: i64):  // 2 preds: ^bb1, ^bb3
          %73 = llvm.icmp "slt" %72, %69 : i64
          llvm.cond_br %73, ^bb3, ^bb4
        ^bb3:  // pred: ^bb2
          %74 = llvm.add %26, %48  : i64
          %75 = llvm.add %74, %70  : i64
          %76 = llvm.add %31, %63  : i64
          %77 = llvm.add %76, %72  : i64
          %78 = llvm.mul %75, %3  : i64
          %79 = llvm.add %78, %77  : i64
          %80 = llvm.getelementptr %arg2[%79] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          llvm.store %12, %80 : !llvm.ptr<f32>
          %81 = llvm.add %72, %9  : i64
          llvm.br ^bb2(%81 : i64)
        ^bb4:  // pred: ^bb2
          %82 = llvm.add %70, %9  : i64
          llvm.br ^bb1(%82 : i64)
        ^bb5(%83: i64):  // 2 preds: ^bb1, ^bb12
          %84 = llvm.icmp "slt" %83, %10 : i64
          llvm.cond_br %84, ^bb6(%8 : i64), ^bb13
        ^bb6(%85: i64):  // 2 preds: ^bb5, ^bb11
          %86 = llvm.icmp "slt" %85, %54 : i64
          llvm.cond_br %86, ^bb7(%8 : i64), ^bb12
        ^bb7(%87: i64):  // 2 preds: ^bb6, ^bb10
          %88 = llvm.icmp "slt" %87, %69 : i64
          llvm.cond_br %88, ^bb8(%8 : i64), ^bb11
        ^bb8(%89: i64):  // 2 preds: ^bb7, ^bb9
          %90 = llvm.icmp "slt" %89, %11 : i64
          llvm.cond_br %90, ^bb9, ^bb10
        ^bb9:  // pred: ^bb8
          %91 = llvm.add %83, %89  : i64
          %92 = llvm.add %26, %48  : i64
          %93 = llvm.add %92, %85  : i64
          %94 = llvm.mul %93, %10  : i64
          %95 = llvm.add %94, %91  : i64
          %96 = llvm.getelementptr %arg0[%95] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %97 = llvm.load %96 : !llvm.ptr<f32>
          %98 = llvm.add %31, %63  : i64
          %99 = llvm.add %98, %87  : i64
          %100 = llvm.mul %91, %3  : i64
          %101 = llvm.add %100, %99  : i64
          %102 = llvm.getelementptr %arg1[%101] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %103 = llvm.load %102 : !llvm.ptr<f32>
          %104 = llvm.mul %93, %3  : i64
          %105 = llvm.add %104, %99  : i64
          %106 = llvm.getelementptr %arg2[%105] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %107 = llvm.load %106 : !llvm.ptr<f32>
          %108 = llvm.fmul %97, %103  : f32
          %109 = llvm.fadd %107, %108  : f32
          llvm.store %109, %106 : !llvm.ptr<f32>
          %110 = llvm.add %89, %9  : i64
          llvm.br ^bb8(%110 : i64)
        ^bb10:  // pred: ^bb8
          %111 = llvm.add %87, %9  : i64
          llvm.br ^bb7(%111 : i64)
        ^bb11:  // pred: ^bb7
          %112 = llvm.add %85, %9  : i64
          llvm.br ^bb6(%112 : i64)
        ^bb12:  // pred: ^bb6
          %113 = llvm.add %83, %11  : i64
          llvm.br ^bb5(%113 : i64)
        ^bb13:  // pred: ^bb5
          llvm.return
        }
      }
    }
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1 = arith.constant 1 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c2097152 = arith.constant 2097152 : index
    %c1048576 = arith.constant 1048576 : index
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c16 = arith.constant 16 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::SerializeTargetExecutablesPass (iree-hal-serialize-target-executables) //----- //
hal.executable private @matmul_static_dispatch_0 {
  hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<6012xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::SerializeExecutablesPass (iree-hal-serialize-executables) //----- //
hal.executable private @matmul_static_dispatch_0 {
  hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<6012xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %device_0 = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_1 = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_1 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device_2 = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_2 : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    cf.br ^bb4
  ^bb4:  // pred: ^bb3
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"0x080000004355444198E8FFFF1000000050000000440000005800000001000000040000002C0000006D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F35313278313032347832353600000000010000000000000001000000200000000800000001000000FA1600002F2F0A2F2F2047656E657261746564206279204C4C564D204E56505458204261636B2D456E640A2F2F0A0A2E76657273696F6E20362E300A2E74617267657420736D5F37300A2E616464726573735F73697A652036340A0A092F2F202E676C6F626C096D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235360A0A2E76697369626C65202E656E747279206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F353132783130323478323536280A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F302C0A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F312C0A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F320A290A2E6D61786E7469642033322C20382C20310A7B0A092E726567202E70726564200925703C32353E3B0A092E726567202E62313620092572733C353E3B0A092E726567202E623332200925723C31323E3B0A092E726567202E663332200925663C31323E3B0A092E726567202E62363420092572643C3134333E3B0A0A096C642E706172616D2E753634200925726435362C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F305D3B0A096C642E706172616D2E753634200925726435372C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F325D3B0A09637674612E746F2E676C6F62616C2E7536342009257264312C2025726435373B0A096C642E706172616D2E753634200925726435382C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F315D3B0A09637674612E746F2E676C6F62616C2E7536342009257264322C2025726435383B0A09637674612E746F2E676C6F62616C2E7536342009257264332C2025726435363B0A096D6F762E75333220092572322C202563746169642E783B0A096376742E7536342E7533322009257264342C202572323B0A096D6F762E75333220092572312C202563746169642E793B0A0973686C2E62333220092572332C202572312C20353B0A096376742E7536342E7533322009257264352C202572333B0A096D6F762E753634200925726435392C203531323B0A097375622E733634200925726436302C2025726435392C20257264353B0A096D696E2E733634200925726436312C2025726436302C2033323B0A096D756C2E776964652E7533322009257264362C202572322C203132383B0A096D6F762E753634200925726436322C20313032343B0A097375622E733634200925726436332C2025726436322C20257264363B0A096D696E2E733634200925726436342C2025726436332C203132383B0A096D6F762E75333220092572342C20257469642E783B0A096376742E7536342E753332200925726436352C202572343B0A096D6F762E75333220092572352C20257469642E793B0A096376742E7536342E753332200925726436362C202572353B0A09736574702E6C742E73363420092570312C2025726436312C20313B0A096E65672E733634200925726436372C2025726436313B0A096164642E733634200925726436382C2025726436312C202D313B0A0973656C702E623634200925726436392C2025726436372C2025726436382C202570313B0A096376742E7533322E75363420092572362C2025726436393B0A097368722E73333220092572372C202572362C2033313B0A097368722E75333220092572382C202572372C2032393B0A096164642E73333220092572392C202572362C202572383B0A097368722E7333322009257231302C202572392C20333B0A096376742E7336342E733332200925726437302C20257231303B0A096E65672E733634200925726437312C2025726437303B0A096164642E733634200925726437322C2025726437302C20313B0A0973656C702E623634200925726437332C2025726437312C2025726437322C202570313B0A096D756C2E6C6F2E7336342009257264372C2025726437332C2025726436363B0A097375622E733634200925726437342C2025726436312C20257264373B0A096D696E2E7336342009257264382C2025726437342C2025726437333B0A096D61782E7336342009257264392C20257264382C20303B0A09736574702E6C742E73363420092570322C2025726436342C20313B0A096E65672E733634200925726437352C2025726436343B0A096164642E733634200925726437362C2025726436342C202D313B0A0973656C702E623634200925726437372C2025726437352C2025726437362C202570323B0A097368722E733634200925726437382C2025726437372C2036333B0A097368722E753634200925726437392C2025726437382C2035393B0A096164642E733634200925726438302C2025726437372C2025726437393B0A097368722E733634200925726438312C2025726438302C20353B0A096E65672E733634200925726438322C2025726438313B0A096164642E733634200925726438332C2025726438312C20313B0A0973656C702E623634200925726438342C2025726438322C2025726438332C202570323B0A096D756C2E6C6F2E733634200925726431302C2025726438342C2025726436353B0A097375622E733634200925726438352C2025726436342C2025726431303B0A096D696E2E733634200925726431312C2025726438352C2025726438343B0A096D61782E733634200925726431322C2025726431312C20303B0A09736574702E6C742E73363420092570332C20257264382C20313B0A0973686C2E62363420092572643132342C20257264372C2031303B0A0973686C2E62363420092572643132352C2025726431302C20323B0A0973686C2E62363420092572643132362C20257264342C20393B0A09736574702E6C742E7336342009257032342C2025726431312C20313B0A0940257033206272612009244C5F5F4242305F31323B0A096D6F762E75363420092572643133322C20303B0A0973686C2E623634200925726431342C2025726431322C20323B0A09616E642E62363420200925726431352C20257264392C20313B0A09736574702E65712E73363420092570342C20257264392C20313B0A09736574702E65712E7336342009257032332C2025726431342C20303B0A096D6F762E7531362009257273342C20303B0A0940257034206272612009244C5F5F4242305F393B0A096272612E756E692009244C5F5F4242305F323B0A244C5F5F4242305F393A0A09736574702E65712E7336342009257031322C2025726431352C20303B0A096F722E70726564202009257031332C20257031322C20257032343B0A094025703133206272612009244C5F5F4242305F31323B0A0973686C2E6233322009257231312C202572312C2031353B0A096164642E733634200925726438382C202572643132342C2025726431303B0A096376742E7536342E753332200925726438392C20257231313B0A096164642E733634200925726439302C2025726438382C2025726438393B0A096164642E733634200925726439312C2025726439302C20257264363B0A0973686C2E623634200925726431332C2025726439312C20323B0A096164642E73363420092572643130342C2025726431332C202572643133323B0A096164642E733634200925726432372C20257264312C202572643130343B0A096D6F762E75363420092572643133332C20303B0A094025703233206272612009244C5F5F4242305F31323B0A244C5F5F4242305F31313A0A096164642E73363420092572643130362C2025726432372C202572643133333B0A0973742E676C6F62616C2E753820095B2572643130365D2C20257273343B0A096164642E73363420092572643133332C202572643133332C20313B0A09736574702E6C742E7536342009257031352C202572643133332C2025726431343B0A094025703135206272612009244C5F5F4242305F31313B0A244C5F5F4242305F31323A0A096164642E733634200925726433302C20257264372C20257264353B0A096164642E733634200925726433312C2025726431302C20257264363B0A096D756C2E776964652E75333220092572643130392C202572312C2033323736383B0A096164642E73363420092572643131302C202572643132342C202572643130393B0A096164642E73363420092572643131312C202572643131302C20257264333B0A096164642E73363420092572643133352C202572643131312C20343B0A096164642E73363420092572643131342C202572643132352C202572643132363B0A096164642E73363420092572643131352C202572643131342C20257264323B0A096164642E73363420092572643133342C202572643131352C20343039363B0A096D6F762E75363420092572643133362C20303B0A096272612E756E692009244C5F5F4242305F31333B0A244C5F5F4242305F32313A0A096164642E733634200925726435332C202572643133362C2033323B0A096164642E73363420092572643133352C202572643133352C203132383B0A096164642E73363420092572643133342C202572643133342C203133313037323B0A09736574702E6C742E7536342009257032312C202572643133362C203232343B0A096D6F762E75363420092572643133362C2025726435333B0A094025703231206272612009244C5F5F4242305F31333B0A096272612E756E692009244C5F5F4242305F32323B0A244C5F5F4242305F31333A0A0940257033206272612009244C5F5F4242305F32313B0A096D6F762E75363420092572643133382C20303B0A096D6F762E75363420092572643133372C202572643133353B0A096272612E756E692009244C5F5F4242305F31353B0A244C5F5F4242305F32303A0A096164642E73363420092572643133382C202572643133382C20313B0A096164642E73363420092572643133372C202572643133372C20313032343B0A09736574702E6E652E7336342009257032302C202572643133382C20257264393B0A094025703230206272612009244C5F5F4242305F31353B0A096272612E756E692009244C5F5F4242305F32313B0A244C5F5F4242305F31353A0A094025703234206272612009244C5F5F4242305F32303B0A096164642E73363420092572643131382C2025726433302C202572643133383B0A0973686C2E623634200925726434312C202572643131382C2031303B0A096D6F762E75363420092572643131372C20303B0A096D6F762E75363420092572643133392C202572643133343B0A096D6F762E75363420092572643134302C202572643131373B0A244C5F5F4242305F31373A0A096164642E73363420092572643132302C2025726433312C202572643134303B0A096164642E73363420092572643132312C202572643132302C2025726434313B0A0973686C2E62363420092572643132322C202572643132312C20323B0A096164642E733634200925726434342C20257264312C202572643132323B0A096C642E676C6F62616C2E6633322009256631312C205B25726434345D3B0A096D6F762E75363420092572643134312C202572643133393B0A096D6F762E75363420092572643134322C202572643131373B0A244C5F5F4242305F31383A0A096164642E73363420092572643132332C202572643133372C202572643134323B0A096C642E676C6F62616C2E66333220092566342C205B2572643132332B2D345D3B0A096C642E676C6F62616C2E66333220092566352C205B2572643134312B2D343039365D3B0A096D756C2E726E2E66333220092566362C202566342C202566353B0A096164642E726E2E66333220092566372C20256631312C202566363B0A0973742E676C6F62616C2E66333220095B25726434345D2C202566373B0A096C642E676C6F62616C2E66333220092566382C205B2572643132335D3B0A096C642E676C6F62616C2E66333220092566392C205B2572643134315D3B0A096D756C2E726E2E6633322009256631302C202566382C202566393B0A096164642E726E2E6633322009256631312C202566372C20256631303B0A0973742E676C6F62616C2E66333220095B25726434345D2C20256631313B0A096164642E73363420092572643134322C202572643134322C20383B0A096164642E73363420092572643134312C202572643134312C20383139323B0A09736574702E6E652E7336342009257031382C202572643134322C203132383B0A094025703138206272612009244C5F5F4242305F31383B0A096164642E73363420092572643134302C202572643134302C20313B0A096164642E73363420092572643133392C202572643133392C20343B0A09736574702E6E652E7336342009257031392C202572643134302C2025726431323B0A094025703139206272612009244C5F5F4242305F31373B0A096272612E756E692009244C5F5F4242305F32303B0A244C5F5F4242305F32323A0A097265743B0A244C5F5F4242305F323A0A09616E642E6236342020092572643132382C20257264392C20393232333337323033363835343737353830363B0A0973686C2E623634200925726439332C20257264372C2031323B0A096164642E733634200925726439352C2025726439332C202572643132353B0A096D756C2E776964652E753332200925726439362C202572312C203133313037323B0A096164642E733634200925726439372C2025726439352C2025726439363B0A096164642E733634200925726439392C2025726439372C202572643132363B0A096164642E733634200925726431372C20257264312C2025726439393B0A096D6F762E75363420092572643133322C20303B0A096272612E756E692009244C5F5F4242305F333B0A244C5F5F4242305F383A0A096164642E73363420092572643133322C202572643133322C20383139323B0A096164642E73363420092572643132382C202572643132382C202D323B0A09736574702E65712E7336342009257031302C202572643132382C20303B0A094025703130206272612009244C5F5F4242305F393B0A244C5F5F4242305F333A0A094025703234206272612009244C5F5F4242305F383B0A096164642E733634200925726432302C2025726431372C202572643133323B0A096D6F762E75363420092572643133302C20303B0A094025703233206272612009244C5F5F4242305F363B0A244C5F5F4242305F353A0A096164642E73363420092572643130312C2025726432302C202572643133303B0A0973742E676C6F62616C2E753820095B2572643130315D2C20257273343B0A096164642E73363420092572643133302C202572643133302C20313B0A09736574702E6C742E75363420092570372C202572643133302C2025726431343B0A0940257037206272612009244C5F5F4242305F353B0A244C5F5F4242305F363A0A096164642E733634200925726432332C2025726432302C20343039363B0A096D6F762E75363420092572643133312C20303B0A094025703233206272612009244C5F5F4242305F383B0A244C5F5F4242305F373A0A096164642E73363420092572643130332C2025726432332C202572643133313B0A0973742E676C6F62616C2E753820095B2572643130335D2C20257273343B0A096164642E73363420092572643133312C202572643133312C20313B0A09736574702E6C742E75363420092570392C202572643133312C2025726431343B0A0940257039206272612009244C5F5F4242305F373B0A096272612E756E692009244C5F5F4242305F383B0A0A7D0A00000C001400040008000C001000"> : vector<6012xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1 = arith.constant 1 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c2097152 = arith.constant 2097152 : index
    %c1048576 = arith.constant 1048576 : index
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c16 = arith.constant 16 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %device_0 = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_1 = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_1 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device_2 = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_2 : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<6012xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1 = arith.constant 1 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c2097152 = arith.constant 2097152 : index
    %c1048576 = arith.constant 1048576 : index
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c16 = arith.constant 16 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<6012xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1 = arith.constant 1 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c2097152 = arith.constant 2097152 : index
    %c1048576 = arith.constant 1048576 : index
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c16 = arith.constant 16 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
  %c256 = arith.constant 256 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1 = arith.constant 1 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c2097152 = arith.constant 2097152 : index
  %c1048576 = arith.constant 1048576 : index
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c-1_i64 = arith.constant -1 : i64
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c16 = arith.constant 16 : index
  %c-1_i32 = arith.constant -1 : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<6012xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<6012xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<6012xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<6012xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<6012xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c2 = arith.constant 2 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After Inliner (inline) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"0x080000004355444198E8FFFF1000000050000000440000005800000001000000040000002C0000006D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F35313278313032347832353600000000010000000000000001000000200000000800000001000000FA1600002F2F0A2F2F2047656E657261746564206279204C4C564D204E56505458204261636B2D456E640A2F2F0A0A2E76657273696F6E20362E300A2E74617267657420736D5F37300A2E616464726573735F73697A652036340A0A092F2F202E676C6F626C096D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235360A0A2E76697369626C65202E656E747279206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F353132783130323478323536280A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F302C0A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F312C0A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F320A290A2E6D61786E7469642033322C20382C20310A7B0A092E726567202E70726564200925703C32353E3B0A092E726567202E62313620092572733C353E3B0A092E726567202E623332200925723C31323E3B0A092E726567202E663332200925663C31323E3B0A092E726567202E62363420092572643C3134333E3B0A0A096C642E706172616D2E753634200925726435362C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F305D3B0A096C642E706172616D2E753634200925726435372C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F325D3B0A09637674612E746F2E676C6F62616C2E7536342009257264312C2025726435373B0A096C642E706172616D2E753634200925726435382C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F315D3B0A09637674612E746F2E676C6F62616C2E7536342009257264322C2025726435383B0A09637674612E746F2E676C6F62616C2E7536342009257264332C2025726435363B0A096D6F762E75333220092572322C202563746169642E783B0A096376742E7536342E7533322009257264342C202572323B0A096D6F762E75333220092572312C202563746169642E793B0A0973686C2E62333220092572332C202572312C20353B0A096376742E7536342E7533322009257264352C202572333B0A096D6F762E753634200925726435392C203531323B0A097375622E733634200925726436302C2025726435392C20257264353B0A096D696E2E733634200925726436312C2025726436302C2033323B0A096D756C2E776964652E7533322009257264362C202572322C203132383B0A096D6F762E753634200925726436322C20313032343B0A097375622E733634200925726436332C2025726436322C20257264363B0A096D696E2E733634200925726436342C2025726436332C203132383B0A096D6F762E75333220092572342C20257469642E783B0A096376742E7536342E753332200925726436352C202572343B0A096D6F762E75333220092572352C20257469642E793B0A096376742E7536342E753332200925726436362C202572353B0A09736574702E6C742E73363420092570312C2025726436312C20313B0A096E65672E733634200925726436372C2025726436313B0A096164642E733634200925726436382C2025726436312C202D313B0A0973656C702E623634200925726436392C2025726436372C2025726436382C202570313B0A096376742E7533322E75363420092572362C2025726436393B0A097368722E73333220092572372C202572362C2033313B0A097368722E75333220092572382C202572372C2032393B0A096164642E73333220092572392C202572362C202572383B0A097368722E7333322009257231302C202572392C20333B0A096376742E7336342E733332200925726437302C20257231303B0A096E65672E733634200925726437312C2025726437303B0A096164642E733634200925726437322C2025726437302C20313B0A0973656C702E623634200925726437332C2025726437312C2025726437322C202570313B0A096D756C2E6C6F2E7336342009257264372C2025726437332C2025726436363B0A097375622E733634200925726437342C2025726436312C20257264373B0A096D696E2E7336342009257264382C2025726437342C2025726437333B0A096D61782E7336342009257264392C20257264382C20303B0A09736574702E6C742E73363420092570322C2025726436342C20313B0A096E65672E733634200925726437352C2025726436343B0A096164642E733634200925726437362C2025726436342C202D313B0A0973656C702E623634200925726437372C2025726437352C2025726437362C202570323B0A097368722E733634200925726437382C2025726437372C2036333B0A097368722E753634200925726437392C2025726437382C2035393B0A096164642E733634200925726438302C2025726437372C2025726437393B0A097368722E733634200925726438312C2025726438302C20353B0A096E65672E733634200925726438322C2025726438313B0A096164642E733634200925726438332C2025726438312C20313B0A0973656C702E623634200925726438342C2025726438322C2025726438332C202570323B0A096D756C2E6C6F2E733634200925726431302C2025726438342C2025726436353B0A097375622E733634200925726438352C2025726436342C2025726431303B0A096D696E2E733634200925726431312C2025726438352C2025726438343B0A096D61782E733634200925726431322C2025726431312C20303B0A09736574702E6C742E73363420092570332C20257264382C20313B0A0973686C2E62363420092572643132342C20257264372C2031303B0A0973686C2E62363420092572643132352C2025726431302C20323B0A0973686C2E62363420092572643132362C20257264342C20393B0A09736574702E6C742E7336342009257032342C2025726431312C20313B0A0940257033206272612009244C5F5F4242305F31323B0A096D6F762E75363420092572643133322C20303B0A0973686C2E623634200925726431342C2025726431322C20323B0A09616E642E62363420200925726431352C20257264392C20313B0A09736574702E65712E73363420092570342C20257264392C20313B0A09736574702E65712E7336342009257032332C2025726431342C20303B0A096D6F762E7531362009257273342C20303B0A0940257034206272612009244C5F5F4242305F393B0A096272612E756E692009244C5F5F4242305F323B0A244C5F5F4242305F393A0A09736574702E65712E7336342009257031322C2025726431352C20303B0A096F722E70726564202009257031332C20257031322C20257032343B0A094025703133206272612009244C5F5F4242305F31323B0A0973686C2E6233322009257231312C202572312C2031353B0A096164642E733634200925726438382C202572643132342C2025726431303B0A096376742E7536342E753332200925726438392C20257231313B0A096164642E733634200925726439302C2025726438382C2025726438393B0A096164642E733634200925726439312C2025726439302C20257264363B0A0973686C2E623634200925726431332C2025726439312C20323B0A096164642E73363420092572643130342C2025726431332C202572643133323B0A096164642E733634200925726432372C20257264312C202572643130343B0A096D6F762E75363420092572643133332C20303B0A094025703233206272612009244C5F5F4242305F31323B0A244C5F5F4242305F31313A0A096164642E73363420092572643130362C2025726432372C202572643133333B0A0973742E676C6F62616C2E753820095B2572643130365D2C20257273343B0A096164642E73363420092572643133332C202572643133332C20313B0A09736574702E6C742E7536342009257031352C202572643133332C2025726431343B0A094025703135206272612009244C5F5F4242305F31313B0A244C5F5F4242305F31323A0A096164642E733634200925726433302C20257264372C20257264353B0A096164642E733634200925726433312C2025726431302C20257264363B0A096D756C2E776964652E75333220092572643130392C202572312C2033323736383B0A096164642E73363420092572643131302C202572643132342C202572643130393B0A096164642E73363420092572643131312C202572643131302C20257264333B0A096164642E73363420092572643133352C202572643131312C20343B0A096164642E73363420092572643131342C202572643132352C202572643132363B0A096164642E73363420092572643131352C202572643131342C20257264323B0A096164642E73363420092572643133342C202572643131352C20343039363B0A096D6F762E75363420092572643133362C20303B0A096272612E756E692009244C5F5F4242305F31333B0A244C5F5F4242305F32313A0A096164642E733634200925726435332C202572643133362C2033323B0A096164642E73363420092572643133352C202572643133352C203132383B0A096164642E73363420092572643133342C202572643133342C203133313037323B0A09736574702E6C742E7536342009257032312C202572643133362C203232343B0A096D6F762E75363420092572643133362C2025726435333B0A094025703231206272612009244C5F5F4242305F31333B0A096272612E756E692009244C5F5F4242305F32323B0A244C5F5F4242305F31333A0A0940257033206272612009244C5F5F4242305F32313B0A096D6F762E75363420092572643133382C20303B0A096D6F762E75363420092572643133372C202572643133353B0A096272612E756E692009244C5F5F4242305F31353B0A244C5F5F4242305F32303A0A096164642E73363420092572643133382C202572643133382C20313B0A096164642E73363420092572643133372C202572643133372C20313032343B0A09736574702E6E652E7336342009257032302C202572643133382C20257264393B0A094025703230206272612009244C5F5F4242305F31353B0A096272612E756E692009244C5F5F4242305F32313B0A244C5F5F4242305F31353A0A094025703234206272612009244C5F5F4242305F32303B0A096164642E73363420092572643131382C2025726433302C202572643133383B0A0973686C2E623634200925726434312C202572643131382C2031303B0A096D6F762E75363420092572643131372C20303B0A096D6F762E75363420092572643133392C202572643133343B0A096D6F762E75363420092572643134302C202572643131373B0A244C5F5F4242305F31373A0A096164642E73363420092572643132302C2025726433312C202572643134303B0A096164642E73363420092572643132312C202572643132302C2025726434313B0A0973686C2E62363420092572643132322C202572643132312C20323B0A096164642E733634200925726434342C20257264312C202572643132323B0A096C642E676C6F62616C2E6633322009256631312C205B25726434345D3B0A096D6F762E75363420092572643134312C202572643133393B0A096D6F762E75363420092572643134322C202572643131373B0A244C5F5F4242305F31383A0A096164642E73363420092572643132332C202572643133372C202572643134323B0A096C642E676C6F62616C2E66333220092566342C205B2572643132332B2D345D3B0A096C642E676C6F62616C2E66333220092566352C205B2572643134312B2D343039365D3B0A096D756C2E726E2E66333220092566362C202566342C202566353B0A096164642E726E2E66333220092566372C20256631312C202566363B0A0973742E676C6F62616C2E66333220095B25726434345D2C202566373B0A096C642E676C6F62616C2E66333220092566382C205B2572643132335D3B0A096C642E676C6F62616C2E66333220092566392C205B2572643134315D3B0A096D756C2E726E2E6633322009256631302C202566382C202566393B0A096164642E726E2E6633322009256631312C202566372C20256631303B0A0973742E676C6F62616C2E66333220095B25726434345D2C20256631313B0A096164642E73363420092572643134322C202572643134322C20383B0A096164642E73363420092572643134312C202572643134312C20383139323B0A09736574702E6E652E7336342009257031382C202572643134322C203132383B0A094025703138206272612009244C5F5F4242305F31383B0A096164642E73363420092572643134302C202572643134302C20313B0A096164642E73363420092572643133392C202572643133392C20343B0A09736574702E6E652E7336342009257031392C202572643134302C2025726431323B0A094025703139206272612009244C5F5F4242305F31373B0A096272612E756E692009244C5F5F4242305F32303B0A244C5F5F4242305F32323A0A097265743B0A244C5F5F4242305F323A0A09616E642E6236342020092572643132382C20257264392C20393232333337323033363835343737353830363B0A0973686C2E623634200925726439332C20257264372C2031323B0A096164642E733634200925726439352C2025726439332C202572643132353B0A096D756C2E776964652E753332200925726439362C202572312C203133313037323B0A096164642E733634200925726439372C2025726439352C2025726439363B0A096164642E733634200925726439392C2025726439372C202572643132363B0A096164642E733634200925726431372C20257264312C2025726439393B0A096D6F762E75363420092572643133322C20303B0A096272612E756E692009244C5F5F4242305F333B0A244C5F5F4242305F383A0A096164642E73363420092572643133322C202572643133322C20383139323B0A096164642E73363420092572643132382C202572643132382C202D323B0A09736574702E65712E7336342009257031302C202572643132382C20303B0A094025703130206272612009244C5F5F4242305F393B0A244C5F5F4242305F333A0A094025703234206272612009244C5F5F4242305F383B0A096164642E733634200925726432302C2025726431372C202572643133323B0A096D6F762E75363420092572643133302C20303B0A094025703233206272612009244C5F5F4242305F363B0A244C5F5F4242305F353A0A096164642E73363420092572643130312C2025726432302C202572643133303B0A0973742E676C6F62616C2E753820095B2572643130315D2C20257273343B0A096164642E73363420092572643133302C202572643133302C20313B0A09736574702E6C742E75363420092570372C202572643133302C2025726431343B0A0940257037206272612009244C5F5F4242305F353B0A244C5F5F4242305F363A0A096164642E733634200925726432332C2025726432302C20343039363B0A096D6F762E75363420092572643133312C20303B0A094025703233206272612009244C5F5F4242305F383B0A244C5F5F4242305F373A0A096164642E73363420092572643130332C2025726432332C202572643133313B0A0973742E676C6F62616C2E753820095B2572643130335D2C20257273343B0A096164642E73363420092572643133312C202572643133312C20313B0A09736574702E6C742E75363420092570392C202572643133312C2025726431343B0A0940257039206272612009244C5F5F4242305F373B0A096272612E756E692009244C5F5F4242305F383B0A0A7D0A00000C001400040008000C001000"> : vector<6012xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"0x080000004355444198E8FFFF1000000050000000440000005800000001000000040000002C0000006D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F35313278313032347832353600000000010000000000000001000000200000000800000001000000FA1600002F2F0A2F2F2047656E657261746564206279204C4C564D204E56505458204261636B2D456E640A2F2F0A0A2E76657273696F6E20362E300A2E74617267657420736D5F37300A2E616464726573735F73697A652036340A0A092F2F202E676C6F626C096D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235360A0A2E76697369626C65202E656E747279206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F353132783130323478323536280A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F302C0A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F312C0A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F320A290A2E6D61786E7469642033322C20382C20310A7B0A092E726567202E70726564200925703C32353E3B0A092E726567202E62313620092572733C353E3B0A092E726567202E623332200925723C31323E3B0A092E726567202E663332200925663C31323E3B0A092E726567202E62363420092572643C3134333E3B0A0A096C642E706172616D2E753634200925726435362C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F305D3B0A096C642E706172616D2E753634200925726435372C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F325D3B0A09637674612E746F2E676C6F62616C2E7536342009257264312C2025726435373B0A096C642E706172616D2E753634200925726435382C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F315D3B0A09637674612E746F2E676C6F62616C2E7536342009257264322C2025726435383B0A09637674612E746F2E676C6F62616C2E7536342009257264332C2025726435363B0A096D6F762E75333220092572322C202563746169642E783B0A096376742E7536342E7533322009257264342C202572323B0A096D6F762E75333220092572312C202563746169642E793B0A0973686C2E62333220092572332C202572312C20353B0A096376742E7536342E7533322009257264352C202572333B0A096D6F762E753634200925726435392C203531323B0A097375622E733634200925726436302C2025726435392C20257264353B0A096D696E2E733634200925726436312C2025726436302C2033323B0A096D756C2E776964652E7533322009257264362C202572322C203132383B0A096D6F762E753634200925726436322C20313032343B0A097375622E733634200925726436332C2025726436322C20257264363B0A096D696E2E733634200925726436342C2025726436332C203132383B0A096D6F762E75333220092572342C20257469642E783B0A096376742E7536342E753332200925726436352C202572343B0A096D6F762E75333220092572352C20257469642E793B0A096376742E7536342E753332200925726436362C202572353B0A09736574702E6C742E73363420092570312C2025726436312C20313B0A096E65672E733634200925726436372C2025726436313B0A096164642E733634200925726436382C2025726436312C202D313B0A0973656C702E623634200925726436392C2025726436372C2025726436382C202570313B0A096376742E7533322E75363420092572362C2025726436393B0A097368722E73333220092572372C202572362C2033313B0A097368722E75333220092572382C202572372C2032393B0A096164642E73333220092572392C202572362C202572383B0A097368722E7333322009257231302C202572392C20333B0A096376742E7336342E733332200925726437302C20257231303B0A096E65672E733634200925726437312C2025726437303B0A096164642E733634200925726437322C2025726437302C20313B0A0973656C702E623634200925726437332C2025726437312C2025726437322C202570313B0A096D756C2E6C6F2E7336342009257264372C2025726437332C2025726436363B0A097375622E733634200925726437342C2025726436312C20257264373B0A096D696E2E7336342009257264382C2025726437342C2025726437333B0A096D61782E7336342009257264392C20257264382C20303B0A09736574702E6C742E73363420092570322C2025726436342C20313B0A096E65672E733634200925726437352C2025726436343B0A096164642E733634200925726437362C2025726436342C202D313B0A0973656C702E623634200925726437372C2025726437352C2025726437362C202570323B0A097368722E733634200925726437382C2025726437372C2036333B0A097368722E753634200925726437392C2025726437382C2035393B0A096164642E733634200925726438302C2025726437372C2025726437393B0A097368722E733634200925726438312C2025726438302C20353B0A096E65672E733634200925726438322C2025726438313B0A096164642E733634200925726438332C2025726438312C20313B0A0973656C702E623634200925726438342C2025726438322C2025726438332C202570323B0A096D756C2E6C6F2E733634200925726431302C2025726438342C2025726436353B0A097375622E733634200925726438352C2025726436342C2025726431303B0A096D696E2E733634200925726431312C2025726438352C2025726438343B0A096D61782E733634200925726431322C2025726431312C20303B0A09736574702E6C742E73363420092570332C20257264382C20313B0A0973686C2E62363420092572643132342C20257264372C2031303B0A0973686C2E62363420092572643132352C2025726431302C20323B0A0973686C2E62363420092572643132362C20257264342C20393B0A09736574702E6C742E7336342009257032342C2025726431312C20313B0A0940257033206272612009244C5F5F4242305F31323B0A096D6F762E75363420092572643133322C20303B0A0973686C2E623634200925726431342C2025726431322C20323B0A09616E642E62363420200925726431352C20257264392C20313B0A09736574702E65712E73363420092570342C20257264392C20313B0A09736574702E65712E7336342009257032332C2025726431342C20303B0A096D6F762E7531362009257273342C20303B0A0940257034206272612009244C5F5F4242305F393B0A096272612E756E692009244C5F5F4242305F323B0A244C5F5F4242305F393A0A09736574702E65712E7336342009257031322C2025726431352C20303B0A096F722E70726564202009257031332C20257031322C20257032343B0A094025703133206272612009244C5F5F4242305F31323B0A0973686C2E6233322009257231312C202572312C2031353B0A096164642E733634200925726438382C202572643132342C2025726431303B0A096376742E7536342E753332200925726438392C20257231313B0A096164642E733634200925726439302C2025726438382C2025726438393B0A096164642E733634200925726439312C2025726439302C20257264363B0A0973686C2E623634200925726431332C2025726439312C20323B0A096164642E73363420092572643130342C2025726431332C202572643133323B0A096164642E733634200925726432372C20257264312C202572643130343B0A096D6F762E75363420092572643133332C20303B0A094025703233206272612009244C5F5F4242305F31323B0A244C5F5F4242305F31313A0A096164642E73363420092572643130362C2025726432372C202572643133333B0A0973742E676C6F62616C2E753820095B2572643130365D2C20257273343B0A096164642E73363420092572643133332C202572643133332C20313B0A09736574702E6C742E7536342009257031352C202572643133332C2025726431343B0A094025703135206272612009244C5F5F4242305F31313B0A244C5F5F4242305F31323A0A096164642E733634200925726433302C20257264372C20257264353B0A096164642E733634200925726433312C2025726431302C20257264363B0A096D756C2E776964652E75333220092572643130392C202572312C2033323736383B0A096164642E73363420092572643131302C202572643132342C202572643130393B0A096164642E73363420092572643131312C202572643131302C20257264333B0A096164642E73363420092572643133352C202572643131312C20343B0A096164642E73363420092572643131342C202572643132352C202572643132363B0A096164642E73363420092572643131352C202572643131342C20257264323B0A096164642E73363420092572643133342C202572643131352C20343039363B0A096D6F762E75363420092572643133362C20303B0A096272612E756E692009244C5F5F4242305F31333B0A244C5F5F4242305F32313A0A096164642E733634200925726435332C202572643133362C2033323B0A096164642E73363420092572643133352C202572643133352C203132383B0A096164642E73363420092572643133342C202572643133342C203133313037323B0A09736574702E6C742E7536342009257032312C202572643133362C203232343B0A096D6F762E75363420092572643133362C2025726435333B0A094025703231206272612009244C5F5F4242305F31333B0A096272612E756E692009244C5F5F4242305F32323B0A244C5F5F4242305F31333A0A0940257033206272612009244C5F5F4242305F32313B0A096D6F762E75363420092572643133382C20303B0A096D6F762E75363420092572643133372C202572643133353B0A096272612E756E692009244C5F5F4242305F31353B0A244C5F5F4242305F32303A0A096164642E73363420092572643133382C202572643133382C20313B0A096164642E73363420092572643133372C202572643133372C20313032343B0A09736574702E6E652E7336342009257032302C202572643133382C20257264393B0A094025703230206272612009244C5F5F4242305F31353B0A096272612E756E692009244C5F5F4242305F32313B0A244C5F5F4242305F31353A0A094025703234206272612009244C5F5F4242305F32303B0A096164642E73363420092572643131382C2025726433302C202572643133383B0A0973686C2E623634200925726434312C202572643131382C2031303B0A096D6F762E75363420092572643131372C20303B0A096D6F762E75363420092572643133392C202572643133343B0A096D6F762E75363420092572643134302C202572643131373B0A244C5F5F4242305F31373A0A096164642E73363420092572643132302C2025726433312C202572643134303B0A096164642E73363420092572643132312C202572643132302C2025726434313B0A0973686C2E62363420092572643132322C202572643132312C20323B0A096164642E733634200925726434342C20257264312C202572643132323B0A096C642E676C6F62616C2E6633322009256631312C205B25726434345D3B0A096D6F762E75363420092572643134312C202572643133393B0A096D6F762E75363420092572643134322C202572643131373B0A244C5F5F4242305F31383A0A096164642E73363420092572643132332C202572643133372C202572643134323B0A096C642E676C6F62616C2E66333220092566342C205B2572643132332B2D345D3B0A096C642E676C6F62616C2E66333220092566352C205B2572643134312B2D343039365D3B0A096D756C2E726E2E66333220092566362C202566342C202566353B0A096164642E726E2E66333220092566372C20256631312C202566363B0A0973742E676C6F62616C2E66333220095B25726434345D2C202566373B0A096C642E676C6F62616C2E66333220092566382C205B2572643132335D3B0A096C642E676C6F62616C2E66333220092566392C205B2572643134315D3B0A096D756C2E726E2E6633322009256631302C202566382C202566393B0A096164642E726E2E6633322009256631312C202566372C20256631303B0A0973742E676C6F62616C2E66333220095B25726434345D2C20256631313B0A096164642E73363420092572643134322C202572643134322C20383B0A096164642E73363420092572643134312C202572643134312C20383139323B0A09736574702E6E652E7336342009257031382C202572643134322C203132383B0A094025703138206272612009244C5F5F4242305F31383B0A096164642E73363420092572643134302C202572643134302C20313B0A096164642E73363420092572643133392C202572643133392C20343B0A09736574702E6E652E7336342009257031392C202572643134302C2025726431323B0A094025703139206272612009244C5F5F4242305F31373B0A096272612E756E692009244C5F5F4242305F32303B0A244C5F5F4242305F32323A0A097265743B0A244C5F5F4242305F323A0A09616E642E6236342020092572643132382C20257264392C20393232333337323033363835343737353830363B0A0973686C2E623634200925726439332C20257264372C2031323B0A096164642E733634200925726439352C2025726439332C202572643132353B0A096D756C2E776964652E753332200925726439362C202572312C203133313037323B0A096164642E733634200925726439372C2025726439352C2025726439363B0A096164642E733634200925726439392C2025726439372C202572643132363B0A096164642E733634200925726431372C20257264312C2025726439393B0A096D6F762E75363420092572643133322C20303B0A096272612E756E692009244C5F5F4242305F333B0A244C5F5F4242305F383A0A096164642E73363420092572643133322C202572643133322C20383139323B0A096164642E73363420092572643132382C202572643132382C202D323B0A09736574702E65712E7336342009257031302C202572643132382C20303B0A094025703130206272612009244C5F5F4242305F393B0A244C5F5F4242305F333A0A094025703234206272612009244C5F5F4242305F383B0A096164642E733634200925726432302C2025726431372C202572643133323B0A096D6F762E75363420092572643133302C20303B0A094025703233206272612009244C5F5F4242305F363B0A244C5F5F4242305F353A0A096164642E73363420092572643130312C2025726432302C202572643133303B0A0973742E676C6F62616C2E753820095B2572643130315D2C20257273343B0A096164642E73363420092572643133302C202572643133302C20313B0A09736574702E6C742E75363420092570372C202572643133302C2025726431343B0A0940257037206272612009244C5F5F4242305F353B0A244C5F5F4242305F363A0A096164642E733634200925726432332C2025726432302C20343039363B0A096D6F762E75363420092572643133312C20303B0A094025703233206272612009244C5F5F4242305F383B0A244C5F5F4242305F373A0A096164642E73363420092572643130332C2025726432332C202572643133313B0A0973742E676C6F62616C2E753820095B2572643130335D2C20257273343B0A096164642E73363420092572643133312C202572643133312C20313B0A09736574702E6C742E75363420092570392C202572643133312C2025726431343B0A0940257039206272612009244C5F5F4242305F373B0A096272612E756E692009244C5F5F4242305F383B0A0A7D0A00000C001400040008000C001000"> : vector<6012xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After SCFForLoopCanonicalization (scf-for-loop-canonicalization) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SCFForLoopCanonicalization (scf-for-loop-canonicalization) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c2 = arith.constant 2 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After LoopCoalescing (affine-loop-coalescing) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c2 = arith.constant 2 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c2 = arith.constant 2 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c2 = arith.constant 2 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ArithUnsignedWhenEquivalent (arith-unsigned-when-equivalent) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c2 = arith.constant 2 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ArithUnsignedWhenEquivalent (arith-unsigned-when-equivalent) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c2 = arith.constant 2 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After PropagateSubranges (iree-util-propagate-subranges) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<6012xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<6012xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<6012xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c1048576 = arith.constant 1048576 : index
    %c2097152 = arith.constant 2097152 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c1 = arith.constant 1 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
  %c-1_i32 = arith.constant -1 : i32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c2 = arith.constant 2 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c1048576 = arith.constant 1048576 : index
  %c2097152 = arith.constant 2097152 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c1 = arith.constant 1 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<6012xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1 = arith.constant 1 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c2097152 = arith.constant 2097152 : index
    %c1048576 = arith.constant 1048576 : index
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c16 = arith.constant 16 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<6012xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1 = arith.constant 1 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c2097152 = arith.constant 2097152 : index
    %c1048576 = arith.constant 1048576 : index
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c16 = arith.constant 16 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_matmul_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@matmul_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_matmul_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<6012xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @matmul_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1 = arith.constant 1 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c2097152 = arith.constant 2097152 : index
    %c1048576 = arith.constant 1048576 : index
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c16 = arith.constant 16 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_matmul_static_dispatch_0 = util.global.load @_executable_matmul_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c256, %c1024]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c1048576) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_1 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2097152}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c1048576], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c2097152]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_matmul_static_dispatch_0 : !hal.executable)[0] workgroups([%c8, %c16, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_1 : !hal.buffer)[%c0, %c2097152] shape([%c512, %c1024]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::ConversionPass (iree-vm-conversion) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
    vm.initializer {
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %buffer = vm.rodata.inline "_utf8_hal_executable_format_EAB228F999C2D3A1" {alignment = 1 : i64} : !vm.buffer = "hal.executable.format"
      %buffer_0 = vm.rodata.inline "_utf8_cuda_nvptx_fb_B15B42B96FDBACC" {alignment = 1 : i64} : !vm.buffer = "cuda-nvptx-fb"
      %0:2 = vm.call @hal.device.query.i64(%ref, %buffer, %buffer_0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %c1 = vm.const.i32 1
      %2 = vm.and.i32 %1, %c1 : i32
      %zero = vm.const.i32.zero
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %c1_1 = vm.const.i32 1
      %zero_2 = vm.const.i32.zero
      %zero_3 = vm.const.i32.zero
      %c7 = vm.const.i32 7
      %c1_4 = vm.const.i32 1
      %c1_5 = vm.const.i32 1
      %c7_6 = vm.const.i32 7
      %c1_7 = vm.const.i32 1
      %c2 = vm.const.i32 2
      %c7_8 = vm.const.i32 7
      %zero_9 = vm.const.i32.zero
      %ref_10 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_2, [(%zero_3, %c7, %c1_4), (%c1_5, %c7_6, %c1_7), (%c2, %c7_8, %zero_9)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %zero_11 = vm.const.i32.zero
      %ref_12 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_11, [%ref_10]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_12, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %buffer_13 = vm.rodata.inline "_utf8_cuda_nvptx_fb_B15B42B96FDBACC" {alignment = 1 : i64} : !vm.buffer = "cuda-nvptx-fb"
      %null = vm.const.ref.zero : !vm.buffer
      %ref_14 = vm.call.variadic @hal.executable.create(%ref, %buffer_13, %matmul_static_dispatch_0_cuda_nvptx_fb, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_14 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      %null_15 = vm.const.ref.zero : !vm.ref<!hal.executable>
      vm.br ^bb3(%null_15 : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
    vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
    vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c256 = vm.const.i64 256
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c1_0 = vm.const.i64 1
      %c1024 = vm.const.i64 1024
      %c512 = vm.const.i64 512
      %c2097152 = vm.const.i64 2097152
      %c1048576 = vm.const.i64 1048576
      %c524288 = vm.const.i64 524288
      %zero = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c2 = vm.const.i64 2
      %c8 = vm.const.i64 8
      %c16 = vm.const.i64 16
      %c-1_1 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      %buffer = vm.rodata.inline "_utf8_tensor_3C6209B4FD120BDC" {alignment = 1 : i64} : !vm.buffer = "tensor"
      vm.call.variadic @hal.buffer_view.assert(%arg0, %buffer, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_2 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_3 = vm.call @hal.device.allocator(%ref_2) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %buffer_4 = vm.rodata.inline "_utf8_tensor_3C6209B4FD120BDC" {alignment = 1 : i64} : !vm.buffer = "tensor"
      %c16_5 = vm.const.i32 16
      %c3075 = vm.const.i32 3075
      vm.call @hal.buffer.assert(%ref, %buffer_4, %ref_3, %c524288, %c16_5, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %buffer_6 = vm.rodata.inline "_utf8_tensor_3C6209B4FD120BDC" {alignment = 1 : i64} : !vm.buffer = "tensor"
      vm.call.variadic @hal.buffer_view.assert(%arg1, %buffer_6, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %buffer_8 = vm.rodata.inline "_utf8_tensor_3C6209B4FD120BDC" {alignment = 1 : i64} : !vm.buffer = "tensor"
      %c16_9 = vm.const.i32 16
      %c3075_10 = vm.const.i32 3075
      vm.call @hal.buffer.assert(%ref_7, %buffer_8, %ref_3, %c1048576, %c16_9, %c3075_10) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %c50 = vm.const.i32 50
      %c150998019 = vm.const.i32 150998019
      %ref_11 = vm.call @hal.allocator.allocate(%ref_3, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %c17 = vm.const.i32 17
      %c3 = vm.const.i32 3
      %zero_12 = vm.const.i32.zero
      %ref_13 = vm.call @hal.command_buffer.create(%ref_2, %c17, %c3, %zero_12) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %zero_14 = vm.const.i32.zero
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_13, %_pipeline_layout_0, %zero, [(%zero, %zero_14, %ref, %zero, %c524288), (%c1_0, %zero_14, %ref_7, %zero, %c1048576), (%c2, %zero_14, %ref_11, %zero, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      %zero_15 = vm.const.i32.zero
      %c8_16 = vm.const.i32 8
      %c16_17 = vm.const.i32 16
      %c1_18 = vm.const.i32 1
      vm.call @hal.command_buffer.dispatch(%ref_13, %_executable_matmul_static_dispatch_0, %zero_15, %c8_16, %c16_17, %c1_18) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      %c28 = vm.const.i32 28
      %c13 = vm.const.i32 13
      %zero_19 = vm.const.i32.zero
      vm.call @hal.command_buffer.execution_barrier(%ref_13, %c28, %c13, %zero_19) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_13) : (!vm.ref<!hal.command_buffer>) -> ()
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %zero_20 = vm.const.i32.zero
      %ref_21 = vm.call @hal.fence.create(%ref_2, %zero_20) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_2, %c-1, %null, %ref_21, [%ref_13]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_21]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_fail %0, "failed to wait on timepoint"
      %ref_22 = vm.call.variadic @hal.buffer_view.create(%ref_11, %zero, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_22 : !vm.ref<!hal.buffer_view>
    ^bb2:  // pred: ^bb0
      %c2_23 = vm.const.i32 2
      vm.fail %c2_23, "device not supported in the compiled configuration"
    }
    vm.export @matmul_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::HoistInlinedRodataPass (iree-vm-hoist-inlined-rodata) //----- //
vm.module public @module {
  vm.global.i32 private @_device_query_0 : i32
  vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC_0 {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.initializer {
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %c1 = vm.const.i32 1
    %2 = vm.and.i32 %1, %c1 : i32
    %zero = vm.const.i32.zero
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %c1_0 = vm.const.i32 1
    %zero_1 = vm.const.i32.zero
    %zero_2 = vm.const.i32.zero
    %c7 = vm.const.i32 7
    %c1_3 = vm.const.i32 1
    %c1_4 = vm.const.i32 1
    %c7_5 = vm.const.i32 7
    %c1_6 = vm.const.i32 1
    %c2 = vm.const.i32 2
    %c7_7 = vm.const.i32 7
    %zero_8 = vm.const.i32.zero
    %ref_9 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_1, [(%zero_2, %c7, %c1_3), (%c1_4, %c7_5, %c1_6), (%c2, %c7_7, %zero_8)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %zero_10 = vm.const.i32.zero
    %ref_11 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_10, [%ref_9]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_11, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC_0 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC_0 : !vm.buffer
    %null = vm.const.ref.zero : !vm.buffer
    %ref_12 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC_0, %matmul_static_dispatch_0_cuda_nvptx_fb, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_12 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    %null_13 = vm.const.ref.zero : !vm.ref<!hal.executable>
    vm.br ^bb3(%null_13 : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
  vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
  vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
  vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
  vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC_1 {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC_2 {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC_3 {alignment = 1 : i64} "tensor"
  vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c256 = vm.const.i64 256
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c1_0 = vm.const.i64 1
    %c1024 = vm.const.i64 1024
    %c512 = vm.const.i64 512
    %c2097152 = vm.const.i64 2097152
    %c1048576 = vm.const.i64 1048576
    %c524288 = vm.const.i64 524288
    %zero = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c2 = vm.const.i64 2
    %c8 = vm.const.i64 8
    %c16 = vm.const.i64 16
    %c-1_1 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_2 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_3 = vm.call @hal.device.allocator(%ref_2) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC_1 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC_1 : !vm.buffer
    %c16_4 = vm.const.i32 16
    %c3075 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC_1, %ref_3, %c524288, %c16_4, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_tensor_3C6209B4FD120BDC_2 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC_2 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC_2, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_5 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_3C6209B4FD120BDC_3 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC_3 : !vm.buffer
    %c16_6 = vm.const.i32 16
    %c3075_7 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref_5, %_utf8_tensor_3C6209B4FD120BDC_3, %ref_3, %c1048576, %c16_6, %c3075_7) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %c50 = vm.const.i32 50
    %c150998019 = vm.const.i32 150998019
    %ref_8 = vm.call @hal.allocator.allocate(%ref_3, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %c17 = vm.const.i32 17
    %c3 = vm.const.i32 3
    %zero_9 = vm.const.i32.zero
    %ref_10 = vm.call @hal.command_buffer.create(%ref_2, %c17, %c3, %zero_9) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %zero_11 = vm.const.i32.zero
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_10, %_pipeline_layout_0, %zero, [(%zero, %zero_11, %ref, %zero, %c524288), (%c1_0, %zero_11, %ref_5, %zero, %c1048576), (%c2, %zero_11, %ref_8, %zero, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    %zero_12 = vm.const.i32.zero
    %c8_13 = vm.const.i32 8
    %c16_14 = vm.const.i32 16
    %c1_15 = vm.const.i32 1
    vm.call @hal.command_buffer.dispatch(%ref_10, %_executable_matmul_static_dispatch_0, %zero_12, %c8_13, %c16_14, %c1_15) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    %c28 = vm.const.i32 28
    %c13 = vm.const.i32 13
    %zero_16 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_10, %c28, %c13, %zero_16) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_10) : (!vm.ref<!hal.command_buffer>) -> ()
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %zero_17 = vm.const.i32.zero
    %ref_18 = vm.call @hal.fence.create(%ref_2, %zero_17) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_2, %c-1, %null, %ref_18, [%ref_10]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_18]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %0, "failed to wait on timepoint"
    %ref_19 = vm.call.variadic @hal.buffer_view.create(%ref_8, %zero, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_19 : !vm.ref<!hal.buffer_view>
  ^bb2:  // pred: ^bb0
    %c2_20 = vm.const.i32 2
    vm.fail %c2_20, "device not supported in the compiled configuration"
  }
  vm.export @matmul_static attributes {iree.abi.stub}
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::DeduplicateRodataPass (iree-vm-deduplicate-rodata) //----- //
vm.module public @module {
  vm.global.i32 private @_device_query_0 : i32
  vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"0x080000004355444198E8FFFF1000000050000000440000005800000001000000040000002C0000006D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F35313278313032347832353600000000010000000000000001000000200000000800000001000000FA1600002F2F0A2F2F2047656E657261746564206279204C4C564D204E56505458204261636B2D456E640A2F2F0A0A2E76657273696F6E20362E300A2E74617267657420736D5F37300A2E616464726573735F73697A652036340A0A092F2F202E676C6F626C096D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235360A0A2E76697369626C65202E656E747279206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F353132783130323478323536280A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F302C0A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F312C0A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F320A290A2E6D61786E7469642033322C20382C20310A7B0A092E726567202E70726564200925703C32353E3B0A092E726567202E62313620092572733C353E3B0A092E726567202E623332200925723C31323E3B0A092E726567202E663332200925663C31323E3B0A092E726567202E62363420092572643C3134333E3B0A0A096C642E706172616D2E753634200925726435362C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F305D3B0A096C642E706172616D2E753634200925726435372C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F325D3B0A09637674612E746F2E676C6F62616C2E7536342009257264312C2025726435373B0A096C642E706172616D2E753634200925726435382C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F315D3B0A09637674612E746F2E676C6F62616C2E7536342009257264322C2025726435383B0A09637674612E746F2E676C6F62616C2E7536342009257264332C2025726435363B0A096D6F762E75333220092572322C202563746169642E783B0A096376742E7536342E7533322009257264342C202572323B0A096D6F762E75333220092572312C202563746169642E793B0A0973686C2E62333220092572332C202572312C20353B0A096376742E7536342E7533322009257264352C202572333B0A096D6F762E753634200925726435392C203531323B0A097375622E733634200925726436302C2025726435392C20257264353B0A096D696E2E733634200925726436312C2025726436302C2033323B0A096D756C2E776964652E7533322009257264362C202572322C203132383B0A096D6F762E753634200925726436322C20313032343B0A097375622E733634200925726436332C2025726436322C20257264363B0A096D696E2E733634200925726436342C2025726436332C203132383B0A096D6F762E75333220092572342C20257469642E783B0A096376742E7536342E753332200925726436352C202572343B0A096D6F762E75333220092572352C20257469642E793B0A096376742E7536342E753332200925726436362C202572353B0A09736574702E6C742E73363420092570312C2025726436312C20313B0A096E65672E733634200925726436372C2025726436313B0A096164642E733634200925726436382C2025726436312C202D313B0A0973656C702E623634200925726436392C2025726436372C2025726436382C202570313B0A096376742E7533322E75363420092572362C2025726436393B0A097368722E73333220092572372C202572362C2033313B0A097368722E75333220092572382C202572372C2032393B0A096164642E73333220092572392C202572362C202572383B0A097368722E7333322009257231302C202572392C20333B0A096376742E7336342E733332200925726437302C20257231303B0A096E65672E733634200925726437312C2025726437303B0A096164642E733634200925726437322C2025726437302C20313B0A0973656C702E623634200925726437332C2025726437312C2025726437322C202570313B0A096D756C2E6C6F2E7336342009257264372C2025726437332C2025726436363B0A097375622E733634200925726437342C2025726436312C20257264373B0A096D696E2E7336342009257264382C2025726437342C2025726437333B0A096D61782E7336342009257264392C20257264382C20303B0A09736574702E6C742E73363420092570322C2025726436342C20313B0A096E65672E733634200925726437352C2025726436343B0A096164642E733634200925726437362C2025726436342C202D313B0A0973656C702E623634200925726437372C2025726437352C2025726437362C202570323B0A097368722E733634200925726437382C2025726437372C2036333B0A097368722E753634200925726437392C2025726437382C2035393B0A096164642E733634200925726438302C2025726437372C2025726437393B0A097368722E733634200925726438312C2025726438302C20353B0A096E65672E733634200925726438322C2025726438313B0A096164642E733634200925726438332C2025726438312C20313B0A0973656C702E623634200925726438342C2025726438322C2025726438332C202570323B0A096D756C2E6C6F2E733634200925726431302C2025726438342C2025726436353B0A097375622E733634200925726438352C2025726436342C2025726431303B0A096D696E2E733634200925726431312C2025726438352C2025726438343B0A096D61782E733634200925726431322C2025726431312C20303B0A09736574702E6C742E73363420092570332C20257264382C20313B0A0973686C2E62363420092572643132342C20257264372C2031303B0A0973686C2E62363420092572643132352C2025726431302C20323B0A0973686C2E62363420092572643132362C20257264342C20393B0A09736574702E6C742E7336342009257032342C2025726431312C20313B0A0940257033206272612009244C5F5F4242305F31323B0A096D6F762E75363420092572643133322C20303B0A0973686C2E623634200925726431342C2025726431322C20323B0A09616E642E62363420200925726431352C20257264392C20313B0A09736574702E65712E73363420092570342C20257264392C20313B0A09736574702E65712E7336342009257032332C2025726431342C20303B0A096D6F762E7531362009257273342C20303B0A0940257034206272612009244C5F5F4242305F393B0A096272612E756E692009244C5F5F4242305F323B0A244C5F5F4242305F393A0A09736574702E65712E7336342009257031322C2025726431352C20303B0A096F722E70726564202009257031332C20257031322C20257032343B0A094025703133206272612009244C5F5F4242305F31323B0A0973686C2E6233322009257231312C202572312C2031353B0A096164642E733634200925726438382C202572643132342C2025726431303B0A096376742E7536342E753332200925726438392C20257231313B0A096164642E733634200925726439302C2025726438382C2025726438393B0A096164642E733634200925726439312C2025726439302C20257264363B0A0973686C2E623634200925726431332C2025726439312C20323B0A096164642E73363420092572643130342C2025726431332C202572643133323B0A096164642E733634200925726432372C20257264312C202572643130343B0A096D6F762E75363420092572643133332C20303B0A094025703233206272612009244C5F5F4242305F31323B0A244C5F5F4242305F31313A0A096164642E73363420092572643130362C2025726432372C202572643133333B0A0973742E676C6F62616C2E753820095B2572643130365D2C20257273343B0A096164642E73363420092572643133332C202572643133332C20313B0A09736574702E6C742E7536342009257031352C202572643133332C2025726431343B0A094025703135206272612009244C5F5F4242305F31313B0A244C5F5F4242305F31323A0A096164642E733634200925726433302C20257264372C20257264353B0A096164642E733634200925726433312C2025726431302C20257264363B0A096D756C2E776964652E75333220092572643130392C202572312C2033323736383B0A096164642E73363420092572643131302C202572643132342C202572643130393B0A096164642E73363420092572643131312C202572643131302C20257264333B0A096164642E73363420092572643133352C202572643131312C20343B0A096164642E73363420092572643131342C202572643132352C202572643132363B0A096164642E73363420092572643131352C202572643131342C20257264323B0A096164642E73363420092572643133342C202572643131352C20343039363B0A096D6F762E75363420092572643133362C20303B0A096272612E756E692009244C5F5F4242305F31333B0A244C5F5F4242305F32313A0A096164642E733634200925726435332C202572643133362C2033323B0A096164642E73363420092572643133352C202572643133352C203132383B0A096164642E73363420092572643133342C202572643133342C203133313037323B0A09736574702E6C742E7536342009257032312C202572643133362C203232343B0A096D6F762E75363420092572643133362C2025726435333B0A094025703231206272612009244C5F5F4242305F31333B0A096272612E756E692009244C5F5F4242305F32323B0A244C5F5F4242305F31333A0A0940257033206272612009244C5F5F4242305F32313B0A096D6F762E75363420092572643133382C20303B0A096D6F762E75363420092572643133372C202572643133353B0A096272612E756E692009244C5F5F4242305F31353B0A244C5F5F4242305F32303A0A096164642E73363420092572643133382C202572643133382C20313B0A096164642E73363420092572643133372C202572643133372C20313032343B0A09736574702E6E652E7336342009257032302C202572643133382C20257264393B0A094025703230206272612009244C5F5F4242305F31353B0A096272612E756E692009244C5F5F4242305F32313B0A244C5F5F4242305F31353A0A094025703234206272612009244C5F5F4242305F32303B0A096164642E73363420092572643131382C2025726433302C202572643133383B0A0973686C2E623634200925726434312C202572643131382C2031303B0A096D6F762E75363420092572643131372C20303B0A096D6F762E75363420092572643133392C202572643133343B0A096D6F762E75363420092572643134302C202572643131373B0A244C5F5F4242305F31373A0A096164642E73363420092572643132302C2025726433312C202572643134303B0A096164642E73363420092572643132312C202572643132302C2025726434313B0A0973686C2E62363420092572643132322C202572643132312C20323B0A096164642E733634200925726434342C20257264312C202572643132323B0A096C642E676C6F62616C2E6633322009256631312C205B25726434345D3B0A096D6F762E75363420092572643134312C202572643133393B0A096D6F762E75363420092572643134322C202572643131373B0A244C5F5F4242305F31383A0A096164642E73363420092572643132332C202572643133372C202572643134323B0A096C642E676C6F62616C2E66333220092566342C205B2572643132332B2D345D3B0A096C642E676C6F62616C2E66333220092566352C205B2572643134312B2D343039365D3B0A096D756C2E726E2E66333220092566362C202566342C202566353B0A096164642E726E2E66333220092566372C20256631312C202566363B0A0973742E676C6F62616C2E66333220095B25726434345D2C202566373B0A096C642E676C6F62616C2E66333220092566382C205B2572643132335D3B0A096C642E676C6F62616C2E66333220092566392C205B2572643134315D3B0A096D756C2E726E2E6633322009256631302C202566382C202566393B0A096164642E726E2E6633322009256631312C202566372C20256631303B0A0973742E676C6F62616C2E66333220095B25726434345D2C20256631313B0A096164642E73363420092572643134322C202572643134322C20383B0A096164642E73363420092572643134312C202572643134312C20383139323B0A09736574702E6E652E7336342009257031382C202572643134322C203132383B0A094025703138206272612009244C5F5F4242305F31383B0A096164642E73363420092572643134302C202572643134302C20313B0A096164642E73363420092572643133392C202572643133392C20343B0A09736574702E6E652E7336342009257031392C202572643134302C2025726431323B0A094025703139206272612009244C5F5F4242305F31373B0A096272612E756E692009244C5F5F4242305F32303B0A244C5F5F4242305F32323A0A097265743B0A244C5F5F4242305F323A0A09616E642E6236342020092572643132382C20257264392C20393232333337323033363835343737353830363B0A0973686C2E623634200925726439332C20257264372C2031323B0A096164642E733634200925726439352C2025726439332C202572643132353B0A096D756C2E776964652E753332200925726439362C202572312C203133313037323B0A096164642E733634200925726439372C2025726439352C2025726439363B0A096164642E733634200925726439392C2025726439372C202572643132363B0A096164642E733634200925726431372C20257264312C2025726439393B0A096D6F762E75363420092572643133322C20303B0A096272612E756E692009244C5F5F4242305F333B0A244C5F5F4242305F383A0A096164642E73363420092572643133322C202572643133322C20383139323B0A096164642E73363420092572643132382C202572643132382C202D323B0A09736574702E65712E7336342009257031302C202572643132382C20303B0A094025703130206272612009244C5F5F4242305F393B0A244C5F5F4242305F333A0A094025703234206272612009244C5F5F4242305F383B0A096164642E733634200925726432302C2025726431372C202572643133323B0A096D6F762E75363420092572643133302C20303B0A094025703233206272612009244C5F5F4242305F363B0A244C5F5F4242305F353A0A096164642E73363420092572643130312C2025726432302C202572643133303B0A0973742E676C6F62616C2E753820095B2572643130315D2C20257273343B0A096164642E73363420092572643133302C202572643133302C20313B0A09736574702E6C742E75363420092570372C202572643133302C2025726431343B0A0940257037206272612009244C5F5F4242305F353B0A244C5F5F4242305F363A0A096164642E733634200925726432332C2025726432302C20343039363B0A096D6F762E75363420092572643133312C20303B0A094025703233206272612009244C5F5F4242305F383B0A244C5F5F4242305F373A0A096164642E73363420092572643130332C2025726432332C202572643133313B0A0973742E676C6F62616C2E753820095B2572643130335D2C20257273343B0A096164642E73363420092572643133312C202572643133312C20313B0A09736574702E6C742E75363420092570392C202572643133312C2025726431343B0A0940257039206272612009244C5F5F4242305F373B0A096272612E756E692009244C5F5F4242305F383B0A0A7D0A00000C001400040008000C001000"> : vector<6012xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.initializer {
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %c1 = vm.const.i32 1
    %2 = vm.and.i32 %1, %c1 : i32
    %zero = vm.const.i32.zero
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %c1_0 = vm.const.i32 1
    %zero_1 = vm.const.i32.zero
    %zero_2 = vm.const.i32.zero
    %c7 = vm.const.i32 7
    %c1_3 = vm.const.i32 1
    %c1_4 = vm.const.i32 1
    %c7_5 = vm.const.i32 7
    %c1_6 = vm.const.i32 1
    %c2 = vm.const.i32 2
    %c7_7 = vm.const.i32 7
    %zero_8 = vm.const.i32.zero
    %ref_9 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_1, [(%zero_2, %c7, %c1_3), (%c1_4, %c7_5, %c1_6), (%c2, %c7_7, %zero_8)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %zero_10 = vm.const.i32.zero
    %ref_11 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_10, [%ref_9]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_11, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC_12 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %null = vm.const.ref.zero : !vm.buffer
    %ref_13 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC_12, %matmul_static_dispatch_0_cuda_nvptx_fb, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_13 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    %null_14 = vm.const.ref.zero : !vm.ref<!hal.executable>
    vm.br ^bb3(%null_14 : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
  vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
  vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
  vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
  vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c256 = vm.const.i64 256
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c1_0 = vm.const.i64 1
    %c1024 = vm.const.i64 1024
    %c512 = vm.const.i64 512
    %c2097152 = vm.const.i64 2097152
    %c1048576 = vm.const.i64 1048576
    %c524288 = vm.const.i64 524288
    %zero = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c2 = vm.const.i64 2
    %c8 = vm.const.i64 8
    %c16 = vm.const.i64 16
    %c-1_1 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_2 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_3 = vm.call @hal.device.allocator(%ref_2) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC_4 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    %c16_5 = vm.const.i32 16
    %c3075 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC_4, %ref_3, %c524288, %c16_5, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_tensor_3C6209B4FD120BDC_6 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC_6, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_7 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_3C6209B4FD120BDC_8 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    %c16_9 = vm.const.i32 16
    %c3075_10 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC_8, %ref_3, %c1048576, %c16_9, %c3075_10) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %c50 = vm.const.i32 50
    %c150998019 = vm.const.i32 150998019
    %ref_11 = vm.call @hal.allocator.allocate(%ref_3, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %c17 = vm.const.i32 17
    %c3 = vm.const.i32 3
    %zero_12 = vm.const.i32.zero
    %ref_13 = vm.call @hal.command_buffer.create(%ref_2, %c17, %c3, %zero_12) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %zero_14 = vm.const.i32.zero
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_13, %_pipeline_layout_0, %zero, [(%zero, %zero_14, %ref, %zero, %c524288), (%c1_0, %zero_14, %ref_7, %zero, %c1048576), (%c2, %zero_14, %ref_11, %zero, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    %zero_15 = vm.const.i32.zero
    %c8_16 = vm.const.i32 8
    %c16_17 = vm.const.i32 16
    %c1_18 = vm.const.i32 1
    vm.call @hal.command_buffer.dispatch(%ref_13, %_executable_matmul_static_dispatch_0, %zero_15, %c8_16, %c16_17, %c1_18) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    %c28 = vm.const.i32 28
    %c13 = vm.const.i32 13
    %zero_19 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_13, %c28, %c13, %zero_19) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_13) : (!vm.ref<!hal.command_buffer>) -> ()
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %zero_20 = vm.const.i32.zero
    %ref_21 = vm.call @hal.fence.create(%ref_2, %zero_20) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_2, %c-1, %null, %ref_21, [%ref_13]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_21]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %0, "failed to wait on timepoint"
    %ref_22 = vm.call.variadic @hal.buffer_view.create(%ref_11, %zero, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_22 : !vm.ref<!hal.buffer_view>
  ^bb2:  // pred: ^bb0
    %c2_23 = vm.const.i32 2
    vm.fail %c2_23, "device not supported in the compiled configuration"
  }
  vm.export @matmul_static attributes {iree.abi.stub}
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC_3 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC_3, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
    vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
    vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c8 = vm.const.i32 8
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c1_0 = vm.const.i64 1
      %c1024 = vm.const.i64 1024
      %c512 = vm.const.i64 512
      %c2097152 = vm.const.i64 2097152
      %c1048576 = vm.const.i64 1048576
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c2_2 = vm.const.i64 2
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC_6 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC_6, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_tensor_3C6209B4FD120BDC_7 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC_7, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_3C6209B4FD120BDC_9 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC_9, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_10 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_11 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_8, %zero_1, %c1048576), (%c2_2, %zero, %ref_10, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_12 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3(%0 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_13 : !vm.ref<!hal.buffer_view>
    ^bb3(%1: i32):  // pred: ^bb1
      vm.fail %1, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @matmul_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
    vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
    vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c8 = vm.const.i32 8
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c1_0 = vm.const.i64 1
      %c1024 = vm.const.i64 1024
      %c512 = vm.const.i64 512
      %c2097152 = vm.const.i64 2097152
      %c1048576 = vm.const.i64 1048576
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c2_2 = vm.const.i64 2
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3(%0 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_10 : !vm.ref<!hal.buffer_view>
    ^bb3(%1: i32):  // pred: ^bb1
      vm.fail %1, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @matmul_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
    vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
    vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c8 = vm.const.i32 8
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c1_0 = vm.const.i64 1
      %c1024 = vm.const.i64 1024
      %c512 = vm.const.i64 512
      %c2097152 = vm.const.i64 2097152
      %c1048576 = vm.const.i64 1048576
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c2_2 = vm.const.i64 2
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_10 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @matmul_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
    vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
    vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c8 = vm.const.i32 8
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c1_0 = vm.const.i64 1
      %c1024 = vm.const.i64 1024
      %c512 = vm.const.i64 512
      %c2097152 = vm.const.i64 2097152
      %c1048576 = vm.const.i64 1048576
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c2_2 = vm.const.i64 2
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_10 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @matmul_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
    vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
    vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c8 = vm.const.i32 8
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c1_0 = vm.const.i64 1
      %c1024 = vm.const.i64 1024
      %c512 = vm.const.i64 512
      %c2097152 = vm.const.i64 2097152
      %c1048576 = vm.const.i64 1048576
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c2_2 = vm.const.i64 2
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_10 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @matmul_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::ResolveRodataLoadsPass (iree-vm-resolve-rodata-loads) //----- //
vm.module public @module {
  vm.global.i32 private @_device_query_0 : i32
  vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.initializer {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
  vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
  vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
  vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
  vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c8 = vm.const.i32 8
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c1_0 = vm.const.i64 1
    %c1024 = vm.const.i64 1024
    %c512 = vm.const.i64 512
    %c2097152 = vm.const.i64 2097152
    %c1048576 = vm.const.i64 1048576
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c2_2 = vm.const.i64 2
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_10 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @matmul_static attributes {iree.abi.stub}
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.initializer {
  %null = vm.const.ref.zero : !vm.ref<!hal.executable>
  %null_0 = vm.const.ref.zero : !vm.buffer
  %c2 = vm.const.i32 2
  %c7 = vm.const.i32 7
  %zero = vm.const.i32.zero
  %c1 = vm.const.i32 1
  %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
  %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
  %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
  %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
  %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
  %2 = vm.and.i32 %1, %c1 : i32
  %3 = vm.select.i32 %0#0, %2, %zero : i32
  %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
  %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
  vm.global.store.i32 %3, @_device_query_0 : i32
  vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.cond_br %3, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
  %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
  vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
^bb2:  // pred: ^bb0
  vm.br ^bb3(%null : !vm.ref<!hal.executable>)
^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
  vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
  %c2 = vm.const.i32 2
  %null = vm.const.ref.zero : !vm.ref<!hal.fence>
  %c13 = vm.const.i32 13
  %c28 = vm.const.i32 28
  %c8 = vm.const.i32 8
  %zero = vm.const.i32.zero
  %c3 = vm.const.i32 3
  %c17 = vm.const.i32 17
  %c150998019 = vm.const.i32 150998019
  %c50 = vm.const.i32 50
  %c3075 = vm.const.i32 3075
  %c16 = vm.const.i32 16
  %c256 = vm.const.i64 256
  %c1 = vm.const.i32 1
  %c553648160 = vm.const.i32 553648160
  %c1_0 = vm.const.i64 1
  %c1024 = vm.const.i64 1024
  %c512 = vm.const.i64 512
  %c2097152 = vm.const.i64 2097152
  %c1048576 = vm.const.i64 1048576
  %c524288 = vm.const.i64 524288
  %zero_1 = vm.const.i64.zero
  %c-1 = vm.const.i64 -1
  %c2_2 = vm.const.i64 2
  %c-1_3 = vm.const.i32 -1
  %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
  %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
  vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
  %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
  vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
  vm.cond_br %_device_query_0, ^bb1, ^bb4
^bb1:  // pred: ^bb0
  vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
  %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
  %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %0, ^bb3, ^bb2
^bb2:  // pred: ^bb1
  %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
  vm.return %ref_10 : !vm.ref<!hal.buffer_view>
^bb3:  // pred: ^bb1
  vm.fail %0, "failed to wait on timepoint"
^bb4:  // pred: ^bb0
  vm.fail %c2, "device not supported in the compiled configuration"
}

// -----// IR Dump After Inliner (inline) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
    vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
    vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c8 = vm.const.i32 8
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c1_0 = vm.const.i64 1
      %c1024 = vm.const.i64 1024
      %c512 = vm.const.i64 512
      %c2097152 = vm.const.i64 2097152
      %c1048576 = vm.const.i64 1048576
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c2_2 = vm.const.i64 2
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_10 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @matmul_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c8 = vm.const.i32 8
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c1_0 = vm.const.i64 1
      %c1024 = vm.const.i64 1024
      %c512 = vm.const.i64 512
      %c2097152 = vm.const.i64 2097152
      %c1048576 = vm.const.i64 1048576
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c2_2 = vm.const.i64 2
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_10 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @matmul_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c8 = vm.const.i32 8
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c1_0 = vm.const.i64 1
      %c1024 = vm.const.i64 1024
      %c512 = vm.const.i64 512
      %c2097152 = vm.const.i64 2097152
      %c1048576 = vm.const.i64 1048576
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c2_2 = vm.const.i64 2
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_10 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @matmul_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c8 = vm.const.i32 8
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c1_0 = vm.const.i64 1
      %c1024 = vm.const.i64 1024
      %c512 = vm.const.i64 512
      %c2097152 = vm.const.i64 2097152
      %c1048576 = vm.const.i64 1048576
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c2_2 = vm.const.i64 2
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_10 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @matmul_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"0x080000004355444198E8FFFF1000000050000000440000005800000001000000040000002C0000006D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F35313278313032347832353600000000010000000000000001000000200000000800000001000000FA1600002F2F0A2F2F2047656E657261746564206279204C4C564D204E56505458204261636B2D456E640A2F2F0A0A2E76657273696F6E20362E300A2E74617267657420736D5F37300A2E616464726573735F73697A652036340A0A092F2F202E676C6F626C096D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235360A0A2E76697369626C65202E656E747279206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F353132783130323478323536280A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F302C0A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F312C0A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F320A290A2E6D61786E7469642033322C20382C20310A7B0A092E726567202E70726564200925703C32353E3B0A092E726567202E62313620092572733C353E3B0A092E726567202E623332200925723C31323E3B0A092E726567202E663332200925663C31323E3B0A092E726567202E62363420092572643C3134333E3B0A0A096C642E706172616D2E753634200925726435362C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F305D3B0A096C642E706172616D2E753634200925726435372C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F325D3B0A09637674612E746F2E676C6F62616C2E7536342009257264312C2025726435373B0A096C642E706172616D2E753634200925726435382C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F315D3B0A09637674612E746F2E676C6F62616C2E7536342009257264322C2025726435383B0A09637674612E746F2E676C6F62616C2E7536342009257264332C2025726435363B0A096D6F762E75333220092572322C202563746169642E783B0A096376742E7536342E7533322009257264342C202572323B0A096D6F762E75333220092572312C202563746169642E793B0A0973686C2E62333220092572332C202572312C20353B0A096376742E7536342E7533322009257264352C202572333B0A096D6F762E753634200925726435392C203531323B0A097375622E733634200925726436302C2025726435392C20257264353B0A096D696E2E733634200925726436312C2025726436302C2033323B0A096D756C2E776964652E7533322009257264362C202572322C203132383B0A096D6F762E753634200925726436322C20313032343B0A097375622E733634200925726436332C2025726436322C20257264363B0A096D696E2E733634200925726436342C2025726436332C203132383B0A096D6F762E75333220092572342C20257469642E783B0A096376742E7536342E753332200925726436352C202572343B0A096D6F762E75333220092572352C20257469642E793B0A096376742E7536342E753332200925726436362C202572353B0A09736574702E6C742E73363420092570312C2025726436312C20313B0A096E65672E733634200925726436372C2025726436313B0A096164642E733634200925726436382C2025726436312C202D313B0A0973656C702E623634200925726436392C2025726436372C2025726436382C202570313B0A096376742E7533322E75363420092572362C2025726436393B0A097368722E73333220092572372C202572362C2033313B0A097368722E75333220092572382C202572372C2032393B0A096164642E73333220092572392C202572362C202572383B0A097368722E7333322009257231302C202572392C20333B0A096376742E7336342E733332200925726437302C20257231303B0A096E65672E733634200925726437312C2025726437303B0A096164642E733634200925726437322C2025726437302C20313B0A0973656C702E623634200925726437332C2025726437312C2025726437322C202570313B0A096D756C2E6C6F2E7336342009257264372C2025726437332C2025726436363B0A097375622E733634200925726437342C2025726436312C20257264373B0A096D696E2E7336342009257264382C2025726437342C2025726437333B0A096D61782E7336342009257264392C20257264382C20303B0A09736574702E6C742E73363420092570322C2025726436342C20313B0A096E65672E733634200925726437352C2025726436343B0A096164642E733634200925726437362C2025726436342C202D313B0A0973656C702E623634200925726437372C2025726437352C2025726437362C202570323B0A097368722E733634200925726437382C2025726437372C2036333B0A097368722E753634200925726437392C2025726437382C2035393B0A096164642E733634200925726438302C2025726437372C2025726437393B0A097368722E733634200925726438312C2025726438302C20353B0A096E65672E733634200925726438322C2025726438313B0A096164642E733634200925726438332C2025726438312C20313B0A0973656C702E623634200925726438342C2025726438322C2025726438332C202570323B0A096D756C2E6C6F2E733634200925726431302C2025726438342C2025726436353B0A097375622E733634200925726438352C2025726436342C2025726431303B0A096D696E2E733634200925726431312C2025726438352C2025726438343B0A096D61782E733634200925726431322C2025726431312C20303B0A09736574702E6C742E73363420092570332C20257264382C20313B0A0973686C2E62363420092572643132342C20257264372C2031303B0A0973686C2E62363420092572643132352C2025726431302C20323B0A0973686C2E62363420092572643132362C20257264342C20393B0A09736574702E6C742E7336342009257032342C2025726431312C20313B0A0940257033206272612009244C5F5F4242305F31323B0A096D6F762E75363420092572643133322C20303B0A0973686C2E623634200925726431342C2025726431322C20323B0A09616E642E62363420200925726431352C20257264392C20313B0A09736574702E65712E73363420092570342C20257264392C20313B0A09736574702E65712E7336342009257032332C2025726431342C20303B0A096D6F762E7531362009257273342C20303B0A0940257034206272612009244C5F5F4242305F393B0A096272612E756E692009244C5F5F4242305F323B0A244C5F5F4242305F393A0A09736574702E65712E7336342009257031322C2025726431352C20303B0A096F722E70726564202009257031332C20257031322C20257032343B0A094025703133206272612009244C5F5F4242305F31323B0A0973686C2E6233322009257231312C202572312C2031353B0A096164642E733634200925726438382C202572643132342C2025726431303B0A096376742E7536342E753332200925726438392C20257231313B0A096164642E733634200925726439302C2025726438382C2025726438393B0A096164642E733634200925726439312C2025726439302C20257264363B0A0973686C2E623634200925726431332C2025726439312C20323B0A096164642E73363420092572643130342C2025726431332C202572643133323B0A096164642E733634200925726432372C20257264312C202572643130343B0A096D6F762E75363420092572643133332C20303B0A094025703233206272612009244C5F5F4242305F31323B0A244C5F5F4242305F31313A0A096164642E73363420092572643130362C2025726432372C202572643133333B0A0973742E676C6F62616C2E753820095B2572643130365D2C20257273343B0A096164642E73363420092572643133332C202572643133332C20313B0A09736574702E6C742E7536342009257031352C202572643133332C2025726431343B0A094025703135206272612009244C5F5F4242305F31313B0A244C5F5F4242305F31323A0A096164642E733634200925726433302C20257264372C20257264353B0A096164642E733634200925726433312C2025726431302C20257264363B0A096D756C2E776964652E75333220092572643130392C202572312C2033323736383B0A096164642E73363420092572643131302C202572643132342C202572643130393B0A096164642E73363420092572643131312C202572643131302C20257264333B0A096164642E73363420092572643133352C202572643131312C20343B0A096164642E73363420092572643131342C202572643132352C202572643132363B0A096164642E73363420092572643131352C202572643131342C20257264323B0A096164642E73363420092572643133342C202572643131352C20343039363B0A096D6F762E75363420092572643133362C20303B0A096272612E756E692009244C5F5F4242305F31333B0A244C5F5F4242305F32313A0A096164642E733634200925726435332C202572643133362C2033323B0A096164642E73363420092572643133352C202572643133352C203132383B0A096164642E73363420092572643133342C202572643133342C203133313037323B0A09736574702E6C742E7536342009257032312C202572643133362C203232343B0A096D6F762E75363420092572643133362C2025726435333B0A094025703231206272612009244C5F5F4242305F31333B0A096272612E756E692009244C5F5F4242305F32323B0A244C5F5F4242305F31333A0A0940257033206272612009244C5F5F4242305F32313B0A096D6F762E75363420092572643133382C20303B0A096D6F762E75363420092572643133372C202572643133353B0A096272612E756E692009244C5F5F4242305F31353B0A244C5F5F4242305F32303A0A096164642E73363420092572643133382C202572643133382C20313B0A096164642E73363420092572643133372C202572643133372C20313032343B0A09736574702E6E652E7336342009257032302C202572643133382C20257264393B0A094025703230206272612009244C5F5F4242305F31353B0A096272612E756E692009244C5F5F4242305F32313B0A244C5F5F4242305F31353A0A094025703234206272612009244C5F5F4242305F32303B0A096164642E73363420092572643131382C2025726433302C202572643133383B0A0973686C2E623634200925726434312C202572643131382C2031303B0A096D6F762E75363420092572643131372C20303B0A096D6F762E75363420092572643133392C202572643133343B0A096D6F762E75363420092572643134302C202572643131373B0A244C5F5F4242305F31373A0A096164642E73363420092572643132302C2025726433312C202572643134303B0A096164642E73363420092572643132312C202572643132302C2025726434313B0A0973686C2E62363420092572643132322C202572643132312C20323B0A096164642E733634200925726434342C20257264312C202572643132323B0A096C642E676C6F62616C2E6633322009256631312C205B25726434345D3B0A096D6F762E75363420092572643134312C202572643133393B0A096D6F762E75363420092572643134322C202572643131373B0A244C5F5F4242305F31383A0A096164642E73363420092572643132332C202572643133372C202572643134323B0A096C642E676C6F62616C2E66333220092566342C205B2572643132332B2D345D3B0A096C642E676C6F62616C2E66333220092566352C205B2572643134312B2D343039365D3B0A096D756C2E726E2E66333220092566362C202566342C202566353B0A096164642E726E2E66333220092566372C20256631312C202566363B0A0973742E676C6F62616C2E66333220095B25726434345D2C202566373B0A096C642E676C6F62616C2E66333220092566382C205B2572643132335D3B0A096C642E676C6F62616C2E66333220092566392C205B2572643134315D3B0A096D756C2E726E2E6633322009256631302C202566382C202566393B0A096164642E726E2E6633322009256631312C202566372C20256631303B0A0973742E676C6F62616C2E66333220095B25726434345D2C20256631313B0A096164642E73363420092572643134322C202572643134322C20383B0A096164642E73363420092572643134312C202572643134312C20383139323B0A09736574702E6E652E7336342009257031382C202572643134322C203132383B0A094025703138206272612009244C5F5F4242305F31383B0A096164642E73363420092572643134302C202572643134302C20313B0A096164642E73363420092572643133392C202572643133392C20343B0A09736574702E6E652E7336342009257031392C202572643134302C2025726431323B0A094025703139206272612009244C5F5F4242305F31373B0A096272612E756E692009244C5F5F4242305F32303B0A244C5F5F4242305F32323A0A097265743B0A244C5F5F4242305F323A0A09616E642E6236342020092572643132382C20257264392C20393232333337323033363835343737353830363B0A0973686C2E623634200925726439332C20257264372C2031323B0A096164642E733634200925726439352C2025726439332C202572643132353B0A096D756C2E776964652E753332200925726439362C202572312C203133313037323B0A096164642E733634200925726439372C2025726439352C2025726439363B0A096164642E733634200925726439392C2025726439372C202572643132363B0A096164642E733634200925726431372C20257264312C2025726439393B0A096D6F762E75363420092572643133322C20303B0A096272612E756E692009244C5F5F4242305F333B0A244C5F5F4242305F383A0A096164642E73363420092572643133322C202572643133322C20383139323B0A096164642E73363420092572643132382C202572643132382C202D323B0A09736574702E65712E7336342009257031302C202572643132382C20303B0A094025703130206272612009244C5F5F4242305F393B0A244C5F5F4242305F333A0A094025703234206272612009244C5F5F4242305F383B0A096164642E733634200925726432302C2025726431372C202572643133323B0A096D6F762E75363420092572643133302C20303B0A094025703233206272612009244C5F5F4242305F363B0A244C5F5F4242305F353A0A096164642E73363420092572643130312C2025726432302C202572643133303B0A0973742E676C6F62616C2E753820095B2572643130315D2C20257273343B0A096164642E73363420092572643133302C202572643133302C20313B0A09736574702E6C742E75363420092570372C202572643133302C2025726431343B0A0940257037206272612009244C5F5F4242305F353B0A244C5F5F4242305F363A0A096164642E733634200925726432332C2025726432302C20343039363B0A096D6F762E75363420092572643133312C20303B0A094025703233206272612009244C5F5F4242305F383B0A244C5F5F4242305F373A0A096164642E73363420092572643130332C2025726432332C202572643133313B0A0973742E676C6F62616C2E753820095B2572643130335D2C20257273343B0A096164642E73363420092572643133312C202572643133312C20313B0A09736574702E6C742E75363420092570392C202572643133312C2025726431343B0A0940257039206272612009244C5F5F4242305F373B0A096272612E756E692009244C5F5F4242305F383B0A0A7D0A00000C001400040008000C001000"> : vector<6012xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c8 = vm.const.i32 8
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c1_0 = vm.const.i64 1
      %c1024 = vm.const.i64 1024
      %c512 = vm.const.i64 512
      %c2097152 = vm.const.i64 2097152
      %c1048576 = vm.const.i64 1048576
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c2_2 = vm.const.i64 2
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_10 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @matmul_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c8 = vm.const.i32 8
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c1_0 = vm.const.i64 1
      %c1024 = vm.const.i64 1024
      %c512 = vm.const.i64 512
      %c2097152 = vm.const.i64 2097152
      %c1048576 = vm.const.i64 1048576
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c2_2 = vm.const.i64 2
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_10 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @matmul_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"0x080000004355444198E8FFFF1000000050000000440000005800000001000000040000002C0000006D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F35313278313032347832353600000000010000000000000001000000200000000800000001000000FA1600002F2F0A2F2F2047656E657261746564206279204C4C564D204E56505458204261636B2D456E640A2F2F0A0A2E76657273696F6E20362E300A2E74617267657420736D5F37300A2E616464726573735F73697A652036340A0A092F2F202E676C6F626C096D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235360A0A2E76697369626C65202E656E747279206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F353132783130323478323536280A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F302C0A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F312C0A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F320A290A2E6D61786E7469642033322C20382C20310A7B0A092E726567202E70726564200925703C32353E3B0A092E726567202E62313620092572733C353E3B0A092E726567202E623332200925723C31323E3B0A092E726567202E663332200925663C31323E3B0A092E726567202E62363420092572643C3134333E3B0A0A096C642E706172616D2E753634200925726435362C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F305D3B0A096C642E706172616D2E753634200925726435372C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F325D3B0A09637674612E746F2E676C6F62616C2E7536342009257264312C2025726435373B0A096C642E706172616D2E753634200925726435382C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F315D3B0A09637674612E746F2E676C6F62616C2E7536342009257264322C2025726435383B0A09637674612E746F2E676C6F62616C2E7536342009257264332C2025726435363B0A096D6F762E75333220092572322C202563746169642E783B0A096376742E7536342E7533322009257264342C202572323B0A096D6F762E75333220092572312C202563746169642E793B0A0973686C2E62333220092572332C202572312C20353B0A096376742E7536342E7533322009257264352C202572333B0A096D6F762E753634200925726435392C203531323B0A097375622E733634200925726436302C2025726435392C20257264353B0A096D696E2E733634200925726436312C2025726436302C2033323B0A096D756C2E776964652E7533322009257264362C202572322C203132383B0A096D6F762E753634200925726436322C20313032343B0A097375622E733634200925726436332C2025726436322C20257264363B0A096D696E2E733634200925726436342C2025726436332C203132383B0A096D6F762E75333220092572342C20257469642E783B0A096376742E7536342E753332200925726436352C202572343B0A096D6F762E75333220092572352C20257469642E793B0A096376742E7536342E753332200925726436362C202572353B0A09736574702E6C742E73363420092570312C2025726436312C20313B0A096E65672E733634200925726436372C2025726436313B0A096164642E733634200925726436382C2025726436312C202D313B0A0973656C702E623634200925726436392C2025726436372C2025726436382C202570313B0A096376742E7533322E75363420092572362C2025726436393B0A097368722E73333220092572372C202572362C2033313B0A097368722E75333220092572382C202572372C2032393B0A096164642E73333220092572392C202572362C202572383B0A097368722E7333322009257231302C202572392C20333B0A096376742E7336342E733332200925726437302C20257231303B0A096E65672E733634200925726437312C2025726437303B0A096164642E733634200925726437322C2025726437302C20313B0A0973656C702E623634200925726437332C2025726437312C2025726437322C202570313B0A096D756C2E6C6F2E7336342009257264372C2025726437332C2025726436363B0A097375622E733634200925726437342C2025726436312C20257264373B0A096D696E2E7336342009257264382C2025726437342C2025726437333B0A096D61782E7336342009257264392C20257264382C20303B0A09736574702E6C742E73363420092570322C2025726436342C20313B0A096E65672E733634200925726437352C2025726436343B0A096164642E733634200925726437362C2025726436342C202D313B0A0973656C702E623634200925726437372C2025726437352C2025726437362C202570323B0A097368722E733634200925726437382C2025726437372C2036333B0A097368722E753634200925726437392C2025726437382C2035393B0A096164642E733634200925726438302C2025726437372C2025726437393B0A097368722E733634200925726438312C2025726438302C20353B0A096E65672E733634200925726438322C2025726438313B0A096164642E733634200925726438332C2025726438312C20313B0A0973656C702E623634200925726438342C2025726438322C2025726438332C202570323B0A096D756C2E6C6F2E733634200925726431302C2025726438342C2025726436353B0A097375622E733634200925726438352C2025726436342C2025726431303B0A096D696E2E733634200925726431312C2025726438352C2025726438343B0A096D61782E733634200925726431322C2025726431312C20303B0A09736574702E6C742E73363420092570332C20257264382C20313B0A0973686C2E62363420092572643132342C20257264372C2031303B0A0973686C2E62363420092572643132352C2025726431302C20323B0A0973686C2E62363420092572643132362C20257264342C20393B0A09736574702E6C742E7336342009257032342C2025726431312C20313B0A0940257033206272612009244C5F5F4242305F31323B0A096D6F762E75363420092572643133322C20303B0A0973686C2E623634200925726431342C2025726431322C20323B0A09616E642E62363420200925726431352C20257264392C20313B0A09736574702E65712E73363420092570342C20257264392C20313B0A09736574702E65712E7336342009257032332C2025726431342C20303B0A096D6F762E7531362009257273342C20303B0A0940257034206272612009244C5F5F4242305F393B0A096272612E756E692009244C5F5F4242305F323B0A244C5F5F4242305F393A0A09736574702E65712E7336342009257031322C2025726431352C20303B0A096F722E70726564202009257031332C20257031322C20257032343B0A094025703133206272612009244C5F5F4242305F31323B0A0973686C2E6233322009257231312C202572312C2031353B0A096164642E733634200925726438382C202572643132342C2025726431303B0A096376742E7536342E753332200925726438392C20257231313B0A096164642E733634200925726439302C2025726438382C2025726438393B0A096164642E733634200925726439312C2025726439302C20257264363B0A0973686C2E623634200925726431332C2025726439312C20323B0A096164642E73363420092572643130342C2025726431332C202572643133323B0A096164642E733634200925726432372C20257264312C202572643130343B0A096D6F762E75363420092572643133332C20303B0A094025703233206272612009244C5F5F4242305F31323B0A244C5F5F4242305F31313A0A096164642E73363420092572643130362C2025726432372C202572643133333B0A0973742E676C6F62616C2E753820095B2572643130365D2C20257273343B0A096164642E73363420092572643133332C202572643133332C20313B0A09736574702E6C742E7536342009257031352C202572643133332C2025726431343B0A094025703135206272612009244C5F5F4242305F31313B0A244C5F5F4242305F31323A0A096164642E733634200925726433302C20257264372C20257264353B0A096164642E733634200925726433312C2025726431302C20257264363B0A096D756C2E776964652E75333220092572643130392C202572312C2033323736383B0A096164642E73363420092572643131302C202572643132342C202572643130393B0A096164642E73363420092572643131312C202572643131302C20257264333B0A096164642E73363420092572643133352C202572643131312C20343B0A096164642E73363420092572643131342C202572643132352C202572643132363B0A096164642E73363420092572643131352C202572643131342C20257264323B0A096164642E73363420092572643133342C202572643131352C20343039363B0A096D6F762E75363420092572643133362C20303B0A096272612E756E692009244C5F5F4242305F31333B0A244C5F5F4242305F32313A0A096164642E733634200925726435332C202572643133362C2033323B0A096164642E73363420092572643133352C202572643133352C203132383B0A096164642E73363420092572643133342C202572643133342C203133313037323B0A09736574702E6C742E7536342009257032312C202572643133362C203232343B0A096D6F762E75363420092572643133362C2025726435333B0A094025703231206272612009244C5F5F4242305F31333B0A096272612E756E692009244C5F5F4242305F32323B0A244C5F5F4242305F31333A0A0940257033206272612009244C5F5F4242305F32313B0A096D6F762E75363420092572643133382C20303B0A096D6F762E75363420092572643133372C202572643133353B0A096272612E756E692009244C5F5F4242305F31353B0A244C5F5F4242305F32303A0A096164642E73363420092572643133382C202572643133382C20313B0A096164642E73363420092572643133372C202572643133372C20313032343B0A09736574702E6E652E7336342009257032302C202572643133382C20257264393B0A094025703230206272612009244C5F5F4242305F31353B0A096272612E756E692009244C5F5F4242305F32313B0A244C5F5F4242305F31353A0A094025703234206272612009244C5F5F4242305F32303B0A096164642E73363420092572643131382C2025726433302C202572643133383B0A0973686C2E623634200925726434312C202572643131382C2031303B0A096D6F762E75363420092572643131372C20303B0A096D6F762E75363420092572643133392C202572643133343B0A096D6F762E75363420092572643134302C202572643131373B0A244C5F5F4242305F31373A0A096164642E73363420092572643132302C2025726433312C202572643134303B0A096164642E73363420092572643132312C202572643132302C2025726434313B0A0973686C2E62363420092572643132322C202572643132312C20323B0A096164642E733634200925726434342C20257264312C202572643132323B0A096C642E676C6F62616C2E6633322009256631312C205B25726434345D3B0A096D6F762E75363420092572643134312C202572643133393B0A096D6F762E75363420092572643134322C202572643131373B0A244C5F5F4242305F31383A0A096164642E73363420092572643132332C202572643133372C202572643134323B0A096C642E676C6F62616C2E66333220092566342C205B2572643132332B2D345D3B0A096C642E676C6F62616C2E66333220092566352C205B2572643134312B2D343039365D3B0A096D756C2E726E2E66333220092566362C202566342C202566353B0A096164642E726E2E66333220092566372C20256631312C202566363B0A0973742E676C6F62616C2E66333220095B25726434345D2C202566373B0A096C642E676C6F62616C2E66333220092566382C205B2572643132335D3B0A096C642E676C6F62616C2E66333220092566392C205B2572643134315D3B0A096D756C2E726E2E6633322009256631302C202566382C202566393B0A096164642E726E2E6633322009256631312C202566372C20256631303B0A0973742E676C6F62616C2E66333220095B25726434345D2C20256631313B0A096164642E73363420092572643134322C202572643134322C20383B0A096164642E73363420092572643134312C202572643134312C20383139323B0A09736574702E6E652E7336342009257031382C202572643134322C203132383B0A094025703138206272612009244C5F5F4242305F31383B0A096164642E73363420092572643134302C202572643134302C20313B0A096164642E73363420092572643133392C202572643133392C20343B0A09736574702E6E652E7336342009257031392C202572643134302C2025726431323B0A094025703139206272612009244C5F5F4242305F31373B0A096272612E756E692009244C5F5F4242305F32303B0A244C5F5F4242305F32323A0A097265743B0A244C5F5F4242305F323A0A09616E642E6236342020092572643132382C20257264392C20393232333337323033363835343737353830363B0A0973686C2E623634200925726439332C20257264372C2031323B0A096164642E733634200925726439352C2025726439332C202572643132353B0A096D756C2E776964652E753332200925726439362C202572312C203133313037323B0A096164642E733634200925726439372C2025726439352C2025726439363B0A096164642E733634200925726439392C2025726439372C202572643132363B0A096164642E733634200925726431372C20257264312C2025726439393B0A096D6F762E75363420092572643133322C20303B0A096272612E756E692009244C5F5F4242305F333B0A244C5F5F4242305F383A0A096164642E73363420092572643133322C202572643133322C20383139323B0A096164642E73363420092572643132382C202572643132382C202D323B0A09736574702E65712E7336342009257031302C202572643132382C20303B0A094025703130206272612009244C5F5F4242305F393B0A244C5F5F4242305F333A0A094025703234206272612009244C5F5F4242305F383B0A096164642E733634200925726432302C2025726431372C202572643133323B0A096D6F762E75363420092572643133302C20303B0A094025703233206272612009244C5F5F4242305F363B0A244C5F5F4242305F353A0A096164642E73363420092572643130312C2025726432302C202572643133303B0A0973742E676C6F62616C2E753820095B2572643130315D2C20257273343B0A096164642E73363420092572643133302C202572643133302C20313B0A09736574702E6C742E75363420092570372C202572643133302C2025726431343B0A0940257037206272612009244C5F5F4242305F353B0A244C5F5F4242305F363A0A096164642E733634200925726432332C2025726432302C20343039363B0A096D6F762E75363420092572643133312C20303B0A094025703233206272612009244C5F5F4242305F383B0A244C5F5F4242305F373A0A096164642E73363420092572643130332C2025726432332C202572643133313B0A0973742E676C6F62616C2E753820095B2572643130335D2C20257273343B0A096164642E73363420092572643133312C202572643133312C20313B0A09736574702E6C742E75363420092570392C202572643133312C2025726431343B0A0940257039206272612009244C5F5F4242305F373B0A096272612E756E692009244C5F5F4242305F383B0A0A7D0A00000C001400040008000C001000"> : vector<6012xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c8 = vm.const.i32 8
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c1_0 = vm.const.i64 1
      %c1024 = vm.const.i64 1024
      %c512 = vm.const.i64 512
      %c2097152 = vm.const.i64 2097152
      %c1048576 = vm.const.i64 1048576
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c2_2 = vm.const.i64 2
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_10 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @matmul_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::GlobalInitializationPass (iree-vm-global-initialization) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c8 = vm.const.i32 8
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c1_0 = vm.const.i64 1
    %c1024 = vm.const.i64 1024
    %c512 = vm.const.i64 512
    %c2097152 = vm.const.i64 2097152
    %c1048576 = vm.const.i64 1048576
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c2_2 = vm.const.i64 2
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_10 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @matmul_static attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.br ^bb4
  ^bb4:  // pred: ^bb3
    vm.return
  }
  vm.export @__deinit
  vm.func private @__deinit() {
    vm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c8 = vm.const.i32 8
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c1_0 = vm.const.i64 1
      %c1024 = vm.const.i64 1024
      %c512 = vm.const.i64 512
      %c2097152 = vm.const.i64 2097152
      %c1048576 = vm.const.i64 1048576
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c2_2 = vm.const.i64 2
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_10 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @matmul_static attributes {iree.abi.stub}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.export @__deinit
    vm.func private @__deinit() {
      vm.return
    }
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"0x080000004355444198E8FFFF1000000050000000440000005800000001000000040000002C0000006D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F35313278313032347832353600000000010000000000000001000000200000000800000001000000FA1600002F2F0A2F2F2047656E657261746564206279204C4C564D204E56505458204261636B2D456E640A2F2F0A0A2E76657273696F6E20362E300A2E74617267657420736D5F37300A2E616464726573735F73697A652036340A0A092F2F202E676C6F626C096D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235360A0A2E76697369626C65202E656E747279206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F353132783130323478323536280A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F302C0A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F312C0A092E706172616D202E753634206D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F320A290A2E6D61786E7469642033322C20382C20310A7B0A092E726567202E70726564200925703C32353E3B0A092E726567202E62313620092572733C353E3B0A092E726567202E623332200925723C31323E3B0A092E726567202E663332200925663C31323E3B0A092E726567202E62363420092572643C3134333E3B0A0A096C642E706172616D2E753634200925726435362C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F305D3B0A096C642E706172616D2E753634200925726435372C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F325D3B0A09637674612E746F2E676C6F62616C2E7536342009257264312C2025726435373B0A096C642E706172616D2E753634200925726435382C205B6D61746D756C5F7374617469635F64697370617463685F305F6D61746D756C5F3531327831303234783235365F706172616D5F315D3B0A09637674612E746F2E676C6F62616C2E7536342009257264322C2025726435383B0A09637674612E746F2E676C6F62616C2E7536342009257264332C2025726435363B0A096D6F762E75333220092572322C202563746169642E783B0A096376742E7536342E7533322009257264342C202572323B0A096D6F762E75333220092572312C202563746169642E793B0A0973686C2E62333220092572332C202572312C20353B0A096376742E7536342E7533322009257264352C202572333B0A096D6F762E753634200925726435392C203531323B0A097375622E733634200925726436302C2025726435392C20257264353B0A096D696E2E733634200925726436312C2025726436302C2033323B0A096D756C2E776964652E7533322009257264362C202572322C203132383B0A096D6F762E753634200925726436322C20313032343B0A097375622E733634200925726436332C2025726436322C20257264363B0A096D696E2E733634200925726436342C2025726436332C203132383B0A096D6F762E75333220092572342C20257469642E783B0A096376742E7536342E753332200925726436352C202572343B0A096D6F762E75333220092572352C20257469642E793B0A096376742E7536342E753332200925726436362C202572353B0A09736574702E6C742E73363420092570312C2025726436312C20313B0A096E65672E733634200925726436372C2025726436313B0A096164642E733634200925726436382C2025726436312C202D313B0A0973656C702E623634200925726436392C2025726436372C2025726436382C202570313B0A096376742E7533322E75363420092572362C2025726436393B0A097368722E73333220092572372C202572362C2033313B0A097368722E75333220092572382C202572372C2032393B0A096164642E73333220092572392C202572362C202572383B0A097368722E7333322009257231302C202572392C20333B0A096376742E7336342E733332200925726437302C20257231303B0A096E65672E733634200925726437312C2025726437303B0A096164642E733634200925726437322C2025726437302C20313B0A0973656C702E623634200925726437332C2025726437312C2025726437322C202570313B0A096D756C2E6C6F2E7336342009257264372C2025726437332C2025726436363B0A097375622E733634200925726437342C2025726436312C20257264373B0A096D696E2E7336342009257264382C2025726437342C2025726437333B0A096D61782E7336342009257264392C20257264382C20303B0A09736574702E6C742E73363420092570322C2025726436342C20313B0A096E65672E733634200925726437352C2025726436343B0A096164642E733634200925726437362C2025726436342C202D313B0A0973656C702E623634200925726437372C2025726437352C2025726437362C202570323B0A097368722E733634200925726437382C2025726437372C2036333B0A097368722E753634200925726437392C2025726437382C2035393B0A096164642E733634200925726438302C2025726437372C2025726437393B0A097368722E733634200925726438312C2025726438302C20353B0A096E65672E733634200925726438322C2025726438313B0A096164642E733634200925726438332C2025726438312C20313B0A0973656C702E623634200925726438342C2025726438322C2025726438332C202570323B0A096D756C2E6C6F2E733634200925726431302C2025726438342C2025726436353B0A097375622E733634200925726438352C2025726436342C2025726431303B0A096D696E2E733634200925726431312C2025726438352C2025726438343B0A096D61782E733634200925726431322C2025726431312C20303B0A09736574702E6C742E73363420092570332C20257264382C20313B0A0973686C2E62363420092572643132342C20257264372C2031303B0A0973686C2E62363420092572643132352C2025726431302C20323B0A0973686C2E62363420092572643132362C20257264342C20393B0A09736574702E6C742E7336342009257032342C2025726431312C20313B0A0940257033206272612009244C5F5F4242305F31323B0A096D6F762E75363420092572643133322C20303B0A0973686C2E623634200925726431342C2025726431322C20323B0A09616E642E62363420200925726431352C20257264392C20313B0A09736574702E65712E73363420092570342C20257264392C20313B0A09736574702E65712E7336342009257032332C2025726431342C20303B0A096D6F762E7531362009257273342C20303B0A0940257034206272612009244C5F5F4242305F393B0A096272612E756E692009244C5F5F4242305F323B0A244C5F5F4242305F393A0A09736574702E65712E7336342009257031322C2025726431352C20303B0A096F722E70726564202009257031332C20257031322C20257032343B0A094025703133206272612009244C5F5F4242305F31323B0A0973686C2E6233322009257231312C202572312C2031353B0A096164642E733634200925726438382C202572643132342C2025726431303B0A096376742E7536342E753332200925726438392C20257231313B0A096164642E733634200925726439302C2025726438382C2025726438393B0A096164642E733634200925726439312C2025726439302C20257264363B0A0973686C2E623634200925726431332C2025726439312C20323B0A096164642E73363420092572643130342C2025726431332C202572643133323B0A096164642E733634200925726432372C20257264312C202572643130343B0A096D6F762E75363420092572643133332C20303B0A094025703233206272612009244C5F5F4242305F31323B0A244C5F5F4242305F31313A0A096164642E73363420092572643130362C2025726432372C202572643133333B0A0973742E676C6F62616C2E753820095B2572643130365D2C20257273343B0A096164642E73363420092572643133332C202572643133332C20313B0A09736574702E6C742E7536342009257031352C202572643133332C2025726431343B0A094025703135206272612009244C5F5F4242305F31313B0A244C5F5F4242305F31323A0A096164642E733634200925726433302C20257264372C20257264353B0A096164642E733634200925726433312C2025726431302C20257264363B0A096D756C2E776964652E75333220092572643130392C202572312C2033323736383B0A096164642E73363420092572643131302C202572643132342C202572643130393B0A096164642E73363420092572643131312C202572643131302C20257264333B0A096164642E73363420092572643133352C202572643131312C20343B0A096164642E73363420092572643131342C202572643132352C202572643132363B0A096164642E73363420092572643131352C202572643131342C20257264323B0A096164642E73363420092572643133342C202572643131352C20343039363B0A096D6F762E75363420092572643133362C20303B0A096272612E756E692009244C5F5F4242305F31333B0A244C5F5F4242305F32313A0A096164642E733634200925726435332C202572643133362C2033323B0A096164642E73363420092572643133352C202572643133352C203132383B0A096164642E73363420092572643133342C202572643133342C203133313037323B0A09736574702E6C742E7536342009257032312C202572643133362C203232343B0A096D6F762E75363420092572643133362C2025726435333B0A094025703231206272612009244C5F5F4242305F31333B0A096272612E756E692009244C5F5F4242305F32323B0A244C5F5F4242305F31333A0A0940257033206272612009244C5F5F4242305F32313B0A096D6F762E75363420092572643133382C20303B0A096D6F762E75363420092572643133372C202572643133353B0A096272612E756E692009244C5F5F4242305F31353B0A244C5F5F4242305F32303A0A096164642E73363420092572643133382C202572643133382C20313B0A096164642E73363420092572643133372C202572643133372C20313032343B0A09736574702E6E652E7336342009257032302C202572643133382C20257264393B0A094025703230206272612009244C5F5F4242305F31353B0A096272612E756E692009244C5F5F4242305F32313B0A244C5F5F4242305F31353A0A094025703234206272612009244C5F5F4242305F32303B0A096164642E73363420092572643131382C2025726433302C202572643133383B0A0973686C2E623634200925726434312C202572643131382C2031303B0A096D6F762E75363420092572643131372C20303B0A096D6F762E75363420092572643133392C202572643133343B0A096D6F762E75363420092572643134302C202572643131373B0A244C5F5F4242305F31373A0A096164642E73363420092572643132302C2025726433312C202572643134303B0A096164642E73363420092572643132312C202572643132302C2025726434313B0A0973686C2E62363420092572643132322C202572643132312C20323B0A096164642E733634200925726434342C20257264312C202572643132323B0A096C642E676C6F62616C2E6633322009256631312C205B25726434345D3B0A096D6F762E75363420092572643134312C202572643133393B0A096D6F762E75363420092572643134322C202572643131373B0A244C5F5F4242305F31383A0A096164642E73363420092572643132332C202572643133372C202572643134323B0A096C642E676C6F62616C2E66333220092566342C205B2572643132332B2D345D3B0A096C642E676C6F62616C2E66333220092566352C205B2572643134312B2D343039365D3B0A096D756C2E726E2E66333220092566362C202566342C202566353B0A096164642E726E2E66333220092566372C20256631312C202566363B0A0973742E676C6F62616C2E66333220095B25726434345D2C202566373B0A096C642E676C6F62616C2E66333220092566382C205B2572643132335D3B0A096C642E676C6F62616C2E66333220092566392C205B2572643134315D3B0A096D756C2E726E2E6633322009256631302C202566382C202566393B0A096164642E726E2E6633322009256631312C202566372C20256631303B0A0973742E676C6F62616C2E66333220095B25726434345D2C20256631313B0A096164642E73363420092572643134322C202572643134322C20383B0A096164642E73363420092572643134312C202572643134312C20383139323B0A09736574702E6E652E7336342009257031382C202572643134322C203132383B0A094025703138206272612009244C5F5F4242305F31383B0A096164642E73363420092572643134302C202572643134302C20313B0A096164642E73363420092572643133392C202572643133392C20343B0A09736574702E6E652E7336342009257031392C202572643134302C2025726431323B0A094025703139206272612009244C5F5F4242305F31373B0A096272612E756E692009244C5F5F4242305F32303B0A244C5F5F4242305F32323A0A097265743B0A244C5F5F4242305F323A0A09616E642E6236342020092572643132382C20257264392C20393232333337323033363835343737353830363B0A0973686C2E623634200925726439332C20257264372C2031323B0A096164642E733634200925726439352C2025726439332C202572643132353B0A096D756C2E776964652E753332200925726439362C202572312C203133313037323B0A096164642E733634200925726439372C2025726439352C2025726439363B0A096164642E733634200925726439392C2025726439372C202572643132363B0A096164642E733634200925726431372C20257264312C2025726439393B0A096D6F762E75363420092572643133322C20303B0A096272612E756E692009244C5F5F4242305F333B0A244C5F5F4242305F383A0A096164642E73363420092572643133322C202572643133322C20383139323B0A096164642E73363420092572643132382C202572643132382C202D323B0A09736574702E65712E7336342009257031302C202572643132382C20303B0A094025703130206272612009244C5F5F4242305F393B0A244C5F5F4242305F333A0A094025703234206272612009244C5F5F4242305F383B0A096164642E733634200925726432302C2025726431372C202572643133323B0A096D6F762E75363420092572643133302C20303B0A094025703233206272612009244C5F5F4242305F363B0A244C5F5F4242305F353A0A096164642E73363420092572643130312C2025726432302C202572643133303B0A0973742E676C6F62616C2E753820095B2572643130315D2C20257273343B0A096164642E73363420092572643133302C202572643133302C20313B0A09736574702E6C742E75363420092570372C202572643133302C2025726431343B0A0940257037206272612009244C5F5F4242305F353B0A244C5F5F4242305F363A0A096164642E733634200925726432332C2025726432302C20343039363B0A096D6F762E75363420092572643133312C20303B0A094025703233206272612009244C5F5F4242305F383B0A244C5F5F4242305F373A0A096164642E73363420092572643130332C2025726432332C202572643133313B0A0973742E676C6F62616C2E753820095B2572643130335D2C20257273343B0A096164642E73363420092572643133312C202572643133312C20313B0A09736574702E6C742E75363420092570392C202572643133312C2025726431343B0A0940257039206272612009244C5F5F4242305F373B0A096272612E756E692009244C5F5F4242305F383B0A0A7D0A00000C001400040008000C001000"> : vector<6012xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c8 = vm.const.i32 8
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c1_0 = vm.const.i64 1
      %c1024 = vm.const.i64 1024
      %c512 = vm.const.i64 512
      %c2097152 = vm.const.i64 2097152
      %c1048576 = vm.const.i64 1048576
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c2_2 = vm.const.i64 2
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_10 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @matmul_static attributes {iree.abi.stub}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.export @__deinit
    vm.func private @__deinit() {
      vm.return
    }
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c8 = vm.const.i32 8
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c1_0 = vm.const.i64 1
      %c1024 = vm.const.i64 1024
      %c512 = vm.const.i64 512
      %c2097152 = vm.const.i64 2097152
      %c1048576 = vm.const.i64 1048576
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c2_2 = vm.const.i64 2
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_10 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @matmul_static attributes {iree.abi.stub}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.export @__deinit
    vm.func private @__deinit() {
      vm.return
    }
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::DropEmptyModuleInitializersPass (iree-vm-drop-empty-module-initializers) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c8 = vm.const.i32 8
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c1_0 = vm.const.i64 1
    %c1024 = vm.const.i64 1024
    %c512 = vm.const.i64 512
    %c2097152 = vm.const.i64 2097152
    %c1048576 = vm.const.i64 1048576
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c2_2 = vm.const.i64 2
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_10 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @matmul_static attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After DropCompilerHints (iree-util-drop-compiler-hints) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c8 = vm.const.i32 8
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c1_0 = vm.const.i64 1
      %c1024 = vm.const.i64 1024
      %c512 = vm.const.i64 512
      %c2097152 = vm.const.i64 2097152
      %c1048576 = vm.const.i64 1048576
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c2_2 = vm.const.i64 2
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_10 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @matmul_static attributes {iree.abi.stub}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::GlobalInitializationPass (iree-vm-global-initialization) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c8 = vm.const.i32 8
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c1_0 = vm.const.i64 1
    %c1024 = vm.const.i64 1024
    %c512 = vm.const.i64 512
    %c2097152 = vm.const.i64 2097152
    %c1048576 = vm.const.i64 1048576
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c2_2 = vm.const.i64 2
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_10 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @matmul_static attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.br ^bb4
  ^bb4:  // pred: ^bb3
    vm.return
  }
  vm.export @__deinit
  vm.func private @__deinit() {
    vm.return
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::DropEmptyModuleInitializersPass (iree-vm-drop-empty-module-initializers) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c8 = vm.const.i32 8
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c1_0 = vm.const.i64 1
    %c1024 = vm.const.i64 1024
    %c512 = vm.const.i64 512
    %c2097152 = vm.const.i64 2097152
    %c1048576 = vm.const.i64 1048576
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c2_2 = vm.const.i64 2
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_10 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @matmul_static attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.br ^bb4
  ^bb4:  // pred: ^bb3
    vm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
  %c2 = vm.const.i32 2
  %null = vm.const.ref.zero : !vm.ref<!hal.fence>
  %c13 = vm.const.i32 13
  %c28 = vm.const.i32 28
  %c8 = vm.const.i32 8
  %zero = vm.const.i32.zero
  %c3 = vm.const.i32 3
  %c17 = vm.const.i32 17
  %c150998019 = vm.const.i32 150998019
  %c50 = vm.const.i32 50
  %c3075 = vm.const.i32 3075
  %c16 = vm.const.i32 16
  %c256 = vm.const.i64 256
  %c1 = vm.const.i32 1
  %c553648160 = vm.const.i32 553648160
  %c1_0 = vm.const.i64 1
  %c1024 = vm.const.i64 1024
  %c512 = vm.const.i64 512
  %c2097152 = vm.const.i64 2097152
  %c1048576 = vm.const.i64 1048576
  %c524288 = vm.const.i64 524288
  %zero_1 = vm.const.i64.zero
  %c-1 = vm.const.i64 -1
  %c2_2 = vm.const.i64 2
  %c-1_3 = vm.const.i32 -1
  %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
  %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
  vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
  %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
  vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
  vm.cond_br %_device_query_0, ^bb1, ^bb4
^bb1:  // pred: ^bb0
  vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
  %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
  %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %0, ^bb3, ^bb2
^bb2:  // pred: ^bb1
  %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
  vm.return %ref_10 : !vm.ref<!hal.buffer_view>
^bb3:  // pred: ^bb1
  vm.fail %0, "failed to wait on timepoint"
^bb4:  // pred: ^bb0
  vm.fail %c2, "device not supported in the compiled configuration"
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @__init() {
  %null = vm.const.ref.zero : !vm.ref<!hal.executable>
  %null_0 = vm.const.ref.zero : !vm.buffer
  %c2 = vm.const.i32 2
  %c7 = vm.const.i32 7
  %zero = vm.const.i32.zero
  %c1 = vm.const.i32 1
  %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
  %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
  %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
  %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
  %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
  %2 = vm.and.i32 %1, %c1 : i32
  %3 = vm.select.i32 %0#0, %2, %zero : i32
  %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
  %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
  vm.global.store.i32 %3, @_device_query_0 : i32
  vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.cond_br %3, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
  %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
  vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
^bb2:  // pred: ^bb0
  vm.br ^bb3(%null : !vm.ref<!hal.executable>)
^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
  vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.return
}

// -----// IR Dump After Inliner (inline) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c8 = vm.const.i32 8
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c1_0 = vm.const.i64 1
    %c1024 = vm.const.i64 1024
    %c512 = vm.const.i64 512
    %c2097152 = vm.const.i64 2097152
    %c1048576 = vm.const.i64 1048576
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c2_2 = vm.const.i64 2
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_10 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @matmul_static attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After CSE (cse) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c8 = vm.const.i32 8
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c1_0 = vm.const.i64 1
    %c1024 = vm.const.i64 1024
    %c512 = vm.const.i64 512
    %c2097152 = vm.const.i64 2097152
    %c1048576 = vm.const.i64 1048576
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c2_2 = vm.const.i64 2
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_10 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @matmul_static attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c8 = vm.const.i32 8
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c1_0 = vm.const.i64 1
    %c1024 = vm.const.i64 1024
    %c512 = vm.const.i64 512
    %c2097152 = vm.const.i64 2097152
    %c1048576 = vm.const.i64 1048576
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c2_2 = vm.const.i64 2
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_10 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @matmul_static attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After DropCompilerHints (iree-util-drop-compiler-hints) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<6012xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c8 = vm.const.i32 8
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c1_0 = vm.const.i64 1
    %c1024 = vm.const.i64 1024
    %c512 = vm.const.i64 512
    %c2097152 = vm.const.i64 2097152
    %c1048576 = vm.const.i64 1048576
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c2_2 = vm.const.i64 2
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_10 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @matmul_static attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::OrdinalAllocationPass (iree-vm-ordinal-allocation) //----- //
vm.module public @module attributes {ordinal_counts = #vm.ordinal_counts<import_funcs = 19, export_funcs = 2, internal_funcs = 2, global_bytes = 4, global_refs = 2, rodatas = 4, rwdatas = 0>} {
  vm.global.i32 private mutable @_device_query_0 {ordinal = 0 : i32} : i32
  vm.global.ref private mutable @_pipeline_layout_0 {ordinal = 0 : i32} : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_matmul_static_dispatch_0 {ordinal = 1 : i32} : !vm.ref<!hal.executable>
  vm.rodata private @matmul_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers", ordinal = 0 : i32} dense<"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vector<6012xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64, ordinal = 1 : i32} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64, ordinal = 2 : i32} "cuda-nvptx-fb"
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, ordinal = 0 : i32, sym_visibility = "private"}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {ordinal = 1 : i32, sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {ordinal = 2 : i32, sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, ordinal = 3 : i32, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {ordinal = 4 : i32, sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, ordinal = 5 : i32, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {ordinal = 6 : i32, sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {ordinal = 7 : i32, sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {ordinal = 8 : i32, sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {ordinal = 9 : i32, sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {ordinal = 10 : i32, sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, ordinal = 11 : i32, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, ordinal = 12 : i32, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, ordinal = 13 : i32, sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {ordinal = 14 : i32, sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, ordinal = 15 : i32, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {ordinal = 16 : i32, sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {ordinal = 17 : i32, sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, ordinal = 18 : i32, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64, ordinal = 3 : i32} "tensor"
  vm.func private @matmul_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {ordinal = 0 : i32} {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c8 = vm.const.i32 8
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c1_0 = vm.const.i64 1
    %c1024 = vm.const.i64 1024
    %c512 = vm.const.i64 512
    %c2097152 = vm.const.i64 2097152
    %c1048576 = vm.const.i64 1048576
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c2_2 = vm.const.i64 2
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_matmul_static_dispatch_0 = vm.global.load.ref @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c256, %c1024]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c1048576, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_7 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2097152) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_8 = vm.call @hal.command_buffer.create(%ref_4, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_8, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_0, %zero, %ref_6, %zero_1, %c1048576), (%c2_2, %zero, %ref_7, %zero_1, %c2097152)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_8, %_executable_matmul_static_dispatch_0, %zero, %c8, %c16, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_8, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_8) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_9 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %null, %ref_9, [%ref_8]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_9]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_10 = vm.call.variadic @hal.buffer_view.create(%ref_7, %zero_1, %c2097152, %c553648160, %c1, [%c512, %c1024]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_10 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @matmul_static attributes {iree.abi.stub, ordinal = 0 : i32}
  vm.export @__init attributes {ordinal = 1 : i32}
  vm.func private @__init() attributes {ordinal = 1 : i32} {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %matmul_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @matmul_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %matmul_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_matmul_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
}

