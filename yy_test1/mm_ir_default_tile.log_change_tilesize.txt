//----> Yufan woof woof ----->
Pass Manager with 1 passes:
builtin.module(hal.executable(hal.executable.variant(iree-llvmcpu-lower-executable-target{test-lowering-configuration=false use-lowering-pipeline=})))

Yufan:: [setRootConfig contraction] 3
Yufan:: [TileAndDistributeToWorkgroupsPass] 
Yufan:: [TileAndDistributeToWorkgroupsPass] tiledLoops.empty() 1
Yufan:: Again [TileAndDistributeToWorkgroupsPass] tiledLoops.empty() 0
ts 3
256
128
0
Yufan:: tiled Loop scf.for
Yufan:: tiled Loop info scf.for
Yufan:: tiled size 256
Yufan:: tiled Loop scf.for
Yufan:: tiled Loop info scf.for
Yufan:: tiled size 128
// -----// IR Dump After TileAndDistributeToWorkgroups (iree-codegen-tile-and-distribute-to-workgroups) //----- //
hal.executable.variant public @embedded_elf_x86_64, target = <"llvm-cpu", "embedded-elf-x86_64", {cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", native_vector_size = 16 : index, target_triple = "x86_64-unknown-unknown-eabi-elf"}> {
  hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#hal.pipeline.layout<push_constants = 6, sets = [<0, bindings = [<0, storage_buffer>, <1, storage_buffer>, <2, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingPadExpert>} {
  ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index, %arg4: index):
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    hal.return %c8, %c2, %c1 : index, index, index
  }
  builtin.module {
    func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
      %c512 = arith.constant 512 : index
      %c1024 = arith.constant 1024 : index
      %c0 = arith.constant 0 : index
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
      %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
      %workgroup_id_x = hal.interface.workgroup.id[0] : index
      %workgroup_count_x = hal.interface.workgroup.count[0] : index
      %workgroup_id_y = hal.interface.workgroup.id[1] : index
      %workgroup_count_y = hal.interface.workgroup.count[1] : index
      %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
      %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
      scf.for %arg0 = %3 to %c512 step %4 {
        %5 = affine.min affine_map<(d0) -> (256, -d0 + 512)>(%arg0)
        %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
        %7 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
        scf.for %arg1 = %6 to %c1024 step %7 {
          %8 = affine.min affine_map<(d0) -> (128, -d0 + 1024)>(%arg1)
          %9 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%5, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
          %10 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %8], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
          %11 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [%5, %8], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<?x?xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%9, %10 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%11 : tensor<?x?xf32>) -> tensor<?x?xf32>
          flow.dispatch.tensor.store %12, %2, offsets = [%arg0, %arg1], sizes = [%5, %8], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
        }
      }
      return
    }
  }
}

// -----// IR Dump After ConvertToDestinationPassingStyle (iree-codegen-convert-to-destination-passing-style) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.min affine_map<(d0) -> (256, -d0 + 512)>(%arg0)
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %7 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %6 to %c1024 step %7 {
      %8 = affine.min affine_map<(d0) -> (128, -d0 + 1024)>(%arg1)
      %9 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%5, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
      %10 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %8], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
      %11 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [%5, %8], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<?x?xf32>
      %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%9, %10 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%11 : tensor<?x?xf32>) -> tensor<?x?xf32>
      flow.dispatch.tensor.store %12, %2, offsets = [%arg0, %arg1], sizes = [%5, %8], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After FoldAffineMinInDistributedLoops (iree-codegen-fold-affinemin-in-distributed-loops) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %5 to %c1024 step %6 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%c256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %c128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<?x?xf32>
      %10 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%7, %8 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%9 : tensor<?x?xf32>) -> tensor<?x?xf32>
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    scf.for %arg0 = %3 to %c512 step %4 {
      %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
      %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
      scf.for %arg1 = %5 to %c1024 step %6 {
        %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
        %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
        %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
        %10 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%7, %8 : tensor<256x256xf32>, tensor<256x128xf32>) outs(%9 : tensor<256x128xf32>) -> tensor<256x128xf32>
        flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    scf.for %arg0 = %3 to %c512 step %4 {
      %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
      %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
      scf.for %arg1 = %5 to %c1024 step %6 {
        %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
        %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
        %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
        %10 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%7, %8 : tensor<256x256xf32>, tensor<256x128xf32>) outs(%9 : tensor<256x128xf32>) -> tensor<256x128xf32>
        flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After TileAndDecomposeWinogradTransform (iree-linalg-ext-tile-and-decompose-winograd) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %5 to %c1024 step %6 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%7, %8 : tensor<256x256xf32>, tensor<256x128xf32>) outs(%9 : tensor<256x128xf32>) -> tensor<256x128xf32>
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyTileAndFusePass (iree-linalg-strategy-tile-and-fuse-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %5 to %c1024 step %6 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgFuse (linalg-fuse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyPadPass (iree-linalg-strategy-pad-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgFuse (linalg-fuse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyPadPass (iree-linalg-strategy-pad-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgFuse (linalg-fuse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyPadPass (iree-linalg-strategy-pad-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgFuse (linalg-fuse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<4x256xf32>, tensor<256x64xf32>) outs(%extracted_slice_1 : tensor<4x64xf32>) -> tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyTilePass (iree-linalg-strategy-tile-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %extracted_slice_1) -> (tensor<4x64xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [4, 2] [1, 1] : tensor<4x256xf32> to tensor<4x2xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [2, 64] [1, 1] : tensor<256x64xf32> to tensor<2x64xf32>
            %13 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<4x2xf32>, tensor<2x64xf32>) outs(%arg7 : tensor<4x64xf32>) -> tensor<4x64xf32>
            scf.yield %13 : tensor<4x64xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %extracted_slice_1) -> (tensor<4x64xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [4, 2] [1, 1] : tensor<4x256xf32> to tensor<4x2xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [2, 64] [1, 1] : tensor<256x64xf32> to tensor<2x64xf32>
            %13 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<4x2xf32>, tensor<2x64xf32>) outs(%arg7 : tensor<4x64xf32>) -> tensor<4x64xf32>
            scf.yield %13 : tensor<4x64xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %extracted_slice_1) -> (tensor<4x64xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [4, 2] [1, 1] : tensor<4x256xf32> to tensor<4x2xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [2, 64] [1, 1] : tensor<256x64xf32> to tensor<2x64xf32>
            %13 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<4x2xf32>, tensor<2x64xf32>) outs(%arg7 : tensor<4x64xf32>) -> tensor<4x64xf32>
            scf.yield %13 : tensor<4x64xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %extracted_slice_1) -> (tensor<4x64xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [4, 2] [1, 1] : tensor<4x256xf32> to tensor<4x2xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [2, 64] [1, 1] : tensor<256x64xf32> to tensor<2x64xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<4x2xf32>, tensor<2x64xf32>) outs(%arg7 : tensor<4x64xf32>) -> tensor<4x64xf32>
            scf.yield %13 : tensor<4x64xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %extracted_slice_1) -> (tensor<4x64xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [4, 2] [1, 1] : tensor<4x256xf32> to tensor<4x2xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [2, 64] [1, 1] : tensor<256x64xf32> to tensor<2x64xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<4x2xf32>, tensor<2x64xf32>) outs(%arg7 : tensor<4x64xf32>) -> tensor<4x64xf32>
            scf.yield %13 : tensor<4x64xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %extracted_slice_1) -> (tensor<4x64xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [4, 2] [1, 1] : tensor<4x256xf32> to tensor<4x2xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [2, 64] [1, 1] : tensor<256x64xf32> to tensor<2x64xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<4x2xf32>, tensor<2x64xf32>) outs(%arg7 : tensor<4x64xf32>) -> tensor<4x64xf32>
            scf.yield %13 : tensor<4x64xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgSingleTilingExpert (linalg-single-tiling-expert-driver) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %extracted_slice_1) -> (tensor<4x64xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [4, 2] [1, 1] : tensor<4x256xf32> to tensor<4x2xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [2, 64] [1, 1] : tensor<256x64xf32> to tensor<2x64xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<4x2xf32>, tensor<2x64xf32>) outs(%arg7 : tensor<4x64xf32>) -> tensor<4x64xf32>
            scf.yield %13 : tensor<4x64xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %extracted_slice_1) -> (tensor<4x64xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [4, 2] [1, 1] : tensor<4x256xf32> to tensor<4x2xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [2, 64] [1, 1] : tensor<256x64xf32> to tensor<2x64xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<4x2xf32>, tensor<2x64xf32>) outs(%arg7 : tensor<4x64xf32>) -> tensor<4x64xf32>
            scf.yield %13 : tensor<4x64xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %extracted_slice_1) -> (tensor<4x64xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [4, 2] [1, 1] : tensor<4x256xf32> to tensor<4x2xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [2, 64] [1, 1] : tensor<256x64xf32> to tensor<2x64xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<4x2xf32>, tensor<2x64xf32>) outs(%arg7 : tensor<4x64xf32>) -> tensor<4x64xf32>
            scf.yield %13 : tensor<4x64xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyPadPass (iree-linalg-strategy-pad-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %extracted_slice_1) -> (tensor<4x64xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [4, 2] [1, 1] : tensor<4x256xf32> to tensor<4x2xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [2, 64] [1, 1] : tensor<256x64xf32> to tensor<2x64xf32>
            %13 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<4x2xf32>, tensor<2x64xf32>) outs(%arg7 : tensor<4x64xf32>) -> tensor<4x64xf32>
            scf.yield %13 : tensor<4x64xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %extracted_slice_1) -> (tensor<4x64xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [4, 2] [1, 1] : tensor<4x256xf32> to tensor<4x2xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [2, 64] [1, 1] : tensor<256x64xf32> to tensor<2x64xf32>
            %13 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<4x2xf32>, tensor<2x64xf32>) outs(%arg7 : tensor<4x64xf32>) -> tensor<4x64xf32>
            scf.yield %13 : tensor<4x64xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %extracted_slice_1) -> (tensor<4x64xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [4, 2] [1, 1] : tensor<4x256xf32> to tensor<4x2xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [2, 64] [1, 1] : tensor<256x64xf32> to tensor<2x64xf32>
            %13 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<4x2xf32>, tensor<2x64xf32>) outs(%arg7 : tensor<4x64xf32>) -> tensor<4x64xf32>
            scf.yield %13 : tensor<4x64xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %extracted_slice_1) -> (tensor<4x64xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [4, 2] [1, 1] : tensor<4x256xf32> to tensor<4x2xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [2, 64] [1, 1] : tensor<256x64xf32> to tensor<2x64xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<4x2xf32>, tensor<2x64xf32>) outs(%arg7 : tensor<4x64xf32>) -> tensor<4x64xf32>
            scf.yield %13 : tensor<4x64xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %extracted_slice_1) -> (tensor<4x64xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [4, 2] [1, 1] : tensor<4x256xf32> to tensor<4x2xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [2, 64] [1, 1] : tensor<256x64xf32> to tensor<2x64xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<4x2xf32>, tensor<2x64xf32>) outs(%arg7 : tensor<4x64xf32>) -> tensor<4x64xf32>
            scf.yield %13 : tensor<4x64xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %extracted_slice_1) -> (tensor<4x64xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [4, 2] [1, 1] : tensor<4x256xf32> to tensor<4x2xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [2, 64] [1, 1] : tensor<256x64xf32> to tensor<2x64xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<4x2xf32>, tensor<2x64xf32>) outs(%arg7 : tensor<4x64xf32>) -> tensor<4x64xf32>
            scf.yield %13 : tensor<4x64xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgFuse (linalg-fuse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %extracted_slice_1) -> (tensor<4x64xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [4, 2] [1, 1] : tensor<4x256xf32> to tensor<4x2xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [2, 64] [1, 1] : tensor<256x64xf32> to tensor<2x64xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<4x2xf32>, tensor<2x64xf32>) outs(%arg7 : tensor<4x64xf32>) -> tensor<4x64xf32>
            scf.yield %13 : tensor<4x64xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c2 = arith.constant 2 : index
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg0 = %3 to %c512 step %4 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
      scf.for %arg1 = %5 to %c1024 step %6 {
        %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
        %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
        %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %7[%arg2, 0] [4, 256] [1, 1] : tensor<256x256xf32> to tensor<4x256xf32>
          %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
            %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 64] [1, 1] : tensor<256x128xf32> to tensor<256x64xf32>
            %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
            %12 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %extracted_slice_1) -> (tensor<4x64xf32>) {
              %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [4, 2] [1, 1] : tensor<4x256xf32> to tensor<4x2xf32>
              %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [2, 64] [1, 1] : tensor<256x64xf32> to tensor<2x64xf32>
              %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [4, 64, 0], [0, 0, 2]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<4x2xf32>, tensor<2x64xf32>) outs(%arg7 : tensor<4x64xf32>) -> tensor<4x64xf32>
              scf.yield %13 : tensor<4x64xf32>
            }
            %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
            scf.yield %inserted_slice : tensor<256x128xf32>
          }
          scf.yield %11 : tensor<256x128xf32>
        }
        flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After LinalgStrategyVectorizePass (iree-linalg-strategy-vectorize-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %extracted_slice) -> (tensor<4x64xf32>) {
            %13 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<4x2xf32>
            %14 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<2x64xf32>
            %15 = vector.transfer_read %arg7[%c0, %c0], %cst {in_bounds = [true, true]} : tensor<4x64xf32>, vector<4x64xf32>
            %16 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %13, %14, %15 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
            %17 = vector.transfer_write %16, %arg7[%c0, %c0] {in_bounds = [true, true]} : vector<4x64xf32>, tensor<4x64xf32>
            scf.yield %17 : tensor<4x64xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = vector.transfer_read %extracted_slice[%c0, %c0], %cst {in_bounds = [true, true]} : tensor<4x64xf32>, vector<4x64xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %12) -> (vector<4x64xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<4x2xf32>
            %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<2x64xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
            scf.yield %17 : vector<4x64xf32>
          }
          %14 = vector.transfer_write %13, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<4x64xf32>, tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = vector.transfer_read %extracted_slice[%c0, %c0], %cst {in_bounds = [true, true]} : tensor<4x64xf32>, vector<4x64xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %12) -> (vector<4x64xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<4x2xf32>
            %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<2x64xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
            scf.yield %17 : vector<4x64xf32>
          }
          %14 = vector.transfer_write %13, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<4x64xf32>, tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = vector.transfer_read %extracted_slice[%c0, %c0], %cst {in_bounds = [true, true]} : tensor<4x64xf32>, vector<4x64xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %12) -> (vector<4x64xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<4x2xf32>
            %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<2x64xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
            scf.yield %17 : vector<4x64xf32>
          }
          %14 = vector.transfer_write %13, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<4x64xf32>, tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = vector.transfer_read %extracted_slice[%c0, %c0], %cst {in_bounds = [true, true]} : tensor<4x64xf32>, vector<4x64xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %12) -> (vector<4x64xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<4x2xf32>
            %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<2x64xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
            scf.yield %17 : vector<4x64xf32>
          }
          %14 = vector.transfer_write %13, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<4x64xf32>, tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = vector.transfer_read %extracted_slice[%c0, %c0], %cst {in_bounds = [true, true]} : tensor<4x64xf32>, vector<4x64xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %12) -> (vector<4x64xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<4x2xf32>
            %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<2x64xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
            scf.yield %17 : vector<4x64xf32>
          }
          %14 = vector.transfer_write %13, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<4x64xf32>, tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgSingleTilingExpert (linalg-single-tiling-expert-driver) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<256x128xf32> to tensor<4x64xf32>
          %12 = vector.transfer_read %extracted_slice[%c0, %c0], %cst {in_bounds = [true, true]} : tensor<4x64xf32>, vector<4x64xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %12) -> (vector<4x64xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<4x2xf32>
            %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<2x64xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
            scf.yield %17 : vector<4x64xf32>
          }
          %14 = vector.transfer_write %13, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<4x64xf32>, tensor<4x64xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [4, 64] [1, 1] : tensor<4x64xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %12 = vector.transfer_read %arg5[%arg2, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<4x64xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %12) -> (vector<4x64xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<4x2xf32>
            %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<2x64xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
            scf.yield %17 : vector<4x64xf32>
          }
          %14 = vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<4x64xf32>, tensor<256x128xf32>
          scf.yield %14 : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %12 = vector.transfer_read %arg5[%arg2, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<4x64xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %12) -> (vector<4x64xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<4x2xf32>
            %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<2x64xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
            scf.yield %17 : vector<4x64xf32>
          }
          %14 = vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<4x64xf32>, tensor<256x128xf32>
          scf.yield %14 : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After EliminateEmptyTensors (iree-eliminate-empty-tensors) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %cst = arith.constant 0.000000e+00 : f32
    %c2 = arith.constant 2 : index
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg0 = %3 to %c512 step %4 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
      scf.for %arg1 = %5 to %c1024 step %6 {
        %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
        %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
        %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
          %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
            %12 = vector.transfer_read %arg5[%arg2, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<4x64xf32>
            %13 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %12) -> (vector<4x64xf32>) {
              %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<4x2xf32>
              %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<2x64xf32>
              %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
              scf.yield %17 : vector<4x64xf32>
            }
            %14 = vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<4x64xf32>, tensor<256x128xf32>
            scf.yield %14 : tensor<256x128xf32>
          }
          scf.yield %11 : tensor<256x128xf32>
        }
        flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After EmptyTensorToAllocTensor (empty-tensor-to-alloc-tensor) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %cst = arith.constant 0.000000e+00 : f32
    %c2 = arith.constant 2 : index
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg0 = %3 to %c512 step %4 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
      scf.for %arg1 = %5 to %c1024 step %6 {
        %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
        %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
        %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
          %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
            %12 = vector.transfer_read %arg5[%arg2, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<4x64xf32>
            %13 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %12) -> (vector<4x64xf32>) {
              %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<4x2xf32>
              %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<2x64xf32>
              %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
              scf.yield %17 : vector<4x64xf32>
            }
            %14 = vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<4x64xf32>, tensor<256x128xf32>
            scf.yield %14 : tensor<256x128xf32>
          }
          scf.yield %11 : tensor<256x128xf32>
        }
        flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After IREEComprehensiveBufferize (iree-codegen-iree-comprehensive-bufferize) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %cst = arith.constant 0.000000e+00 : f32
    %c2 = arith.constant 2 : index
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %6 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %7 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %9 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg0 = %6 to %c512 step %7 {
      %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.for %arg1 = %8 to %c1024 step %9 {
        %subview_0 = memref.subview %2[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_1 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %subview_1) -> (memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
          %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
            %12 = vector.transfer_read %arg5[%arg2, %arg4], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4x64xf32>
            %13 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %12) -> (vector<4x64xf32>) {
              %14 = vector.transfer_read %subview[%arg2, %arg6], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4x2xf32>
              %15 = vector.transfer_read %subview_0[%arg6, %arg4], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<2x64xf32>
              %16 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %14, %15, %arg7 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
              scf.yield %16 : vector<4x64xf32>
            }
            vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
            scf.yield %arg5 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
          }
          scf.yield %11 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        }
        %subview_2 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%10 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        }
      }
    }
    return
  }
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %cst = arith.constant 0.000000e+00 : f32
    %c2 = arith.constant 2 : index
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c512 = arith.constant 512 : index
    %c1024 = arith.constant 1024 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %6 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %7 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %9 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg0 = %6 to %c512 step %7 {
      %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.for %arg1 = %8 to %c1024 step %9 {
        %subview_0 = memref.subview %2[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_1 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %10 = scf.for %arg2 = %c0 to %c256 step %c4 iter_args(%arg3 = %subview_1) -> (memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
          %11 = scf.for %arg4 = %c0 to %c128 step %c64 iter_args(%arg5 = %arg3) -> (memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
            %12 = vector.transfer_read %arg5[%arg2, %arg4], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4x64xf32>
            %13 = scf.for %arg6 = %c0 to %c256 step %c2 iter_args(%arg7 = %12) -> (vector<4x64xf32>) {
              %14 = vector.transfer_read %subview[%arg2, %arg6], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4x2xf32>
              %15 = vector.transfer_read %subview_0[%arg6, %arg4], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<2x64xf32>
              %16 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %14, %15, %arg7 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
              scf.yield %16 : vector<4x64xf32>
            }
            vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
            scf.yield %arg5 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
          }
          scf.yield %11 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        }
        %subview_2 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%10 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        }
      }
    }
    return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %6 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %7 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %9 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %6 to %c512 step %7 {
    %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.for %arg1 = %8 to %c1024 step %9 {
      %subview_0 = memref.subview %2[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_1 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.for %arg2 = %c0 to %c256 step %c4 {
        scf.for %arg3 = %c0 to %c128 step %c64 {
          %10 = vector.transfer_read %subview_1[%arg2, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4x64xf32>
          %11 = scf.for %arg4 = %c0 to %c256 step %c2 iter_args(%arg5 = %10) -> (vector<4x64xf32>) {
            %12 = vector.transfer_read %subview[%arg2, %arg4], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4x2xf32>
            %13 = vector.transfer_read %subview_0[%arg4, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<2x64xf32>
            %14 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %12, %13, %arg5 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
            scf.yield %14 : vector<4x64xf32>
          }
          vector.transfer_write %11, %subview_1[%arg2, %arg3] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        }
      }
      %subview_2 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_1 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %6 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %7 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %9 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %6 to %c512 step %7 {
    %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.for %arg1 = %8 to %c1024 step %9 {
      %subview_0 = memref.subview %2[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_1 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.for %arg2 = %c0 to %c256 step %c4 {
        scf.for %arg3 = %c0 to %c128 step %c64 {
          %10 = vector.transfer_read %subview_1[%arg2, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4x64xf32>
          %11 = scf.for %arg4 = %c0 to %c256 step %c2 iter_args(%arg5 = %10) -> (vector<4x64xf32>) {
            %12 = vector.transfer_read %subview[%arg2, %arg4], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4x2xf32>
            %13 = vector.transfer_read %subview_0[%arg4, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<2x64xf32>
            %14 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %12, %13, %arg5 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
            scf.yield %14 : vector<4x64xf32>
          }
          vector.transfer_write %11, %subview_1[%arg2, %arg3] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        }
      }
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_1 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_1 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %6 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %7 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %9 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %6 to %c512 step %7 {
    %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.for %arg1 = %8 to %c1024 step %9 {
      %subview_0 = memref.subview %2[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_1 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.for %arg2 = %c0 to %c256 step %c4 {
        scf.for %arg3 = %c0 to %c128 step %c64 {
          %10 = vector.transfer_read %subview_1[%arg2, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4x64xf32>
          %11 = scf.for %arg4 = %c0 to %c256 step %c2 iter_args(%arg5 = %10) -> (vector<4x64xf32>) {
            %12 = vector.transfer_read %subview[%arg2, %arg4], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4x2xf32>
            %13 = vector.transfer_read %subview_0[%arg4, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<2x64xf32>
            %14 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %12, %13, %arg5 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
            scf.yield %14 : vector<4x64xf32>
          }
          vector.transfer_write %11, %subview_1[%arg2, %arg3] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        }
      }
    }
  }
  return
}

// -----// IR Dump After CleanupBufferAllocView (iree-codegen-cleanup-buffer-alloc-view) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %subview_0 = memref.subview %1[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_1 = memref.subview %2[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.for %arg2 = %c0 to %c256 step %c4 {
        scf.for %arg3 = %c0 to %c128 step %c64 {
          %7 = vector.transfer_read %subview_1[%arg2, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4x64xf32>
          %8 = scf.for %arg4 = %c0 to %c256 step %c2 iter_args(%arg5 = %7) -> (vector<4x64xf32>) {
            %9 = vector.transfer_read %subview[%arg2, %arg4], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4x2xf32>
            %10 = vector.transfer_read %subview_0[%arg4, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<2x64xf32>
            %11 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %9, %10, %arg5 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
            scf.yield %11 : vector<4x64xf32>
          }
          vector.transfer_write %8, %subview_1[%arg2, %arg3] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        }
      }
    }
  }
  return
}

// -----// IR Dump After EraseHALDescriptorTypeFromMemRef (iree-codegen-erase-hal-descriptor-type-from-memref) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c512 = arith.constant 512 : index
  %c1024 = arith.constant 1024 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %subview_0 = memref.subview %1[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
      %subview_1 = memref.subview %2[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
      scf.for %arg2 = %c0 to %c256 step %c4 {
        scf.for %arg3 = %c0 to %c128 step %c64 {
          %7 = vector.transfer_read %subview_1[%arg2, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
          %8 = scf.for %arg4 = %c0 to %c256 step %c2 iter_args(%arg5 = %7) -> (vector<4x64xf32>) {
            %9 = vector.transfer_read %subview[%arg2, %arg4], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
            %10 = vector.transfer_read %subview_0[%arg4, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
            %11 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %9, %10, %arg5 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
            scf.yield %11 : vector<4x64xf32>
          }
          vector.transfer_write %8, %subview_1[%arg2, %arg3] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
        }
      }
    }
  }
  return
}

// -----// IR Dump After RemoveSingleIterationLoop (iree-codegen-remove-single-iteration-loop) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %7, %8, %arg3 : vector<4x2xf32>, vector<2x64xf32> into vector<4x64xf32>
        scf.yield %9 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<4x64xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %5) -> (vector<4x64xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<4x2xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<2x64xf32>
        %9 = vector.transpose %7, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %10 = vector.extract %9[0] : vector<2x4xf32>
        %11 = vector.extract %8[0] : vector<2x64xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %13 = vector.extract %9[1] : vector<2x4xf32>
        %14 = vector.extract %8[1] : vector<2x64xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %15 : vector<4x64xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<4x64xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x2xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %5 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %6 = vector.insert %5, %cst [0] : vector<64xf32> into vector<4x64xf32>
      %7 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
      %8 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %6 [1] : vector<64xf32> into vector<4x64xf32>
      %10 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
      %11 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %12 = vector.insert %11, %9 [2] : vector<64xf32> into vector<4x64xf32>
      %13 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
      %14 = vector.load %subview_2[%13, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %12 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %24 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %25 = vector.insert %24, %cst_0 [0] : vector<2xf32> into vector<4x2xf32>
        %26 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
        %27 = vector.load %subview[%26, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %28 = vector.insert %27, %25 [1] : vector<2xf32> into vector<4x2xf32>
        %29 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
        %30 = vector.load %subview[%29, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %31 = vector.insert %30, %28 [2] : vector<2xf32> into vector<4x2xf32>
        %32 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
        %33 = vector.load %subview[%32, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %34 = vector.insert %33, %31 [3] : vector<2xf32> into vector<4x2xf32>
        %35 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %36 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %37 = vector.load %subview_1[%36, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %38 = vector.transpose %34, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %39 = vector.extract %38[0] : vector<2x4xf32>
        %40 = vector.outerproduct %39, %35, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %41 = vector.extract %38[1] : vector<2x4xf32>
        %42 = vector.outerproduct %41, %37, %40 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %42 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
      %19 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %19, %subview_2[%18, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
      %21 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %21, %subview_2[%20, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %22 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
      %23 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %23, %subview_2[%22, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x2xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.insert %21, %cst_0 [0] : vector<2xf32> into vector<4x2xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.insert %23, %22 [1] : vector<2xf32> into vector<4x2xf32>
        %25 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %26 = vector.insert %25, %24 [2] : vector<2xf32> into vector<4x2xf32>
        %27 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %28 = vector.insert %27, %26 [3] : vector<2xf32> into vector<4x2xf32>
        %29 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %30 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %31 = vector.load %subview_1[%30, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %32 = vector.transpose %28, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %33 = vector.extract %32[0] : vector<2x4xf32>
        %34 = vector.outerproduct %33, %29, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %35 = vector.extract %32[1] : vector<2x4xf32>
        %36 = vector.outerproduct %35, %31, %34 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %36 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x2xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.insert %21, %cst_0 [0] : vector<2xf32> into vector<4x2xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.insert %23, %22 [1] : vector<2xf32> into vector<4x2xf32>
        %25 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %26 = vector.insert %25, %24 [2] : vector<2xf32> into vector<4x2xf32>
        %27 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %28 = vector.insert %27, %26 [3] : vector<2xf32> into vector<4x2xf32>
        %29 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %30 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %31 = vector.load %subview_1[%30, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %32 = vector.transpose %28, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %33 = vector.extract %32[0] : vector<2x4xf32>
        %34 = vector.outerproduct %33, %29, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %35 = vector.extract %32[1] : vector<2x4xf32>
        %36 = vector.outerproduct %35, %31, %34 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %36 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x2xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.insert %21, %cst_0 [0] : vector<2xf32> into vector<4x2xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.insert %23, %22 [1] : vector<2xf32> into vector<4x2xf32>
        %25 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %26 = vector.insert %25, %24 [2] : vector<2xf32> into vector<4x2xf32>
        %27 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %28 = vector.insert %27, %26 [3] : vector<2xf32> into vector<4x2xf32>
        %29 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %30 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %31 = vector.load %subview_1[%30, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %32 = vector.transpose %28, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %33 = vector.extract %32[0] : vector<2x4xf32>
        %34 = vector.outerproduct %33, %29, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %35 = vector.extract %32[1] : vector<2x4xf32>
        %36 = vector.outerproduct %35, %31, %34 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %36 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x2xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.insert %21, %cst_0 [0] : vector<2xf32> into vector<4x2xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.insert %23, %22 [1] : vector<2xf32> into vector<4x2xf32>
        %25 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %26 = vector.insert %25, %24 [2] : vector<2xf32> into vector<4x2xf32>
        %27 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %28 = vector.insert %27, %26 [3] : vector<2xf32> into vector<4x2xf32>
        %29 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %30 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %31 = vector.load %subview_1[%30, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %32 = vector.transpose %28, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %33 = vector.extract %32[0] : vector<2x4xf32>
        %34 = vector.outerproduct %33, %29, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %35 = vector.extract %32[1] : vector<2x4xf32>
        %36 = vector.outerproduct %35, %31, %34 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %36 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x2xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.insert %21, %cst_0 [0] : vector<2xf32> into vector<4x2xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.insert %23, %22 [1] : vector<2xf32> into vector<4x2xf32>
        %25 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %26 = vector.insert %25, %24 [2] : vector<2xf32> into vector<4x2xf32>
        %27 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %28 = vector.insert %27, %26 [3] : vector<2xf32> into vector<4x2xf32>
        %29 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %30 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %31 = vector.load %subview_1[%30, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %32 = vector.transpose %28, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %33 = vector.extract %32[0] : vector<2x4xf32>
        %34 = vector.outerproduct %33, %29, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %35 = vector.extract %32[1] : vector<2x4xf32>
        %36 = vector.outerproduct %35, %31, %34 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %36 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x2xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.insert %21, %cst_0 [0] : vector<2xf32> into vector<4x2xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.insert %23, %22 [1] : vector<2xf32> into vector<4x2xf32>
        %25 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %26 = vector.insert %25, %24 [2] : vector<2xf32> into vector<4x2xf32>
        %27 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %28 = vector.insert %27, %26 [3] : vector<2xf32> into vector<4x2xf32>
        %29 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %30 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %31 = vector.load %subview_1[%30, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %32 = vector.transpose %28, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %33 = vector.extract %32[0] : vector<2x4xf32>
        %34 = vector.outerproduct %33, %29, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %35 = vector.extract %32[1] : vector<2x4xf32>
        %36 = vector.outerproduct %35, %31, %34 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %36 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x2xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.insert %21, %cst_0 [0] : vector<2xf32> into vector<4x2xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.insert %23, %22 [1] : vector<2xf32> into vector<4x2xf32>
        %25 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %26 = vector.insert %25, %24 [2] : vector<2xf32> into vector<4x2xf32>
        %27 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %28 = vector.insert %27, %26 [3] : vector<2xf32> into vector<4x2xf32>
        %29 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %30 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %31 = vector.load %subview_1[%30, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %32 = vector.transpose %28, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %33 = vector.extract %32[0] : vector<2x4xf32>
        %34 = vector.outerproduct %33, %29, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %35 = vector.extract %32[1] : vector<2x4xf32>
        %36 = vector.outerproduct %35, %31, %34 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %36 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x2xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.insert %21, %cst_0 [0] : vector<2xf32> into vector<4x2xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.insert %23, %22 [1] : vector<2xf32> into vector<4x2xf32>
        %25 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %26 = vector.insert %25, %24 [2] : vector<2xf32> into vector<4x2xf32>
        %27 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %28 = vector.insert %27, %26 [3] : vector<2xf32> into vector<4x2xf32>
        %29 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %30 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %31 = vector.load %subview_1[%30, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %32 = vector.transpose %28, [1, 0] : vector<4x2xf32> to vector<2x4xf32>
        %33 = vector.extract %32[0] : vector<2x4xf32>
        %34 = vector.outerproduct %33, %29, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %35 = vector.extract %32[1] : vector<2x4xf32>
        %36 = vector.outerproduct %35, %31, %34 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %36 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x2xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.insert %21, %cst_0 [0] : vector<2xf32> into vector<4x2xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.insert %23, %22 [1] : vector<2xf32> into vector<4x2xf32>
        %25 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %26 = vector.insert %25, %24 [2] : vector<2xf32> into vector<4x2xf32>
        %27 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %28 = vector.insert %27, %26 [3] : vector<2xf32> into vector<4x2xf32>
        %29 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %30 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %31 = vector.load %subview_1[%30, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %32 = vector.shape_cast %28 : vector<4x2xf32> to vector<8xf32>
        %33 = vector.shuffle %32, %32 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
        %34 = vector.shape_cast %33 : vector<8xf32> to vector<2x4xf32>
        %35 = vector.extract %34[0] : vector<2x4xf32>
        %36 = vector.outerproduct %35, %29, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %37 = vector.extract %34[1] : vector<2x4xf32>
        %38 = vector.outerproduct %37, %31, %36 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %38 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x2xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.insert %21, %cst_0 [0] : vector<2xf32> into vector<4x2xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.insert %23, %22 [1] : vector<2xf32> into vector<4x2xf32>
        %25 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %26 = vector.insert %25, %24 [2] : vector<2xf32> into vector<4x2xf32>
        %27 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %28 = vector.insert %27, %26 [3] : vector<2xf32> into vector<4x2xf32>
        %29 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %30 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %31 = vector.load %subview_1[%30, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %32 = vector.shape_cast %28 : vector<4x2xf32> to vector<8xf32>
        %33 = vector.shuffle %32, %32 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
        %34 = vector.shape_cast %33 : vector<8xf32> to vector<2x4xf32>
        %35 = vector.extract %34[0] : vector<2x4xf32>
        %36 = vector.outerproduct %35, %29, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %37 = vector.extract %34[1] : vector<2x4xf32>
        %38 = vector.outerproduct %37, %31, %36 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %38 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x2xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.insert %21, %cst_0 [0] : vector<2xf32> into vector<4x2xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.insert %23, %22 [1] : vector<2xf32> into vector<4x2xf32>
        %25 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %26 = vector.insert %25, %24 [2] : vector<2xf32> into vector<4x2xf32>
        %27 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %28 = vector.insert %27, %26 [3] : vector<2xf32> into vector<4x2xf32>
        %29 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %30 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %31 = vector.load %subview_1[%30, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %32 = vector.shape_cast %28 : vector<4x2xf32> to vector<8xf32>
        %33 = vector.shuffle %32, %32 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
        %34 = vector.shape_cast %33 : vector<8xf32> to vector<2x4xf32>
        %35 = vector.extract %34[0] : vector<2x4xf32>
        %36 = vector.outerproduct %35, %29, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %37 = vector.extract %34[1] : vector<2x4xf32>
        %38 = vector.outerproduct %37, %31, %36 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %38 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x2xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.insert %21, %cst_0 [0] : vector<2xf32> into vector<4x2xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.insert %23, %22 [1] : vector<2xf32> into vector<4x2xf32>
        %25 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %26 = vector.insert %25, %24 [2] : vector<2xf32> into vector<4x2xf32>
        %27 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %28 = vector.insert %27, %26 [3] : vector<2xf32> into vector<4x2xf32>
        %29 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %30 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %31 = vector.load %subview_1[%30, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %32 = vector.shape_cast %28 : vector<4x2xf32> to vector<8xf32>
        %33 = vector.shuffle %32, %32 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
        %34 = vector.shape_cast %33 : vector<8xf32> to vector<2x4xf32>
        %35 = vector.extract %34[0] : vector<2x4xf32>
        %36 = vector.outerproduct %35, %29, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %37 = vector.extract %34[1] : vector<2x4xf32>
        %38 = vector.outerproduct %37, %31, %36 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %38 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x2xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.insert %21, %cst_0 [0] : vector<2xf32> into vector<4x2xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.insert %23, %22 [1] : vector<2xf32> into vector<4x2xf32>
        %25 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %26 = vector.insert %25, %24 [2] : vector<2xf32> into vector<4x2xf32>
        %27 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %28 = vector.insert %27, %26 [3] : vector<2xf32> into vector<4x2xf32>
        %29 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %30 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %31 = vector.load %subview_1[%30, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %32 = vector.shape_cast %28 : vector<4x2xf32> to vector<8xf32>
        %33 = vector.shuffle %32, %32 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
        %34 = vector.shape_cast %33 : vector<8xf32> to vector<2x4xf32>
        %35 = vector.extract %34[0] : vector<2x4xf32>
        %36 = vector.outerproduct %35, %29, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %37 = vector.extract %34[1] : vector<2x4xf32>
        %38 = vector.outerproduct %37, %31, %36 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %38 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x2xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.insert %21, %cst_0 [0] : vector<2xf32> into vector<4x2xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.insert %23, %22 [1] : vector<2xf32> into vector<4x2xf32>
        %25 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %26 = vector.insert %25, %24 [2] : vector<2xf32> into vector<4x2xf32>
        %27 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %28 = vector.insert %27, %26 [3] : vector<2xf32> into vector<4x2xf32>
        %29 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %30 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %31 = vector.load %subview_1[%30, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %32 = vector.shape_cast %28 : vector<4x2xf32> to vector<8xf32>
        %33 = vector.shuffle %32, %32 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
        %34 = vector.shape_cast %33 : vector<8xf32> to vector<2x4xf32>
        %35 = vector.extract %34[0] : vector<2x4xf32>
        %36 = vector.outerproduct %35, %29, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %37 = vector.extract %34[1] : vector<2x4xf32>
        %38 = vector.outerproduct %37, %31, %36 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %38 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x2xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.insert %21, %cst_0 [0] : vector<2xf32> into vector<4x2xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.insert %23, %22 [1] : vector<2xf32> into vector<4x2xf32>
        %25 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %26 = vector.insert %25, %24 [2] : vector<2xf32> into vector<4x2xf32>
        %27 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %28 = vector.insert %27, %26 [3] : vector<2xf32> into vector<4x2xf32>
        %29 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %30 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %31 = vector.load %subview_1[%30, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %32 = vector.shape_cast %28 : vector<4x2xf32> to vector<8xf32>
        %33 = vector.shuffle %32, %32 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
        %34 = vector.shape_cast %33 : vector<8xf32> to vector<2x4xf32>
        %35 = vector.extract %34[0] : vector<2x4xf32>
        %36 = vector.outerproduct %35, %29, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %37 = vector.extract %34[1] : vector<2x4xf32>
        %38 = vector.outerproduct %37, %31, %36 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %38 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x2xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.insert %21, %cst_0 [0] : vector<2xf32> into vector<4x2xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.insert %23, %22 [1] : vector<2xf32> into vector<4x2xf32>
        %25 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %26 = vector.insert %25, %24 [2] : vector<2xf32> into vector<4x2xf32>
        %27 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %28 = vector.insert %27, %26 [3] : vector<2xf32> into vector<4x2xf32>
        %29 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %30 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %31 = vector.load %subview_1[%30, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %32 = vector.shape_cast %28 : vector<4x2xf32> to vector<8xf32>
        %33 = vector.shuffle %32, %32 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
        %34 = vector.shape_cast %33 : vector<8xf32> to vector<2x4xf32>
        %35 = vector.extract %34[0] : vector<2x4xf32>
        %36 = vector.outerproduct %35, %29, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %37 = vector.extract %34[1] : vector<2x4xf32>
        %38 = vector.outerproduct %37, %31, %36 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %38 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x2xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.insert %21, %cst_0 [0] : vector<2xf32> into vector<4x2xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.insert %23, %22 [1] : vector<2xf32> into vector<4x2xf32>
        %25 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %26 = vector.insert %25, %24 [2] : vector<2xf32> into vector<4x2xf32>
        %27 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %28 = vector.insert %27, %26 [3] : vector<2xf32> into vector<4x2xf32>
        %29 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %30 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %31 = vector.load %subview_1[%30, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %32 = vector.shape_cast %28 : vector<4x2xf32> to vector<8xf32>
        %33 = vector.shuffle %32, %32 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
        %34 = vector.shape_cast %33 : vector<8xf32> to vector<2x4xf32>
        %35 = vector.extract %34[0] : vector<2x4xf32>
        %36 = vector.outerproduct %35, %29, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %37 = vector.extract %34[1] : vector<2x4xf32>
        %38 = vector.outerproduct %37, %31, %36 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %38 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst_0 [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %23 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %25 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %26 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %27 = vector.load %subview_1[%26, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %28 = vector.insert_strided_slice %21, %cst {offsets = [0], strides = [1]} : vector<2xf32> into vector<8xf32>
        %29 = vector.insert_strided_slice %22, %28 {offsets = [2], strides = [1]} : vector<2xf32> into vector<8xf32>
        %30 = vector.insert_strided_slice %23, %29 {offsets = [4], strides = [1]} : vector<2xf32> into vector<8xf32>
        %31 = vector.insert_strided_slice %24, %30 {offsets = [6], strides = [1]} : vector<2xf32> into vector<8xf32>
        %32 = vector.shuffle %31, %31 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
        %33 = vector.extract_strided_slice %32 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
        %34 = vector.extract_strided_slice %32 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
        %35 = vector.outerproduct %33, %25, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %36 = vector.outerproduct %34, %27, %35 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %36 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst_0 [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %23 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %25 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %26 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %27 = vector.load %subview_1[%26, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %28 = vector.insert_strided_slice %21, %cst {offsets = [0], strides = [1]} : vector<2xf32> into vector<8xf32>
        %29 = vector.insert_strided_slice %22, %28 {offsets = [2], strides = [1]} : vector<2xf32> into vector<8xf32>
        %30 = vector.insert_strided_slice %23, %29 {offsets = [4], strides = [1]} : vector<2xf32> into vector<8xf32>
        %31 = vector.insert_strided_slice %24, %30 {offsets = [6], strides = [1]} : vector<2xf32> into vector<8xf32>
        %32 = vector.shuffle %31, %31 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
        %33 = vector.extract_strided_slice %32 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
        %34 = vector.extract_strided_slice %32 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
        %35 = vector.outerproduct %33, %25, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %36 = vector.outerproduct %34, %27, %35 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %36 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst_0 [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %23 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %25 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %26 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %27 = vector.load %subview_1[%26, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %28 = vector.insert_strided_slice %21, %cst {offsets = [0], strides = [1]} : vector<2xf32> into vector<8xf32>
        %29 = vector.insert_strided_slice %22, %28 {offsets = [2], strides = [1]} : vector<2xf32> into vector<8xf32>
        %30 = vector.insert_strided_slice %23, %29 {offsets = [4], strides = [1]} : vector<2xf32> into vector<8xf32>
        %31 = vector.insert_strided_slice %24, %30 {offsets = [6], strides = [1]} : vector<2xf32> into vector<8xf32>
        %32 = vector.shuffle %31, %31 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
        %33 = vector.extract_strided_slice %32 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
        %34 = vector.extract_strided_slice %32 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
        %35 = vector.outerproduct %33, %25, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %36 = vector.outerproduct %34, %27, %35 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %36 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst_0 [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %23 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %25 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %26 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %27 = vector.load %subview_1[%26, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %28 = vector.insert_strided_slice %21, %cst {offsets = [0], strides = [1]} : vector<2xf32> into vector<8xf32>
        %29 = vector.insert_strided_slice %22, %28 {offsets = [2], strides = [1]} : vector<2xf32> into vector<8xf32>
        %30 = vector.insert_strided_slice %23, %29 {offsets = [4], strides = [1]} : vector<2xf32> into vector<8xf32>
        %31 = vector.insert_strided_slice %24, %30 {offsets = [6], strides = [1]} : vector<2xf32> into vector<8xf32>
        %32 = vector.shuffle %31, %31 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
        %33 = vector.extract_strided_slice %32 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
        %34 = vector.extract_strided_slice %32 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
        %35 = vector.outerproduct %33, %25, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %36 = vector.outerproduct %34, %27, %35 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %36 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst_0 [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %23 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %25 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %26 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %27 = vector.load %subview_1[%26, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %28 = vector.insert_strided_slice %21, %cst {offsets = [0], strides = [1]} : vector<2xf32> into vector<8xf32>
        %29 = vector.insert_strided_slice %22, %28 {offsets = [2], strides = [1]} : vector<2xf32> into vector<8xf32>
        %30 = vector.insert_strided_slice %23, %29 {offsets = [4], strides = [1]} : vector<2xf32> into vector<8xf32>
        %31 = vector.insert_strided_slice %24, %30 {offsets = [6], strides = [1]} : vector<2xf32> into vector<8xf32>
        %32 = vector.shuffle %31, %31 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
        %33 = vector.extract_strided_slice %32 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
        %34 = vector.extract_strided_slice %32 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
        %35 = vector.outerproduct %33, %25, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %36 = vector.outerproduct %34, %27, %35 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %36 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst_0 [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %23 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %25 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %26 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %27 = vector.load %subview_1[%26, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %28 = vector.insert_strided_slice %21, %cst {offsets = [0], strides = [1]} : vector<2xf32> into vector<8xf32>
        %29 = vector.insert_strided_slice %22, %28 {offsets = [2], strides = [1]} : vector<2xf32> into vector<8xf32>
        %30 = vector.insert_strided_slice %23, %29 {offsets = [4], strides = [1]} : vector<2xf32> into vector<8xf32>
        %31 = vector.insert_strided_slice %24, %30 {offsets = [6], strides = [1]} : vector<2xf32> into vector<8xf32>
        %32 = vector.shuffle %31, %31 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
        %33 = vector.extract_strided_slice %32 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
        %34 = vector.extract_strided_slice %32 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
        %35 = vector.outerproduct %33, %25, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %36 = vector.outerproduct %34, %27, %35 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %36 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst_0 [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %23 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %25 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %26 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %27 = vector.load %subview_1[%26, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %28 = vector.insert_strided_slice %21, %cst {offsets = [0], strides = [1]} : vector<2xf32> into vector<8xf32>
        %29 = vector.insert_strided_slice %22, %28 {offsets = [2], strides = [1]} : vector<2xf32> into vector<8xf32>
        %30 = vector.insert_strided_slice %23, %29 {offsets = [4], strides = [1]} : vector<2xf32> into vector<8xf32>
        %31 = vector.insert_strided_slice %24, %30 {offsets = [6], strides = [1]} : vector<2xf32> into vector<8xf32>
        %32 = vector.shuffle %31, %31 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
        %33 = vector.extract_strided_slice %32 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
        %34 = vector.extract_strided_slice %32 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
        %35 = vector.outerproduct %33, %25, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %36 = vector.outerproduct %34, %27, %35 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %36 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst_0 [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %23 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %25 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %26 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %27 = vector.load %subview_1[%26, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %28 = vector.insert_strided_slice %21, %cst {offsets = [0], strides = [1]} : vector<2xf32> into vector<8xf32>
        %29 = vector.insert_strided_slice %22, %28 {offsets = [2], strides = [1]} : vector<2xf32> into vector<8xf32>
        %30 = vector.insert_strided_slice %23, %29 {offsets = [4], strides = [1]} : vector<2xf32> into vector<8xf32>
        %31 = vector.insert_strided_slice %24, %30 {offsets = [6], strides = [1]} : vector<2xf32> into vector<8xf32>
        %32 = vector.shuffle %31, %31 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
        %33 = vector.extract_strided_slice %32 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
        %34 = vector.extract_strided_slice %32 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
        %35 = vector.outerproduct %33, %25, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %36 = vector.outerproduct %34, %27, %35 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %36 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<4x64xf32>
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c4 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c64 {
      %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %9 = vector.insert %8, %cst_0 [0] : vector<64xf32> into vector<4x64xf32>
      %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
      %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
      %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
      %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
        %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %22 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %23 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %24 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
        %25 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %26 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %27 = vector.load %subview_1[%26, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        %28 = vector.insert_strided_slice %21, %cst {offsets = [0], strides = [1]} : vector<2xf32> into vector<8xf32>
        %29 = vector.insert_strided_slice %22, %28 {offsets = [2], strides = [1]} : vector<2xf32> into vector<8xf32>
        %30 = vector.insert_strided_slice %23, %29 {offsets = [4], strides = [1]} : vector<2xf32> into vector<8xf32>
        %31 = vector.insert_strided_slice %24, %30 {offsets = [6], strides = [1]} : vector<2xf32> into vector<8xf32>
        %32 = vector.shuffle %31, %31 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
        %33 = vector.extract_strided_slice %32 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
        %34 = vector.extract_strided_slice %32 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
        %35 = vector.outerproduct %33, %25, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        %36 = vector.outerproduct %34, %27, %35 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
        scf.yield %36 : vector<4x64xf32>
      }
      %17 = vector.extract %16[0] : vector<4x64xf32>
      vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %18 = vector.extract %16[1] : vector<4x64xf32>
      vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %19 = vector.extract %16[2] : vector<4x64xf32>
      vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
      %20 = vector.extract %16[3] : vector<4x64xf32>
      vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
    }
  }
  return
}

// -----// IR Dump After LLVMCPULowerExecutableTarget (iree-llvmcpu-lower-executable-target) //----- //
hal.executable.variant public @embedded_elf_x86_64, target = <"llvm-cpu", "embedded-elf-x86_64", {cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", native_vector_size = 16 : index, target_triple = "x86_64-unknown-unknown-eabi-elf"}> {
  hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#hal.pipeline.layout<push_constants = 6, sets = [<0, bindings = [<0, storage_buffer>, <1, storage_buffer>, <2, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingPadExpert>} {
  ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index, %arg4: index):
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    hal.return %c8, %c2, %c1 : index, index, index
  }
  builtin.module {
    func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
      %cst = arith.constant dense<0.000000e+00> : vector<8xf32>
      %cst_0 = arith.constant dense<0.000000e+00> : vector<4x64xf32>
      %c2 = arith.constant 2 : index
      %c4 = arith.constant 4 : index
      %c64 = arith.constant 64 : index
      %c256 = arith.constant 256 : index
      %c128 = arith.constant 128 : index
      %c0 = arith.constant 0 : index
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
      memref.assume_alignment %0, 64 : memref<512x256xf32>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
      memref.assume_alignment %1, 64 : memref<256x1024xf32>
      %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
      memref.assume_alignment %2, 64 : memref<512x1024xf32>
      %workgroup_id_x = hal.interface.workgroup.id[0] : index
      %workgroup_id_y = hal.interface.workgroup.id[1] : index
      %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
      %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
      %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
      %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
      %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
      scf.for %arg0 = %c0 to %c256 step %c4 {
        %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
        %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
        %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
        scf.for %arg1 = %c0 to %c128 step %c64 {
          %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
          %9 = vector.insert %8, %cst_0 [0] : vector<64xf32> into vector<4x64xf32>
          %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
          %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
          %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
          %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
          %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
          %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
          %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
            %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
            %22 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
            %23 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
            %24 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
            %25 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
            %26 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
            %27 = vector.load %subview_1[%26, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
            %28 = vector.insert_strided_slice %21, %cst {offsets = [0], strides = [1]} : vector<2xf32> into vector<8xf32>
            %29 = vector.insert_strided_slice %22, %28 {offsets = [2], strides = [1]} : vector<2xf32> into vector<8xf32>
            %30 = vector.insert_strided_slice %23, %29 {offsets = [4], strides = [1]} : vector<2xf32> into vector<8xf32>
            %31 = vector.insert_strided_slice %24, %30 {offsets = [6], strides = [1]} : vector<2xf32> into vector<8xf32>
            %32 = vector.shuffle %31, %31 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
            %33 = vector.extract_strided_slice %32 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
            %34 = vector.extract_strided_slice %32 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
            %35 = vector.outerproduct %33, %25, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
            %36 = vector.outerproduct %34, %27, %35 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
            scf.yield %36 : vector<4x64xf32>
          }
          %17 = vector.extract %16[0] : vector<4x64xf32>
          vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
          %18 = vector.extract %16[1] : vector<4x64xf32>
          vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
          %19 = vector.extract %16[2] : vector<4x64xf32>
          vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
          %20 = vector.extract %16[3] : vector<4x64xf32>
          vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
        }
      }
      return
    }
  }
}

#executable_target_embedded_elf_x86_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", native_vector_size = 16 : index, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<()[s0] -> (s0 * 256)>
#map1 = affine_map<()[s0] -> (s0 * 128)>
#map2 = affine_map<(d0) -> (d0 + 1)>
#map3 = affine_map<(d0) -> (d0 + 2)>
#map4 = affine_map<(d0) -> (d0 + 3)>
#pipeline_layout = #hal.pipeline.layout<push_constants = 6, sets = [<0, bindings = [<0, storage_buffer>, <1, storage_buffer>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingPadExpert>
module {
  hal.executable private @matmul_static_dispatch_0_matmul_512x1024x256 {
    hal.executable.variant public @embedded_elf_x86_64, target = #executable_target_embedded_elf_x86_64_ {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index, %arg4: index):
        %c8 = arith.constant 8 : index
        %c2 = arith.constant 2 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c2, %c1 : index, index, index
      }
      builtin.module {
        func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
          %cst = arith.constant dense<0.000000e+00> : vector<8xf32>
          %cst_0 = arith.constant dense<0.000000e+00> : vector<4x64xf32>
          %c2 = arith.constant 2 : index
          %c4 = arith.constant 4 : index
          %c64 = arith.constant 64 : index
          %c256 = arith.constant 256 : index
          %c128 = arith.constant 128 : index
          %c0 = arith.constant 0 : index
          %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) offset(%c0) alignment(64) : memref<512x256xf32>
          memref.assume_alignment %0, 64 : memref<512x256xf32>
          %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) offset(%c0) alignment(64) : memref<256x1024xf32>
          memref.assume_alignment %1, 64 : memref<256x1024xf32>
          %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) offset(%c0) alignment(64) : memref<512x1024xf32>
          memref.assume_alignment %2, 64 : memref<512x1024xf32>
          %workgroup_id_x = hal.interface.workgroup.id[0] : index
          %workgroup_id_y = hal.interface.workgroup.id[1] : index
          %3 = affine.apply #map()[%workgroup_id_y]
          %4 = affine.apply #map1()[%workgroup_id_x]
          %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
          %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
          %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
          scf.for %arg0 = %c0 to %c256 step %c4 {
            %5 = affine.apply #map2(%arg0)
            %6 = affine.apply #map3(%arg0)
            %7 = affine.apply #map4(%arg0)
            scf.for %arg1 = %c0 to %c128 step %c64 {
              %8 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
              %9 = vector.insert %8, %cst_0 [0] : vector<64xf32> into vector<4x64xf32>
              %10 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
              %11 = vector.insert %10, %9 [1] : vector<64xf32> into vector<4x64xf32>
              %12 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
              %13 = vector.insert %12, %11 [2] : vector<64xf32> into vector<4x64xf32>
              %14 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
              %15 = vector.insert %14, %13 [3] : vector<64xf32> into vector<4x64xf32>
              %16 = scf.for %arg2 = %c0 to %c256 step %c2 iter_args(%arg3 = %15) -> (vector<4x64xf32>) {
                %21 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
                %22 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
                %23 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
                %24 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<2xf32>
                %25 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
                %26 = affine.apply #map2(%arg2)
                %27 = vector.load %subview_1[%26, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
                %28 = vector.insert_strided_slice %21, %cst {offsets = [0], strides = [1]} : vector<2xf32> into vector<8xf32>
                %29 = vector.insert_strided_slice %22, %28 {offsets = [2], strides = [1]} : vector<2xf32> into vector<8xf32>
                %30 = vector.insert_strided_slice %23, %29 {offsets = [4], strides = [1]} : vector<2xf32> into vector<8xf32>
                %31 = vector.insert_strided_slice %24, %30 {offsets = [6], strides = [1]} : vector<2xf32> into vector<8xf32>
                %32 = vector.shuffle %31, %31 [0, 2, 4, 6, 1, 3, 5, 7] : vector<8xf32>, vector<8xf32>
                %33 = vector.extract_strided_slice %32 {offsets = [0], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
                %34 = vector.extract_strided_slice %32 {offsets = [4], sizes = [4], strides = [1]} : vector<8xf32> to vector<4xf32>
                %35 = vector.outerproduct %33, %25, %arg3 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
                %36 = vector.outerproduct %34, %27, %35 {kind = #vector.kind<add>} : vector<4xf32>, vector<64xf32>
                scf.yield %36 : vector<4x64xf32>
              }
              %17 = vector.extract %16[0] : vector<4x64xf32>
              vector.store %17, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
              %18 = vector.extract %16[1] : vector<4x64xf32>
              vector.store %18, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
              %19 = vector.extract %16[2] : vector<4x64xf32>
              vector.store %19, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
              %20 = vector.extract %16[3] : vector<4x64xf32>
              vector.store %20, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<64xf32>
            }
          }
          return
        }
      }
    }
  }
}

