// -----// IR Dump After StablehloLegalizeToHloPass (stablehlo-legalize-to-hlo) //----- //
module {
  func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
    %0 = mhlo.add %arg0, %arg1 : tensor<512x256xf32>
    %1 = mhlo.add %0, %arg2 : tensor<512x256xf32>
    return %1 : tensor<512x256xf32>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
  %0 = mhlo.add %arg0, %arg1 : tensor<512x256xf32>
  %1 = mhlo.add %0, %arg2 : tensor<512x256xf32>
  return %1 : tensor<512x256xf32>
}

// -----// IR Dump After LegalizeControlFlowPass (mhlo-legalize-control-flow) //----- //
func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
  %0 = mhlo.add %arg0, %arg1 : tensor<512x256xf32>
  %1 = mhlo.add %0, %arg2 : tensor<512x256xf32>
  return %1 : tensor<512x256xf32>
}

// -----// IR Dump After TopLevelSCFToCFG (iree-top-level-scf-to-cfg) //----- //
func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
  %0 = mhlo.add %arg0, %arg1 : tensor<512x256xf32>
  %1 = mhlo.add %0, %arg2 : tensor<512x256xf32>
  return %1 : tensor<512x256xf32>
}

// -----// IR Dump After MHLOToMHLOPreprocessing (iree-mhlo-to-mhlo-preprocessing) //----- //
func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
  %0 = mhlo.add %arg0, %arg1 : tensor<512x256xf32>
  %1 = mhlo.add %0, %arg2 : tensor<512x256xf32>
  return %1 : tensor<512x256xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
  %0 = mhlo.add %arg0, %arg1 : tensor<512x256xf32>
  %1 = mhlo.add %0, %arg2 : tensor<512x256xf32>
  return %1 : tensor<512x256xf32>
}

// -----// IR Dump After ShapeToShapeLowering (shape-to-shape-lowering) //----- //
func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
  %0 = mhlo.add %arg0, %arg1 : tensor<512x256xf32>
  %1 = mhlo.add %0, %arg2 : tensor<512x256xf32>
  return %1 : tensor<512x256xf32>
}

// -----// IR Dump After ConvertShapeToStandard (convert-shape-to-std) //----- //
module {
  func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
    %0 = mhlo.add %arg0, %arg1 : tensor<512x256xf32>
    %1 = mhlo.add %0, %arg2 : tensor<512x256xf32>
    return %1 : tensor<512x256xf32>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
  %0 = mhlo.add %arg0, %arg1 : tensor<512x256xf32>
  %1 = mhlo.add %0, %arg2 : tensor<512x256xf32>
  return %1 : tensor<512x256xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
  %0 = mhlo.add %arg0, %arg1 : tensor<512x256xf32>
  %1 = mhlo.add %0, %arg2 : tensor<512x256xf32>
  return %1 : tensor<512x256xf32>
}

// -----// IR Dump After Inliner (inline) //----- //
module {
  func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
    %0 = mhlo.add %arg0, %arg1 : tensor<512x256xf32>
    %1 = mhlo.add %0, %arg2 : tensor<512x256xf32>
    return %1 : tensor<512x256xf32>
  }
}


// -----// IR Dump After DemoteI64ToI32 (iree-util-demote-i64-to-i32) //----- //
module {
  func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
    %0 = mhlo.add %arg0, %arg1 : tensor<512x256xf32>
    %1 = mhlo.add %0, %arg2 : tensor<512x256xf32>
    return %1 : tensor<512x256xf32>
  }
}


// -----// IR Dump After DemoteF64ToF32 (iree-util-demote-f64-to-f32) //----- //
module {
  func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
    %0 = mhlo.add %arg0, %arg1 : tensor<512x256xf32>
    %1 = mhlo.add %0, %arg2 : tensor<512x256xf32>
    return %1 : tensor<512x256xf32>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
  %0 = mhlo.add %arg0, %arg1 : tensor<512x256xf32>
  %1 = mhlo.add %0, %arg2 : tensor<512x256xf32>
  return %1 : tensor<512x256xf32>
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
  %0 = mhlo.add %arg0, %arg1 : tensor<512x256xf32>
  %1 = mhlo.add %0, %arg2 : tensor<512x256xf32>
  return %1 : tensor<512x256xf32>
}

// -----// IR Dump After HloLegalizeShapeComputationsPass (hlo-legalize-shape-computations) //----- //
func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
  %0 = mhlo.add %arg0, %arg1 : tensor<512x256xf32>
  %1 = mhlo.add %0, %arg2 : tensor<512x256xf32>
  return %1 : tensor<512x256xf32>
}

// -----// IR Dump After ConvertMHLOToLinalgExt (iree-mhlo-to-linalg-ext) //----- //
func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
  %0 = mhlo.add %arg0, %arg1 : tensor<512x256xf32>
  %1 = mhlo.add %0, %arg2 : tensor<512x256xf32>
  return %1 : tensor<512x256xf32>
}

// -----// IR Dump After ConvertMHLOToLinalgOnTensors (iree-mhlo-to-linalg-on-tensors) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module {
  func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
    %0 = tensor.empty() : tensor<512x256xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%0 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %4 = arith.addf %in, %in_0 : f32
      linalg.yield %4 : f32
    } -> tensor<512x256xf32>
    %2 = tensor.empty() : tensor<512x256xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%1, %arg2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%2 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %4 = arith.addf %in, %in_0 : f32
      linalg.yield %4 : f32
    } -> tensor<512x256xf32>
    return %3 : tensor<512x256xf32>
  }
}


// -----// IR Dump After ReconcileUnrealizedCasts (reconcile-unrealized-casts) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module {
  func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
    %0 = tensor.empty() : tensor<512x256xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%0 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %4 = arith.addf %in, %in_0 : f32
      linalg.yield %4 : f32
    } -> tensor<512x256xf32>
    %2 = tensor.empty() : tensor<512x256xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%1, %arg2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%2 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %4 = arith.addf %in, %in_0 : f32
      linalg.yield %4 : f32
    } -> tensor<512x256xf32>
    return %3 : tensor<512x256xf32>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
  %0 = tensor.empty() : tensor<512x256xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%0 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %4 = arith.addf %in, %in_0 : f32
    linalg.yield %4 : f32
  } -> tensor<512x256xf32>
  %2 = tensor.empty() : tensor<512x256xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%1, %arg2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%2 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %4 = arith.addf %in, %in_0 : f32
    linalg.yield %4 : f32
  } -> tensor<512x256xf32>
  return %3 : tensor<512x256xf32>
}

// -----// IR Dump After VerifyCompilerMHLOInputLegality (iree-mhlo-verify-compiler-input-legality) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module {
  func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
    %0 = tensor.empty() : tensor<512x256xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%0 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %4 = arith.addf %in, %in_0 : f32
      linalg.yield %4 : f32
    } -> tensor<512x256xf32>
    %2 = tensor.empty() : tensor<512x256xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%1, %arg2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%2 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %4 = arith.addf %in, %in_0 : f32
      linalg.yield %4 : f32
    } -> tensor<512x256xf32>
    return %3 : tensor<512x256xf32>
  }
}


// -----// IR Dump After IREEImportPublic (iree-import-public) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module {
  func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
    %0 = tensor.empty() : tensor<512x256xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%0 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %4 = arith.addf %in, %in_0 : f32
      linalg.yield %4 : f32
    } -> tensor<512x256xf32>
    %2 = tensor.empty() : tensor<512x256xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%1, %arg2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%2 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %4 = arith.addf %in, %in_0 : f32
      linalg.yield %4 : f32
    } -> tensor<512x256xf32>
    return %3 : tensor<512x256xf32>
  }
}


// -----// IR Dump After ImportMLProgram (iree-import-ml-program) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module {
  func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
    %0 = tensor.empty() : tensor<512x256xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%0 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %4 = arith.addf %in, %in_0 : f32
      linalg.yield %4 : f32
    } -> tensor<512x256xf32>
    %2 = tensor.empty() : tensor<512x256xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%1, %arg2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%2 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %4 = arith.addf %in, %in_0 : f32
      linalg.yield %4 : f32
    } -> tensor<512x256xf32>
    return %3 : tensor<512x256xf32>
  }
}


// -----// IR Dump After SanitizeModuleNames (iree-sanitize-module-names) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module {
  func.func @twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
    %0 = tensor.empty() : tensor<512x256xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%0 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %4 = arith.addf %in, %in_0 : f32
      linalg.yield %4 : f32
    } -> tensor<512x256xf32>
    %2 = tensor.empty() : tensor<512x256xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%1, %arg2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%2 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %4 = arith.addf %in, %in_0 : f32
      linalg.yield %4 : f32
    } -> tensor<512x256xf32>
    return %3 : tensor<512x256xf32>
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::ABI::WrapEntryPointsPass (iree-abi-wrap-entry-points) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module {
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = call @_twoadd_static(%0, %1, %2) : (tensor<512x256xf32>, tensor<512x256xf32>, tensor<512x256xf32>) -> tensor<512x256xf32>
    %4 = hal.tensor.export %3 : tensor<512x256xf32> -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
  func.func private @_twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
    %0 = tensor.empty() : tensor<512x256xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%0 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %4 = arith.addf %in, %in_0 : f32
      linalg.yield %4 : f32
    } -> tensor<512x256xf32>
    %2 = tensor.empty() : tensor<512x256xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%1, %arg2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%2 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %4 = arith.addf %in, %in_0 : f32
      linalg.yield %4 : f32
    } -> tensor<512x256xf32>
    return %3 : tensor<512x256xf32>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_twoadd_static(%arg0: tensor<512x256xf32>, %arg1: tensor<512x256xf32>, %arg2: tensor<512x256xf32>) -> tensor<512x256xf32> {
  %0 = tensor.empty() : tensor<512x256xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0, %arg1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%0 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %4 = arith.addf %in, %in_0 : f32
    linalg.yield %4 : f32
  } -> tensor<512x256xf32>
  %2 = tensor.empty() : tensor<512x256xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%1, %arg2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%2 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %4 = arith.addf %in, %in_0 : f32
    linalg.yield %4 : f32
  } -> tensor<512x256xf32>
  return %3 : tensor<512x256xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = call @_twoadd_static(%0, %1, %2) : (tensor<512x256xf32>, tensor<512x256xf32>, tensor<512x256xf32>) -> tensor<512x256xf32>
  %4 = hal.tensor.export %3 : tensor<512x256xf32> -> !hal.buffer_view
  return %4 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %8 = arith.addf %in, %in_0 : f32
    linalg.yield %8 : f32
  } -> tensor<512x256xf32>
  %5 = tensor.empty() : tensor<512x256xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%5 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %8 = arith.addf %in, %in_0 : f32
    linalg.yield %8 : f32
  } -> tensor<512x256xf32>
  %7 = hal.tensor.export %6 : tensor<512x256xf32> -> !hal.buffer_view
  return %7 : !hal.buffer_view
}

// -----// IR Dump After Inliner (inline) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module {
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = tensor.empty() : tensor<512x256xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %8 = arith.addf %in, %in_0 : f32
      linalg.yield %8 : f32
    } -> tensor<512x256xf32>
    %5 = tensor.empty() : tensor<512x256xf32>
    %6 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%5 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %8 = arith.addf %in, %in_0 : f32
      linalg.yield %8 : f32
    } -> tensor<512x256xf32>
    %7 = hal.tensor.export %6 : tensor<512x256xf32> -> !hal.buffer_view
    return %7 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %8 = arith.addf %in, %in_0 : f32
    linalg.yield %8 : f32
  } -> tensor<512x256xf32>
  %5 = tensor.empty() : tensor<512x256xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%5 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %8 = arith.addf %in, %in_0 : f32
    linalg.yield %8 : f32
  } -> tensor<512x256xf32>
  %7 = hal.tensor.export %6 : tensor<512x256xf32> -> !hal.buffer_view
  return %7 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module {
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = tensor.empty() : tensor<512x256xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<512x256xf32>
    %5 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<512x256xf32>
    %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After DemoteF64ToF32 (iree-util-demote-f64-to-f32) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module {
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = tensor.empty() : tensor<512x256xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<512x256xf32>
    %5 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<512x256xf32>
    %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After DetachElementwiseFromNamedOps (iree-flow-detach-elementwise-from-named-ops) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After LinalgNamedOpConversion (linalg-named-op-conversion) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After Convert1X1FilterConv2DToMatmul (iree-flow-convert-1x1-filter-conv2d-to-matmul) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After VerifyInputLegality (iree-verify-input-legality) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module {
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = tensor.empty() : tensor<512x256xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<512x256xf32>
    %5 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<512x256xf32>
    %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After ExpandTensorShapes (iree-flow-expand-tensor-shapes) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module {
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = tensor.empty() : tensor<512x256xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<512x256xf32>
    %5 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<512x256xf32>
    %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module attributes {iree.fixedpoint.iteration = 0 : index} {
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = tensor.empty() : tensor<512x256xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<512x256xf32>
    %5 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<512x256xf32>
    %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module attributes {iree.fixedpoint.iteration = 0 : index} {
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = tensor.empty() : tensor<512x256xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<512x256xf32>
    %5 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<512x256xf32>
    %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module attributes {iree.fixedpoint.iteration = 0 : index} {
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = tensor.empty() : tensor<512x256xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<512x256xf32>
    %5 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<512x256xf32>
    %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module {
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = tensor.empty() : tensor<512x256xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<512x256xf32>
    %5 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<512x256xf32>
    %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After TensorPadToTensorInsertSlice (iree-flow-tensor-pad-to-tensor-insert-slice) //----- //
#map = affine_map<(d0, d1) -> (d0, d1)>
module {
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = tensor.empty() : tensor<512x256xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<512x256xf32>
    %5 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %7 = arith.addf %in, %in_0 : f32
      linalg.yield %7 : f32
    } -> tensor<512x256xf32>
    %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After ConvertElementwiseToLinalg (convert-elementwise-to-linalg) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After LinalgFoldUnitExtentDims (linalg-fold-unit-extent-dims) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After InterchangeGenericOps (iree-flow-interchange-generic-ops) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %2 : tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %out: f32):
    %7 = arith.addf %in, %in_0 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %6 = hal.tensor.export %5 : tensor<512x256xf32> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After FusionOfTensorOps (iree-flow-fusion-of-tensor-ops) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1, %2 : tensor<512x256xf32>, tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
    %6 = arith.addf %in, %in_0 : f32
    %7 = arith.addf %6, %in_1 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = hal.tensor.export %4 : tensor<512x256xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After LinalgDetensorize (linalg-detensorize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1, %2 : tensor<512x256xf32>, tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
    %6 = arith.addf %in, %in_0 : f32
    %7 = arith.addf %6, %in_1 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = hal.tensor.export %4 : tensor<512x256xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1, %2 : tensor<512x256xf32>, tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
    %6 = arith.addf %in, %in_0 : f32
    %7 = arith.addf %6, %in_1 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = hal.tensor.export %4 : tensor<512x256xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1, %2 : tensor<512x256xf32>, tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
    %6 = arith.addf %in, %in_0 : f32
    %7 = arith.addf %6, %in_1 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = hal.tensor.export %4 : tensor<512x256xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After CollapseDims (iree-flow-collapse-dims) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1, %2 : tensor<512x256xf32>, tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
    %6 = arith.addf %in, %in_0 : f32
    %7 = arith.addf %6, %in_1 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = hal.tensor.export %4 : tensor<512x256xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After SplitReduction (iree-flow-split-reduction-ops) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1, %2 : tensor<512x256xf32>, tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
    %6 = arith.addf %in, %in_0 : f32
    %7 = arith.addf %6, %in_1 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = hal.tensor.export %4 : tensor<512x256xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After InterchangeGenericOps (iree-flow-interchange-generic-ops) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1, %2 : tensor<512x256xf32>, tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
  ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
    %6 = arith.addf %in, %in_0 : f32
    %7 = arith.addf %6, %in_1 : f32
    linalg.yield %7 : f32
  } -> tensor<512x256xf32>
  %5 = hal.tensor.export %4 : tensor<512x256xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After FormDispatchRegions (iree-flow-form-dispatch-regions) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = tensor.empty() : tensor<512x256xf32>
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c512 = arith.constant 512 : index
  %c1_0 = arith.constant 1 : index
  %4 = affine.apply affine_map<()[s0, s1, s2] -> ((s1 - s0) ceildiv s2)>()[%c0, %c512, %c1_0]
  %c0_1 = arith.constant 0 : index
  %c256 = arith.constant 256 : index
  %c1_2 = arith.constant 1 : index
  %5 = affine.apply affine_map<()[s0, s1, s2] -> ((s1 - s0) ceildiv s2)>()[%c0_1, %c256, %c1_2]
  %6 = flow.dispatch.region[%4, %5] -> (tensor<512x256xf32>) {
    %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0, %1, %2 : tensor<512x256xf32>, tensor<512x256xf32>, tensor<512x256xf32>) outs(%3 : tensor<512x256xf32>) {
    ^bb0(%in: f32, %in_3: f32, %in_4: f32, %out: f32):
      %9 = arith.addf %in, %in_3 : f32
      %10 = arith.addf %9, %in_4 : f32
      linalg.yield %10 : f32
    } -> tensor<512x256xf32>
    flow.return %8 : tensor<512x256xf32>
  } count(%arg3: index, %arg4: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg3, %arg4
    flow.return %x, %y, %z : index, index, index
  }
  %7 = hal.tensor.export %6 : tensor<512x256xf32> -> !hal.buffer_view
  return %7 : !hal.buffer_view
}

// -----// IR Dump After CollapseDimensions (iree-flow-collapse-dimensions) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c131072 = arith.constant 131072 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %collapsed = tensor.collapse_shape %0 [[0, 1]] : tensor<512x256xf32> into tensor<131072xf32>
  %collapsed_0 = tensor.collapse_shape %1 [[0, 1]] : tensor<512x256xf32> into tensor<131072xf32>
  %collapsed_1 = tensor.collapse_shape %2 [[0, 1]] : tensor<512x256xf32> into tensor<131072xf32>
  %3 = tensor.empty() : tensor<131072xf32>
  %4 = flow.dispatch.region[%c131072] -> (tensor<131072xf32>) {
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed, %collapsed_0, %collapsed_1 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%3 : tensor<131072xf32>) {
    ^bb0(%in: f32, %in_2: f32, %in_3: f32, %out: f32):
      %7 = arith.addf %in, %in_2 : f32
      %8 = arith.addf %7, %in_3 : f32
      linalg.yield %8 : f32
    } -> tensor<131072xf32>
    flow.return %6 : tensor<131072xf32>
  } count(%arg3: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg3
    flow.return %x, %y, %z : index, index, index
  }
  %expanded = tensor.expand_shape %4 [[0, 1]] : tensor<131072xf32> into tensor<512x256xf32>
  %5 = hal.tensor.export %expanded : tensor<512x256xf32> -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After FormDispatchWorkgroups (iree-flow-form-dispatch-workgroups) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c131072 = arith.constant 131072 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
  %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
  %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
  %6 = flow.dispatch.workgroups[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32> =
      (%arg3: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg4: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg5: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg6: !flow.dispatch.tensor<writeonly:tensor<131072xf32>>) {
    %9 = flow.dispatch.tensor.load %arg3, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
    %10 = flow.dispatch.tensor.load %arg4, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
    %11 = flow.dispatch.tensor.load %arg5, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
    %12 = tensor.empty() : tensor<131072xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%9, %10, %11 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%12 : tensor<131072xf32>) {
    ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
      %14 = arith.addf %in, %in_0 : f32
      %15 = arith.addf %14, %in_1 : f32
      linalg.yield %15 : f32
    } -> tensor<131072xf32>
    flow.dispatch.tensor.store %13, %arg6, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    flow.return
  } count(%arg3: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg3
    flow.return %x, %y, %z : index, index, index
  }
  %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
  %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After CaptureDispatchDynamicDims (iree-flow-capture-dispatch-dynamic-dims) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c131072 = arith.constant 131072 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
  %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
  %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
  %6 = flow.dispatch.workgroups[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32> =
      (%arg3: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg4: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg5: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg6: !flow.dispatch.tensor<writeonly:tensor<131072xf32>>) {
    %9 = flow.dispatch.tensor.load %arg3, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
    %10 = flow.dispatch.tensor.load %arg4, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
    %11 = flow.dispatch.tensor.load %arg5, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
    %12 = tensor.empty() : tensor<131072xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%9, %10, %11 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%12 : tensor<131072xf32>) {
    ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
      %14 = arith.addf %in, %in_0 : f32
      %15 = arith.addf %14, %in_1 : f32
      linalg.yield %15 : f32
    } -> tensor<131072xf32>
    flow.dispatch.tensor.store %13, %arg6, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    flow.return
  } count(%arg3: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg3
    flow.return %x, %y, %z : index, index, index
  }
  %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
  %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c131072 = arith.constant 131072 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
  %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
  %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
  %6 = flow.dispatch.workgroups[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32> =
      (%arg3: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg4: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg5: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg6: !flow.dispatch.tensor<writeonly:tensor<131072xf32>>) {
    %9 = flow.dispatch.tensor.load %arg3, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
    %10 = flow.dispatch.tensor.load %arg4, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
    %11 = flow.dispatch.tensor.load %arg5, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
    %12 = tensor.empty() : tensor<131072xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%9, %10, %11 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%12 : tensor<131072xf32>) {
    ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
      %14 = arith.addf %in, %in_0 : f32
      %15 = arith.addf %14, %in_1 : f32
      linalg.yield %15 : f32
    } -> tensor<131072xf32>
    flow.dispatch.tensor.store %13, %arg6, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    flow.return
  } count(%arg3: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg3
    flow.return %x, %y, %z : index, index, index
  }
  %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
  %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c131072 = arith.constant 131072 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
  %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
  %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
  %6 = flow.dispatch.workgroups[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32> =
      (%arg3: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg4: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg5: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg6: !flow.dispatch.tensor<writeonly:tensor<131072xf32>>) {
    %9 = flow.dispatch.tensor.load %arg3, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
    %10 = flow.dispatch.tensor.load %arg4, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
    %11 = flow.dispatch.tensor.load %arg5, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
    %12 = tensor.empty() : tensor<131072xf32>
    %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%9, %10, %11 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%12 : tensor<131072xf32>) {
    ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
      %14 = arith.addf %in, %in_0 : f32
      %15 = arith.addf %14, %in_1 : f32
      linalg.yield %15 : f32
    } -> tensor<131072xf32>
    flow.dispatch.tensor.store %13, %arg6, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    flow.return
  } count(%arg3: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg3
    flow.return %x, %y, %z : index, index, index
  }
  %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
  %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After InitializeEmptyTensors (iree-flow-initialize-empty-tensors) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c131072 = arith.constant 131072 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
    %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
    %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
    %6 = flow.dispatch.workgroups[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32> =
        (%arg3: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg4: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg5: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg6: !flow.dispatch.tensor<writeonly:tensor<131072xf32>>) {
      %9 = flow.dispatch.tensor.load %arg3, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
      %10 = flow.dispatch.tensor.load %arg4, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
      %11 = flow.dispatch.tensor.load %arg5, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
      %12 = tensor.empty() : tensor<131072xf32>
      %13 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%9, %10, %11 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%12 : tensor<131072xf32>) {
      ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
        %14 = arith.addf %in, %in_0 : f32
        %15 = arith.addf %14, %in_1 : f32
        linalg.yield %15 : f32
      } -> tensor<131072xf32>
      flow.dispatch.tensor.store %13, %arg6, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
      flow.return
    } count(%arg3: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg3
      flow.return %x, %y, %z : index, index, index
    }
    %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
    %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
    return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After OutlineDispatchRegions (iree-flow-outline-dispatch-regions) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  flow.executable private @twoadd_static_dispatch_0 {
    flow.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<131072xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %3 = tensor.empty() : tensor<131072xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%0, %1, %2 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%3 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %5 = arith.addf %in, %in_0 : f32
          %6 = arith.addf %5, %in_1 : f32
          linalg.yield %6 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %4, %arg3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c131072 = arith.constant 131072 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
    %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
    %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
    %6 = flow.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32>
    %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
    %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
    return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After StripDebugOps (iree-util-strip-debug-ops) //----- //
flow.executable private @twoadd_static_dispatch_0 {
  flow.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<131072xf32>>) {
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
      %1 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
      %2 = flow.dispatch.tensor.load %arg2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
      %3 = tensor.empty() : tensor<131072xf32>
      %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%0, %1, %2 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%3 : tensor<131072xf32>) {
      ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
        %5 = arith.addf %in, %in_0 : f32
        %6 = arith.addf %5, %in_1 : f32
        linalg.yield %6 : f32
      } -> tensor<131072xf32>
      flow.dispatch.tensor.store %4, %arg3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
      return
    }
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c131072 = arith.constant 131072 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
  %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
  %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
  %6 = flow.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32>
  %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
  %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After DeduplicateExecutables (iree-flow-deduplicate-executables) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  flow.executable private @twoadd_static_dispatch_0 {
    flow.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<131072xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %3 = tensor.empty() : tensor<131072xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%0, %1, %2 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%3 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %5 = arith.addf %in, %in_0 : f32
          %6 = arith.addf %5, %in_1 : f32
          linalg.yield %6 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %4, %arg3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c131072 = arith.constant 131072 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
    %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
    %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
    %6 = flow.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32>
    %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
    %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
    return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After CleanupTensorShapes (iree-flow-cleanup-tensor-shapes) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c131072 = arith.constant 131072 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
  %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
  %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
  %6 = flow.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32>
  %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
  %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
flow.executable private @twoadd_static_dispatch_0 {
  flow.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<131072xf32>>) {
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
      %1 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
      %2 = flow.dispatch.tensor.load %arg2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
      %3 = tensor.empty() : tensor<131072xf32>
      %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%0, %1, %2 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%3 : tensor<131072xf32>) {
      ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
        %5 = arith.addf %in, %in_0 : f32
        %6 = arith.addf %5, %in_1 : f32
        linalg.yield %6 : f32
      } -> tensor<131072xf32>
      flow.dispatch.tensor.store %4, %arg3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
      return
    }
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c131072 = arith.constant 131072 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
  %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
  %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
  %6 = flow.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32>
  %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
  %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
flow.executable private @twoadd_static_dispatch_0 {
  flow.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<131072xf32>>) {
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
      %1 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
      %2 = flow.dispatch.tensor.load %arg2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
      %3 = tensor.empty() : tensor<131072xf32>
      %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%0, %1, %2 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%3 : tensor<131072xf32>) {
      ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
        %5 = arith.addf %in, %in_0 : f32
        %6 = arith.addf %5, %in_1 : f32
        linalg.yield %6 : f32
      } -> tensor<131072xf32>
      flow.dispatch.tensor.store %4, %arg3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
      return
    }
  }
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c131072 = arith.constant 131072 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
  %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
  %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
  %6 = flow.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32>
  %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
  %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  flow.executable private @twoadd_static_dispatch_0 {
    flow.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<131072xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %3 = tensor.empty() : tensor<131072xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%0, %1, %2 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%3 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %5 = arith.addf %in, %in_0 : f32
          %6 = arith.addf %5, %in_1 : f32
          linalg.yield %6 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %4, %arg3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c131072 = arith.constant 131072 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
    %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
    %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
    %6 = flow.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32>
    %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
    %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
    return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyInput (iree-stream-verify-input) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  flow.executable private @twoadd_static_dispatch_0 {
    flow.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<131072xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %3 = tensor.empty() : tensor<131072xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%0, %1, %2 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%3 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %5 = arith.addf %in, %in_0 : f32
          %6 = arith.addf %5, %in_1 : f32
          linalg.yield %6 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %4, %arg3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c131072 = arith.constant 131072 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
    %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
    %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
    %6 = flow.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32>
    %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
    %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
    return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After OutlineConstants (iree-stream-outline-constants) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  flow.executable private @twoadd_static_dispatch_0 {
    flow.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<131072xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %3 = tensor.empty() : tensor<131072xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%0, %1, %2 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%3 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %5 = arith.addf %in, %in_0 : f32
          %6 = arith.addf %5, %in_1 : f32
          linalg.yield %6 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %4, %arg3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c131072 = arith.constant 131072 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
    %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
    %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
    %6 = flow.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32>
    %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
    %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
    return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c131072 = arith.constant 131072 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
  %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
  %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
  %6 = flow.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32>
  %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
  %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c131072 = arith.constant 131072 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
  %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
  %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
  %6 = flow.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32>
  %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
  %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c131072 = arith.constant 131072 : index
  %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
  %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
  %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
  %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
  %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
  %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
  %6 = flow.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32>
  %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
  %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  flow.executable private @twoadd_static_dispatch_0 {
    flow.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<131072xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %3 = tensor.empty() : tensor<131072xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%0, %1, %2 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%3 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %5 = arith.addf %in, %in_0 : f32
          %6 = arith.addf %5, %in_1 : f32
          linalg.yield %6 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %4, %arg3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c131072 = arith.constant 131072 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
    %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
    %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
    %6 = flow.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32>
    %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
    %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
    return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  flow.executable private @twoadd_static_dispatch_0 {
    flow.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<131072xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %3 = tensor.empty() : tensor<131072xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%0, %1, %2 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%3 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %5 = arith.addf %in, %in_0 : f32
          %6 = arith.addf %5, %in_1 : f32
          linalg.yield %6 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %4, %arg3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c131072 = arith.constant 131072 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
    %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
    %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
    %6 = flow.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32>
    %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
    %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
    return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  flow.executable private @twoadd_static_dispatch_0 {
    flow.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<131072xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %3 = tensor.empty() : tensor<131072xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%0, %1, %2 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%3 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %5 = arith.addf %in, %in_0 : f32
          %6 = arith.addf %5, %in_1 : f32
          linalg.yield %6 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %4, %arg3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c131072 = arith.constant 131072 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
    %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
    %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
    %6 = flow.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32>
    %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
    %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
    return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  flow.executable private @twoadd_static_dispatch_0 {
    flow.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg1: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg2: !flow.dispatch.tensor<readonly:tensor<131072xf32>>, %arg3: !flow.dispatch.tensor<writeonly:tensor<131072xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %1 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %2 = flow.dispatch.tensor.load %arg2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %3 = tensor.empty() : tensor<131072xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%0, %1, %2 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%3 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %5 = arith.addf %in, %in_0 : f32
          %6 = arith.addf %5, %in_1 : f32
          linalg.yield %6 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %4, %arg3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c131072 = arith.constant 131072 : index
    %0 = hal.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32>
    %1 = hal.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32>
    %2 = hal.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32>
    %3 = flow.tensor.reshape %0 : tensor<512x256xf32> -> tensor<131072xf32>
    %4 = flow.tensor.reshape %1 : tensor<512x256xf32> -> tensor<131072xf32>
    %5 = flow.tensor.reshape %2 : tensor<512x256xf32> -> tensor<131072xf32>
    %6 = flow.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%3, %4, %5) : (tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) -> tensor<131072xf32>
    %7 = flow.tensor.reshape %6 : tensor<131072xf32> -> tensor<512x256xf32>
    %8 = hal.tensor.export %7 : tensor<512x256xf32> -> !hal.buffer_view
    return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After ConvertToStream (iree-stream-conversion) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<512x256xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %c553648160_i32_0 = arith.constant 553648160 : i32
    %c1_i32_1 = arith.constant 1 : i32
    %c512_2 = arith.constant 512 : index
    %c256_3 = arith.constant 256 : index
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512_2, %c256_3]) type(%c553648160_i32_0) encoding(%c1_i32_1)
    %3 = stream.tensor.sizeof tensor<512x256xf32> : index
    %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
    %c553648160_i32_4 = arith.constant 553648160 : i32
    %c1_i32_5 = arith.constant 1 : i32
    %c512_6 = arith.constant 512 : index
    %c256_7 = arith.constant 256 : index
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512_6, %c256_7]) type(%c553648160_i32_4) encoding(%c1_i32_5)
    %6 = stream.tensor.sizeof tensor<512x256xf32> : index
    %7 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%6}
    %8 = stream.async.transfer %7 : !stream.resource<external>{%6} -> !stream.resource<*>{%6}
    %9 = stream.tensor.sizeof tensor<131072xf32> : index
    %10 = stream.tensor.clone %2 : tensor<512x256xf32> in !stream.resource<*>{%0} -> tensor<131072xf32> in !stream.resource<*>{%9}
    %11 = stream.tensor.sizeof tensor<131072xf32> : index
    %12 = stream.tensor.clone %5 : tensor<512x256xf32> in !stream.resource<*>{%3} -> tensor<131072xf32> in !stream.resource<*>{%11}
    %13 = stream.tensor.sizeof tensor<131072xf32> : index
    %14 = stream.tensor.clone %8 : tensor<512x256xf32> in !stream.resource<*>{%6} -> tensor<131072xf32> in !stream.resource<*>{%13}
    %c0 = arith.constant 0 : index
    %15 = stream.tensor.sizeof tensor<131072xf32> : index
    %16 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%10[%c0 to %9 for %9], %12[%c0 to %11 for %11], %14[%c0 to %13 for %13]) : (!stream.resource<*>{%9}, !stream.resource<*>{%11}, !stream.resource<*>{%13}) -> !stream.resource<*>{%15}
    %17 = stream.tensor.sizeof tensor<512x256xf32> : index
    %18 = stream.tensor.clone %16 : tensor<131072xf32> in !stream.resource<*>{%15} -> tensor<512x256xf32> in !stream.resource<*>{%17}
    %19 = stream.async.transfer %18 : !stream.resource<*>{%17} -> !stream.resource<external>{%17}
    %20 = stream.tensor.export %19 : tensor<512x256xf32> in !stream.resource<external>{%17} -> !hal.buffer_view
    return %20 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToTensors (iree-stream-verify-lowering-to-tensors) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<512x256xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %c553648160_i32_0 = arith.constant 553648160 : i32
    %c1_i32_1 = arith.constant 1 : i32
    %c512_2 = arith.constant 512 : index
    %c256_3 = arith.constant 256 : index
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512_2, %c256_3]) type(%c553648160_i32_0) encoding(%c1_i32_1)
    %3 = stream.tensor.sizeof tensor<512x256xf32> : index
    %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
    %c553648160_i32_4 = arith.constant 553648160 : i32
    %c1_i32_5 = arith.constant 1 : i32
    %c512_6 = arith.constant 512 : index
    %c256_7 = arith.constant 256 : index
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512_6, %c256_7]) type(%c553648160_i32_4) encoding(%c1_i32_5)
    %6 = stream.tensor.sizeof tensor<512x256xf32> : index
    %7 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%6}
    %8 = stream.async.transfer %7 : !stream.resource<external>{%6} -> !stream.resource<*>{%6}
    %9 = stream.tensor.sizeof tensor<131072xf32> : index
    %10 = stream.tensor.clone %2 : tensor<512x256xf32> in !stream.resource<*>{%0} -> tensor<131072xf32> in !stream.resource<*>{%9}
    %11 = stream.tensor.sizeof tensor<131072xf32> : index
    %12 = stream.tensor.clone %5 : tensor<512x256xf32> in !stream.resource<*>{%3} -> tensor<131072xf32> in !stream.resource<*>{%11}
    %13 = stream.tensor.sizeof tensor<131072xf32> : index
    %14 = stream.tensor.clone %8 : tensor<512x256xf32> in !stream.resource<*>{%6} -> tensor<131072xf32> in !stream.resource<*>{%13}
    %c0 = arith.constant 0 : index
    %15 = stream.tensor.sizeof tensor<131072xf32> : index
    %16 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%10[%c0 to %9 for %9], %12[%c0 to %11 for %11], %14[%c0 to %13 for %13]) : (!stream.resource<*>{%9}, !stream.resource<*>{%11}, !stream.resource<*>{%13}) -> !stream.resource<*>{%15}
    %17 = stream.tensor.sizeof tensor<512x256xf32> : index
    %18 = stream.tensor.clone %16 : tensor<131072xf32> in !stream.resource<*>{%15} -> tensor<512x256xf32> in !stream.resource<*>{%17}
    %19 = stream.async.transfer %18 : !stream.resource<*>{%17} -> !stream.resource<external>{%17}
    %20 = stream.tensor.export %19 : tensor<512x256xf32> in !stream.resource<external>{%17} -> !hal.buffer_view
    return %20 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.sizeof tensor<512x256xf32> : index
  %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %3 = stream.tensor.sizeof tensor<512x256xf32> : index
  %4 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%3}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %6 = stream.tensor.sizeof tensor<512x256xf32> : index
  %7 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%6}
  %8 = stream.async.transfer %7 : !stream.resource<external>{%6} -> !stream.resource<*>{%6}
  %9 = stream.tensor.sizeof tensor<131072xf32> : index
  %10 = stream.tensor.sizeof tensor<131072xf32> : index
  %11 = stream.tensor.sizeof tensor<131072xf32> : index
  %12 = stream.tensor.sizeof tensor<131072xf32> : index
  %13 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%2[%c0 to %9 for %9], %5[%c0 to %10 for %10], %8[%c0 to %11 for %11]) : (!stream.resource<*>{%9}, !stream.resource<*>{%10}, !stream.resource<*>{%11}) -> !stream.resource<*>{%12}
  %14 = stream.tensor.sizeof tensor<512x256xf32> : index
  %15 = stream.async.transfer %13 : !stream.resource<*>{%14} -> !stream.resource<external>{%14}
  %16 = stream.tensor.export %15 : tensor<512x256xf32> in !stream.resource<external>{%14} -> !hal.buffer_view
  return %16 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.sizeof tensor<512x256xf32> : index
  %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %3 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
  %4 = stream.async.transfer %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %5 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
  %6 = stream.async.transfer %5 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %7 = stream.tensor.sizeof tensor<131072xf32> : index
  %8 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%2[%c0 to %7 for %7], %4[%c0 to %7 for %7], %6[%c0 to %7 for %7]) : (!stream.resource<*>{%7}, !stream.resource<*>{%7}, !stream.resource<*>{%7}) -> !stream.resource<*>{%7}
  %9 = stream.async.transfer %8 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
  %10 = stream.tensor.export %9 : tensor<512x256xf32> in !stream.resource<external>{%0} -> !hal.buffer_view
  return %10 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.sizeof tensor<512x256xf32> : index
  %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %3 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
  %4 = stream.async.transfer %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %5 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
  %6 = stream.async.transfer %5 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %7 = stream.tensor.sizeof tensor<131072xf32> : index
  %8 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%2[%c0 to %7 for %7], %4[%c0 to %7 for %7], %6[%c0 to %7 for %7]) : (!stream.resource<*>{%7}, !stream.resource<*>{%7}, !stream.resource<*>{%7}) -> !stream.resource<*>{%7}
  %9 = stream.async.transfer %8 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
  %10 = stream.tensor.export %9 : tensor<512x256xf32> in !stream.resource<external>{%0} -> !hal.buffer_view
  return %10 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<512x256xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %3 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %4 = stream.async.transfer %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %5 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %6 = stream.async.transfer %5 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %7 = stream.tensor.sizeof tensor<131072xf32> : index
    %8 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%2[%c0 to %7 for %7], %4[%c0 to %7 for %7], %6[%c0 to %7 for %7]) : (!stream.resource<*>{%7}, !stream.resource<*>{%7}, !stream.resource<*>{%7}) -> !stream.resource<*>{%7}
    %9 = stream.async.transfer %8 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %10 = stream.tensor.export %9 : tensor<512x256xf32> in !stream.resource<external>{%0} -> !hal.buffer_view
    return %10 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<512x256xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %3 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %4 = stream.async.transfer %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %5 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %6 = stream.async.transfer %5 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %7 = stream.tensor.sizeof tensor<131072xf32> : index
    %8 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%2[%c0 to %7 for %7], %4[%c0 to %7 for %7], %6[%c0 to %7 for %7]) : (!stream.resource<*>{%7}, !stream.resource<*>{%7}, !stream.resource<*>{%7}) -> !stream.resource<*>{%7}
    %9 = stream.async.transfer %8 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %10 = stream.tensor.export %9 : tensor<512x256xf32> in !stream.resource<external>{%0} -> !hal.buffer_view
    return %10 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<512x256xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %3 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %4 = stream.async.transfer %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %5 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %6 = stream.async.transfer %5 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %7 = stream.tensor.sizeof tensor<131072xf32> : index
    %8 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%2[%c0 to %7 for %7], %4[%c0 to %7 for %7], %6[%c0 to %7 for %7]) : (!stream.resource<*>{%7}, !stream.resource<*>{%7}, !stream.resource<*>{%7}) -> !stream.resource<*>{%7}
    %9 = stream.async.transfer %8 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %10 = stream.tensor.export %9 : tensor<512x256xf32> in !stream.resource<external>{%0} -> !hal.buffer_view
    return %10 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<512x256xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %3 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %4 = stream.async.transfer %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %5 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %6 = stream.async.transfer %5 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %7 = stream.tensor.sizeof tensor<131072xf32> : index
    %8 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%2[%c0 to %7 for %7], %4[%c0 to %7 for %7], %6[%c0 to %7 for %7]) : (!stream.resource<*>{%7}, !stream.resource<*>{%7}, !stream.resource<*>{%7}) -> !stream.resource<*>{%7}
    %9 = stream.async.transfer %8 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %10 = stream.tensor.export %9 : tensor<512x256xf32> in !stream.resource<external>{%0} -> !hal.buffer_view
    return %10 : !hal.buffer_view
  }
}


// -----// IR Dump After CombineInitializers (iree-util-combine-initializers) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<512x256xf32> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %3 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %4 = stream.async.transfer %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %5 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%0}
    %6 = stream.async.transfer %5 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %7 = stream.tensor.sizeof tensor<131072xf32> : index
    %8 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%2[%c0 to %7 for %7], %4[%c0 to %7 for %7], %6[%c0 to %7 for %7]) : (!stream.resource<*>{%7}, !stream.resource<*>{%7}, !stream.resource<*>{%7}) -> !stream.resource<*>{%7}
    %9 = stream.async.transfer %8 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %10 = stream.tensor.export %9 : tensor<512x256xf32> in !stream.resource<external>{%0} -> !hal.buffer_view
    return %10 : !hal.buffer_view
  }
}


// -----// IR Dump After EncodeDeviceTensors (iree-stream-encode-device-tensors) //----- //
stream.executable private @twoadd_static_dispatch_0 {
  stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
    stream.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
      %c0 = arith.constant 0 : index
      %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
      %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
      %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
      %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
      %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
      %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
      %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
      %7 = tensor.empty() : tensor<131072xf32>
      %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
      ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
        %9 = arith.addf %in, %in_0 : f32
        %10 = arith.addf %9, %in_1 : f32
        linalg.yield %10 : f32
      } -> tensor<131072xf32>
      flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
      return
    }
  }
}

// -----// IR Dump After EncodeHostTensors (iree-stream-encode-host-tensors) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %4 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  %6 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c524288 for %c524288], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}) -> !stream.resource<*>{%c524288}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} -> !stream.resource<external>{%c524288}
  %8 = stream.tensor.export %7 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After MaterializeBuiltins (iree-stream-materialize-builtins) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %4 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    %6 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c524288 for %c524288], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}) -> !stream.resource<*>{%c524288}
    %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} -> !stream.resource<external>{%c524288}
    %8 = stream.tensor.export %7 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %4 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  %6 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c524288 for %c524288], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}) -> !stream.resource<*>{%c524288}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} -> !stream.resource<external>{%c524288}
  %8 = stream.tensor.export %7 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %4 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  %6 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c524288 for %c524288], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}) -> !stream.resource<*>{%c524288}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} -> !stream.resource<external>{%c524288}
  %8 = stream.tensor.export %7 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %4 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  %6 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c524288 for %c524288], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}) -> !stream.resource<*>{%c524288}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} -> !stream.resource<external>{%c524288}
  %8 = stream.tensor.export %7 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %4 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    %6 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c524288 for %c524288], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}) -> !stream.resource<*>{%c524288}
    %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} -> !stream.resource<external>{%c524288}
    %8 = stream.tensor.export %7 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %4 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    %6 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c524288 for %c524288], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}) -> !stream.resource<*>{%c524288}
    %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} -> !stream.resource<external>{%c524288}
    %8 = stream.tensor.export %7 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %4 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    %6 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c524288 for %c524288], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}) -> !stream.resource<*>{%c524288}
    %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} -> !stream.resource<external>{%c524288}
    %8 = stream.tensor.export %7 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %4 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    %6 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c524288 for %c524288], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}) -> !stream.resource<*>{%c524288}
    %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} -> !stream.resource<external>{%c524288}
    %8 = stream.tensor.export %7 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeCopyOnWrite (iree-stream-materialize-copy-on-write) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %4 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  %6 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c524288 for %c524288], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}) -> !stream.resource<*>{%c524288}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} -> !stream.resource<external>{%c524288}
  %8 = stream.tensor.export %7 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After ElideAsyncCopies (iree-stream-elide-async-copies) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.async.transfer %2 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %4 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
    %6 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c524288 for %c524288], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}) -> !stream.resource<*>{%c524288}
    %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} -> !stream.resource<external>{%c524288}
    %8 = stream.tensor.export %7 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %4 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  %6 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c524288 for %c524288], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}) -> !stream.resource<*>{%c524288}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} -> !stream.resource<external>{%c524288}
  %8 = stream.tensor.export %7 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After EmplaceAllocations (iree-stream-emplace-allocations) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.async.transfer %2 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %4 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %5 = stream.async.transfer %4 : !stream.resource<external>{%c524288} -> !stream.resource<*>{%c524288}
  %6 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%1[%c0 to %c524288 for %c524288], %3[%c0 to %c524288 for %c524288], %5[%c0 to %c524288 for %c524288]) : (!stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}, !stream.resource<*>{%c524288}) -> !stream.resource<*>{%c524288}
  %7 = stream.async.transfer %6 : !stream.resource<*>{%c524288} -> !stream.resource<external>{%c524288}
  %8 = stream.tensor.export %7 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %8 : !hal.buffer_view
}

// -----// IR Dump After RefineUsage (iree-stream-refine-usage) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%0[%c0 to %c524288 for %c524288], %1[%c0 to %c524288 for %c524288], %2[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
    %4 = stream.tensor.export %3 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%0[%c0 to %c524288 for %c524288], %1[%c0 to %c524288 for %c524288], %2[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
  %4 = stream.tensor.export %3 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%0[%c0 to %c524288 for %c524288], %1[%c0 to %c524288 for %c524288], %2[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
  %4 = stream.tensor.export %3 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%0[%c0 to %c524288 for %c524288], %1[%c0 to %c524288 for %c524288], %2[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
  %4 = stream.tensor.export %3 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%0[%c0 to %c524288 for %c524288], %1[%c0 to %c524288 for %c524288], %2[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
    %4 = stream.tensor.export %3 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%0[%c0 to %c524288 for %c524288], %1[%c0 to %c524288 for %c524288], %2[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
    %4 = stream.tensor.export %3 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%0[%c0 to %c524288 for %c524288], %1[%c0 to %c524288 for %c524288], %2[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
    %4 = stream.tensor.export %3 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%0[%c0 to %c524288 for %c524288], %1[%c0 to %c524288 for %c524288], %2[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
    %4 = stream.tensor.export %3 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After ScheduleExecution (iree-stream-schedule-execution) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288} {
    %5 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c524288 for %c524288], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
    stream.yield %5 : !stream.resource<external>{%c524288}
  } => !stream.timepoint
  %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
  %4 = stream.tensor.export %3 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %4 : !hal.buffer_view
}

// -----// IR Dump After ScheduleConcurrency (iree-stream-schedule-concurrency) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288} {
    %5 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c524288 for %c524288], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
    stream.yield %5 : !stream.resource<external>{%c524288}
  } => !stream.timepoint
  %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
  %4 = stream.tensor.export %3 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %4 : !hal.buffer_view
}

// -----// IR Dump After PropagateTimepoints (iree-stream-propagate-timepoints) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.timepoint.immediate => !stream.timepoint
    %4 = stream.timepoint.immediate => !stream.timepoint
    %5 = stream.timepoint.immediate => !stream.timepoint
    %6 = stream.timepoint.immediate => !stream.timepoint
    %results, %result_timepoint = stream.async.execute await(%6) => with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288} {
      %9 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c524288 for %c524288], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
      stream.yield %9 : !stream.resource<external>{%c524288}
    } => !stream.timepoint
    %7 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
    %8 = stream.tensor.export %7 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288} {
    %5 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c524288 for %c524288], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
    stream.yield %5 : !stream.resource<external>{%c524288}
  } => !stream.timepoint
  %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
  %4 = stream.tensor.export %3 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288} {
    %5 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c524288 for %c524288], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
    stream.yield %5 : !stream.resource<external>{%c524288}
  } => !stream.timepoint
  %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
  %4 = stream.tensor.export %3 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288} {
    %5 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c524288 for %c524288], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
    stream.yield %5 : !stream.resource<external>{%c524288}
  } => !stream.timepoint
  %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
  %4 = stream.tensor.export %3 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288} {
      %5 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c524288 for %c524288], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
      stream.yield %5 : !stream.resource<external>{%c524288}
    } => !stream.timepoint
    %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
    %4 = stream.tensor.export %3 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288} {
      %5 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c524288 for %c524288], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
      stream.yield %5 : !stream.resource<external>{%c524288}
    } => !stream.timepoint
    %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
    %4 = stream.tensor.export %3 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288} {
      %5 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c524288 for %c524288], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
      stream.yield %5 : !stream.resource<external>{%c524288}
    } => !stream.timepoint
    %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
    %4 = stream.tensor.export %3 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288} {
      %5 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c524288 for %c524288], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
      stream.yield %5 : !stream.resource<external>{%c524288}
    } => !stream.timepoint
    %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
    %4 = stream.tensor.export %3 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToAsync (iree-stream-verify-lowering-to-async) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288} {
      %5 = stream.async.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%arg3[%c0 to %c524288 for %c524288], %arg4[%c0 to %c524288 for %c524288], %arg5[%c0 to %c524288 for %c524288]) : (!stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}, !stream.resource<external>{%c524288}) -> !stream.resource<external>{%c524288}
      stream.yield %5 : !stream.resource<external>{%c524288}
    } => !stream.timepoint
    %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c524288}
    %4 = stream.tensor.export %3 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After ScheduleAllocation (iree-stream-schedule-allocation) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %c0_0 = arith.constant 0 : index
  %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
  %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
      wo %arg6[%c0_0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
  %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After PackConstants (iree-stream-pack-constants) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %c0_0 = arith.constant 0 : index
  %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
  %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
      wo %arg6[%c0_0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
  %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After PackAllocations (iree-stream-pack-allocations) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %c0_0 = arith.constant 0 : index
  %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
  %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
      wo %arg6[%c0_0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
  %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After LayoutSlices (iree-stream-layout-slices) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %c0_0 = arith.constant 0 : index
  %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
  %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
      wo %arg6[%c0_0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
  %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After PropagateSubranges (iree-util-propagate-subranges) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %c0_0 = arith.constant 0 : index
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0_0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
  %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
      wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
  %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
  %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
      wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
  %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
  %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
      wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
  %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToCmd (iree-stream-verify-lowering-to-cmd) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
  %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
      wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
  %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
  %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
      wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
  %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
  %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
      wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
  %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
  %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
      wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
  %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
  %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
      wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
  %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
  %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
      wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
  %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
  %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
      wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
  %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#map = affine_map<(d0) -> (d0)>
module attributes {iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module attributes {iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module attributes {iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#map = affine_map<(d0) -> (d0)>
module attributes {iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After ElideTimepoints (iree-stream-elide-timepoints) //----- //
#map = affine_map<(d0) -> (d0)>
module attributes {iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseDispatchBindings (iree-stream-fuse-dispatch-bindings) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding, %arg4: index, %arg5: index, %arg6: index, %arg7: index) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%arg5] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%arg6] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%arg7] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %c0_0 = arith.constant 0 : index
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%c0, %c0, %c0, %c0 : index, index, index, index) {
        ro %arg3[%c0_0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0_0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0_0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0_0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After AnnotateDispatchArguments (iree-stream-annotate-dispatch-arguments) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: !stream.binding {stream.alignment = 64 : index}, %arg4: index {stream.values = [0 : index]}, %arg5: index {stream.values = [0 : index]}, %arg6: index {stream.values = [0 : index]}, %arg7: index {stream.values = [0 : index]}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg4] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%arg5] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%arg6] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%arg7] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %c0_0 = arith.constant 0 : index
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%c0, %c0, %c0, %c0 : index, index, index, index) {
        ro %arg3[%c0_0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0_0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0_0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0_0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After PackDispatchOperands (iree-stream-pack-dispatch-operands) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: !stream.binding {stream.alignment = 64 : index}, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32) {
        %0 = arith.index_castui %arg4 {stream.values = [0 : index]} : i32 to index
        %1 = arith.index_castui %arg5 {stream.values = [0 : index]} : i32 to index
        %2 = arith.index_castui %arg6 {stream.values = [0 : index]} : i32 to index
        %3 = arith.index_castui %arg7 {stream.values = [0 : index]} : i32 to index
        %c0 = arith.constant 0 : index
        %4 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %5 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %6 = stream.binding.subspan %arg2[%2] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %7 = stream.binding.subspan %arg3[%3] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %8 = flow.dispatch.tensor.load %4, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %9 = flow.dispatch.tensor.load %5, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %10 = flow.dispatch.tensor.load %6, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %11 = tensor.empty() : tensor<131072xf32>
        %12 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%8, %9, %10 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%11 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %13 = arith.addf %in, %in_0 : f32
          %14 = arith.addf %13, %in_1 : f32
          linalg.yield %14 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %12, %7, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %c0_0 = arith.constant 0 : index
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c0_i32_2 = arith.constant 0 : i32
    %c0_i32_3 = arith.constant 0 : i32
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%c0_i32, %c0_i32_1, %c0_i32_2, %c0_i32_3 : i32, i32, i32, i32) {
        ro %arg3[%c0_0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0_0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0_0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0_0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: !stream.binding {stream.alignment = 64 : index}, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32) {
        %0 = arith.index_castui %arg4 {stream.values = [0 : index]} : i32 to index
        %1 = arith.index_castui %arg5 {stream.values = [0 : index]} : i32 to index
        %2 = arith.index_castui %arg6 {stream.values = [0 : index]} : i32 to index
        %3 = arith.index_castui %arg7 {stream.values = [0 : index]} : i32 to index
        %4 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %5 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %6 = stream.binding.subspan %arg2[%2] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %7 = stream.binding.subspan %arg3[%3] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %8 = flow.dispatch.tensor.load %4, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %9 = flow.dispatch.tensor.load %5, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %10 = flow.dispatch.tensor.load %6, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %11 = tensor.empty() : tensor<131072xf32>
        %12 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%8, %9, %10 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%11 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %13 = arith.addf %in, %in_0 : f32
          %14 = arith.addf %13, %in_1 : f32
          linalg.yield %14 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %12, %7, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %c0 = arith.constant 0 : index
    %c0_i32 = arith.constant 0 : i32
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072](%c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32) {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldUniformOperands (iree-stream-fold-uniform-operands) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %0 = arith.index_castui %c0_i32 {stream.values = [0 : index]} : i32 to index
        %1 = arith.index_castui %c0_i32 {stream.values = [0 : index]} : i32 to index
        %2 = arith.index_castui %c0_i32 {stream.values = [0 : index]} : i32 to index
        %3 = arith.index_castui %c0_i32 {stream.values = [0 : index]} : i32 to index
        %4 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %5 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %6 = stream.binding.subspan %arg2[%2] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %7 = stream.binding.subspan %arg3[%3] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %8 = flow.dispatch.tensor.load %4, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %9 = flow.dispatch.tensor.load %5, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %10 = flow.dispatch.tensor.load %6, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %11 = tensor.empty() : tensor<131072xf32>
        %12 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%8, %9, %10 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%11 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %13 = arith.addf %in, %in_0 : f32
          %14 = arith.addf %13, %in_1 : f32
          linalg.yield %14 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %12, %7, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %c0 = arith.constant 0 : index
    %c0_i32 = arith.constant 0 : i32
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After MemoizeChannels (iree-stream-memoize-channels) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %0 = arith.index_castui %c0_i32 {stream.values = [0 : index]} : i32 to index
        %1 = arith.index_castui %c0_i32 {stream.values = [0 : index]} : i32 to index
        %2 = arith.index_castui %c0_i32 {stream.values = [0 : index]} : i32 to index
        %3 = arith.index_castui %c0_i32 {stream.values = [0 : index]} : i32 to index
        %4 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %5 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %6 = stream.binding.subspan %arg2[%2] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %7 = stream.binding.subspan %arg3[%3] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %8 = flow.dispatch.tensor.load %4, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %9 = flow.dispatch.tensor.load %5, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %10 = flow.dispatch.tensor.load %6, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %11 = tensor.empty() : tensor<131072xf32>
        %12 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%8, %9, %10 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%11 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %13 = arith.addf %in, %in_0 : f32
          %14 = arith.addf %13, %in_1 : f32
          linalg.yield %14 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %12, %7, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %c0 = arith.constant 0 : index
    %c0_i32 = arith.constant 0 : i32
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
  %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
      wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
  %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
  %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
      wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
  %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
  %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
      wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
  %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c131072 = arith.constant 131072 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
  %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
  %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
    stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
      ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
      ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
      wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
    }
  } => !stream.timepoint
  %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
  %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#map = affine_map<(d0) -> (d0)>
module {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::AssignTargetDevicesPass (iree-hal-assign-target-devices) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::VerifyTargetEnvironmentPass (iree-hal-verify-target-environment) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#map = affine_map<(d0) -> (d0)>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  stream.executable private @twoadd_static_dispatch_0 {
    stream.executable.export public @twoadd_static_dispatch_0_generic_131072 workgroups(%arg0: index) -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg0
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @twoadd_static_dispatch_0_generic_131072(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<131072xf32>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
        %7 = tensor.empty() : tensor<131072xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %9 = arith.addf %in, %in_0 : f32
          %10 = arith.addf %9, %in_1 : f32
          linalg.yield %10 : f32
        } -> tensor<131072xf32>
        flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
        return
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      }
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::(anonymous namespace)::MaterializeInterfacesPass (iree-hal-materialize-interfaces) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#map = affine_map<(d0) -> (d0)>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %x, %y, %z = flow.dispatch.workgroup_count_from_dag_root %arg1
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @twoadd_static_dispatch_0_generic_131072() {
          %c0 = arith.constant 0 : index
          %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
          %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
          %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
          %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
          %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
          %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
          %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
          %7 = tensor.empty() : tensor<131072xf32>
          %8 = linalg.generic {indexing_maps = [#map, #map, #map, #map], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
          ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
            %9 = arith.addf %in, %in_0 : f32
            %10 = arith.addf %9, %in_1 : f32
            linalg.yield %10 : f32
          } -> tensor<131072xf32>
          flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
          return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %1 = stream.tensor.import %arg1 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %2 = stream.tensor.import %arg2 : !hal.buffer_view -> tensor<512x256xf32> in !stream.resource<external>{%c524288}
    %3 = stream.resource.alloc uninitialized : !stream.resource<external>{%c524288}
    %4 = stream.cmd.execute with(%0 as %arg3: !stream.resource<external>{%c524288}, %1 as %arg4: !stream.resource<external>{%c524288}, %2 as %arg5: !stream.resource<external>{%c524288}, %3 as %arg6: !stream.resource<external>{%c524288}) {
      stream.cmd.dispatch @twoadd_static_dispatch_0::@twoadd_static_dispatch_0_generic_131072[%c131072] {
        ro %arg3[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg4[%c0 for %c524288] : !stream.resource<external>{%c524288},
        ro %arg5[%c0 for %c524288] : !stream.resource<external>{%c524288},
        wo %arg6[%c0 for %c524288] : !stream.resource<external>{%c524288}
      } attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>, #hal.interface.binding<0, 2>, #hal.interface.binding<0, 3>]}
    } => !stream.timepoint
    %5 = stream.timepoint.await %4 => %3 : !stream.resource<external>{%c524288}
    %6 = stream.tensor.export %5 : tensor<512x256xf32> in !stream.resource<external>{%c524288} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After TypePropagation (iree-codegen-type-propagation) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
  %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
  %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
  %7 = tensor.empty() : tensor<131072xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
  ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
    %9 = arith.addf %in, %in_0 : f32
    %10 = arith.addf %9, %in_1 : f32
    linalg.yield %10 : f32
  } -> tensor<131072xf32>
  flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  return
}

// -----// IR Dump After BufferizeCopyOnlyDispatches (iree-codegen-bufferize-copy-only-dispatches) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
    %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
    %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
    %7 = tensor.empty() : tensor<131072xf32>
    %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
    ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
      %9 = arith.addf %in, %in_0 : f32
      %10 = arith.addf %9, %in_1 : f32
      linalg.yield %10 : f32
    } -> tensor<131072xf32>
    flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    return
  }
}

// -----// IR Dump After EraseHALDescriptorTypeFromMemRef (iree-codegen-erase-hal-descriptor-type-from-memref) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  %4 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
  %5 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
  %6 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [131072], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<131072xf32>
  %7 = tensor.empty() : tensor<131072xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%4, %5, %6 : tensor<131072xf32>, tensor<131072xf32>, tensor<131072xf32>) outs(%7 : tensor<131072xf32>) {
  ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
    %9 = arith.addf %in, %in_0 : f32
    %10 = arith.addf %9, %in_1 : f32
    linalg.yield %10 : f32
  } -> tensor<131072xf32>
  flow.dispatch.tensor.store %8, %3, offsets = [0], sizes = [131072], strides = [1] : tensor<131072xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  return
}

// -----// IR Dump After TileAndDistributeToWorkgroups (iree-codegen-tile-and-distribute-to-workgroups) //----- //
hal.executable.variant public @cuda_nvptx_fb, target = <"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}> {
  hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<LLVMGPUVectorize>, workgroup_size = [64 : index, 1 : index, 1 : index]} {
  ^bb0(%arg0: !hal.device, %arg1: index):
    %c512 = arith.constant 512 : index
    %c1 = arith.constant 1 : index
    hal.return %c512, %c1, %c1 : index, index, index
  }
  builtin.module {
    func.func @twoadd_static_dispatch_0_generic_131072() {
      %c256 = arith.constant 256 : index
      %c131072 = arith.constant 131072 : index
      %c0 = arith.constant 0 : index
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
      %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
      %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
      %workgroup_id_x = hal.interface.workgroup.id[0] : index
      %workgroup_count_x = hal.interface.workgroup.count[0] : index
      %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
      %5 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_x]
      scf.for %arg0 = %4 to %c131072 step %5 {
        %6 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [%c256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<?xf32>
        %7 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [%c256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<?xf32>
        %8 = flow.dispatch.tensor.load %2, offsets = [%arg0], sizes = [%c256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<?xf32>
        %9 = tensor.empty() : tensor<256xf32>
        %cast = tensor.cast %9 : tensor<256xf32> to tensor<?xf32>
        %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%6, %7, %8 : tensor<?xf32>, tensor<?xf32>, tensor<?xf32>) outs(%cast : tensor<?xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256]]>} {
        ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
          %11 = arith.addf %in, %in_0 : f32
          %12 = arith.addf %11, %in_1 : f32
          linalg.yield %12 : f32
        } -> tensor<?xf32>
        flow.dispatch.tensor.store %10, %3, offsets = [%arg0], sizes = [%c256], strides = [1] : tensor<?xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
      }
      return
    }
  }
}

// -----// IR Dump After TileAndDecomposeAttention (iree-linalg-ext-tile-and-decompose-attention) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %c256 = arith.constant 256 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_x]
  scf.for %arg0 = %4 to %c131072 step %5 {
    %6 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [%c256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<?xf32>
    %7 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [%c256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<?xf32>
    %8 = flow.dispatch.tensor.load %2, offsets = [%arg0], sizes = [%c256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<?xf32>
    %9 = tensor.empty() : tensor<256xf32>
    %cast = tensor.cast %9 : tensor<256xf32> to tensor<?xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%6, %7, %8 : tensor<?xf32>, tensor<?xf32>, tensor<?xf32>) outs(%cast : tensor<?xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256]]>} {
    ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
      %11 = arith.addf %in, %in_0 : f32
      %12 = arith.addf %11, %in_1 : f32
      linalg.yield %12 : f32
    } -> tensor<?xf32>
    flow.dispatch.tensor.store %10, %3, offsets = [%arg0], sizes = [%c256], strides = [1] : tensor<?xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  }
  return
}

// -----// IR Dump After DecomposeSoftmax (iree-linalg-ext-decompose-softmax) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %c256 = arith.constant 256 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_x]
  scf.for %arg0 = %4 to %c131072 step %5 {
    %6 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [%c256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<?xf32>
    %7 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [%c256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<?xf32>
    %8 = flow.dispatch.tensor.load %2, offsets = [%arg0], sizes = [%c256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<?xf32>
    %9 = tensor.empty() : tensor<256xf32>
    %cast = tensor.cast %9 : tensor<256xf32> to tensor<?xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%6, %7, %8 : tensor<?xf32>, tensor<?xf32>, tensor<?xf32>) outs(%cast : tensor<?xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256]]>} {
    ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
      %11 = arith.addf %in, %in_0 : f32
      %12 = arith.addf %11, %in_1 : f32
      linalg.yield %12 : f32
    } -> tensor<?xf32>
    flow.dispatch.tensor.store %10, %3, offsets = [%arg0], sizes = [%c256], strides = [1] : tensor<?xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  }
  return
}

// -----// IR Dump After ConvertToDestinationPassingStyle (iree-codegen-convert-to-destination-passing-style) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %c256 = arith.constant 256 : index
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_x]
  scf.for %arg0 = %4 to %c131072 step %5 {
    %6 = flow.dispatch.tensor.load %3, offsets = [%arg0], sizes = [%c256], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<131072xf32>> -> tensor<?xf32>
    %cast = tensor.cast %6 : tensor<?xf32> to tensor<256xf32>
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [%c256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<?xf32>
    %8 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [%c256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<?xf32>
    %9 = flow.dispatch.tensor.load %2, offsets = [%arg0], sizes = [%c256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<?xf32>
    %cast_0 = tensor.cast %cast : tensor<256xf32> to tensor<?xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7, %8, %9 : tensor<?xf32>, tensor<?xf32>, tensor<?xf32>) outs(%cast_0 : tensor<?xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256]]>} {
    ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
      %11 = arith.addf %in, %in_1 : f32
      %12 = arith.addf %11, %in_2 : f32
      linalg.yield %12 : f32
    } -> tensor<?xf32>
    flow.dispatch.tensor.store %10, %3, offsets = [%arg0], sizes = [%c256], strides = [1] : tensor<?xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_x]
    scf.for %arg0 = %4 to %c131072 step %5 {
      %6 = flow.dispatch.tensor.load %3, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<131072xf32>> -> tensor<256xf32>
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
      %8 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
      %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7, %8, %9 : tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) outs(%6 : tensor<256xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256]]>} {
      ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
        %11 = arith.addf %in, %in_0 : f32
        %12 = arith.addf %11, %in_1 : f32
        linalg.yield %12 : f32
      } -> tensor<256xf32>
      flow.dispatch.tensor.store %10, %3, offsets = [%arg0], sizes = [256], strides = [1] : tensor<256xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_x]
    scf.for %arg0 = %4 to %c131072 step %5 {
      %6 = flow.dispatch.tensor.load %3, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<131072xf32>> -> tensor<256xf32>
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
      %8 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
      %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7, %8, %9 : tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) outs(%6 : tensor<256xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256]]>} {
      ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
        %11 = arith.addf %in, %in_0 : f32
        %12 = arith.addf %11, %in_1 : f32
        linalg.yield %12 : f32
      } -> tensor<256xf32>
      flow.dispatch.tensor.store %10, %3, offsets = [%arg0], sizes = [256], strides = [1] : tensor<256xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    }
    return
  }
}

// -----// IR Dump After WorkgroupSpecialization (iree-codegen-workgroup-specialization) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %c131072 = arith.constant 131072 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_x]
  scf.for %arg0 = %4 to %c131072 step %5 {
    %6 = flow.dispatch.tensor.load %3, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<131072xf32>> -> tensor<256xf32>
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
    %8 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
    %9 = flow.dispatch.tensor.load %2, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7, %8, %9 : tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) outs(%6 : tensor<256xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256]]>} {
    ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
      %11 = arith.addf %in, %in_0 : f32
      %12 = arith.addf %11, %in_1 : f32
      linalg.yield %12 : f32
    } -> tensor<256xf32>
    flow.dispatch.tensor.store %10, %3, offsets = [%arg0], sizes = [256], strides = [1] : tensor<256xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_x]
    scf.for %arg0 = %4 to %c131072 step %5 {
      %6 = flow.dispatch.tensor.load %3, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<131072xf32>> -> tensor<256xf32>
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
      %8 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
      %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7, %8, %9 : tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) outs(%6 : tensor<256xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256]]>} {
      ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
        %11 = arith.addf %in, %in_0 : f32
        %12 = arith.addf %11, %in_1 : f32
        linalg.yield %12 : f32
      } -> tensor<256xf32>
      flow.dispatch.tensor.store %10, %3, offsets = [%arg0], sizes = [256], strides = [1] : tensor<256xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %c131072 = arith.constant 131072 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_x]
    scf.for %arg0 = %4 to %c131072 step %5 {
      %6 = flow.dispatch.tensor.load %3, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<131072xf32>> -> tensor<256xf32>
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
      %8 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
      %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7, %8, %9 : tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) outs(%6 : tensor<256xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256]]>} {
      ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
        %11 = arith.addf %in, %in_0 : f32
        %12 = arith.addf %11, %in_1 : f32
        linalg.yield %12 : f32
      } -> tensor<256xf32>
      flow.dispatch.tensor.store %10, %3, offsets = [%arg0], sizes = [256], strides = [1] : tensor<256xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    }
    return
  }
}

// -----// IR Dump After RemoveSingleIterationLoop (iree-codegen-remove-single-iteration-loop) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
  %5 = flow.dispatch.tensor.load %3, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<131072xf32>> -> tensor<256xf32>
  %6 = flow.dispatch.tensor.load %0, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
  %7 = flow.dispatch.tensor.load %1, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
  %8 = flow.dispatch.tensor.load %2, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%6, %7, %8 : tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) outs(%5 : tensor<256xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256]]>} {
  ^bb0(%in: f32, %in_0: f32, %in_1: f32, %out: f32):
    %10 = arith.addf %in, %in_0 : f32
    %11 = arith.addf %10, %in_1 : f32
    linalg.yield %11 : f32
  } -> tensor<256xf32>
  flow.dispatch.tensor.store %9, %3, offsets = [%4], sizes = [256], strides = [1] : tensor<256xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  return
}

// -----// IR Dump After LLVMGPUTileTensor (iree-llvmgpu-tile-tensor) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
  %5 = flow.dispatch.tensor.load %3, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<131072xf32>> -> tensor<256xf32>
  %6 = flow.dispatch.tensor.load %0, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
  %7 = flow.dispatch.tensor.load %1, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
  %8 = flow.dispatch.tensor.load %2, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
  %9 = scf.foreach_thread (%arg0) in (%c64) shared_outs(%arg1 = %5) -> (tensor<256xf32>) {
    %10 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
    %12 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
    %13 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
    %extracted_slice = tensor.extract_slice %6[%10] [4] [1] : tensor<256xf32> to tensor<4xf32>
    %extracted_slice_0 = tensor.extract_slice %7[%11] [4] [1] : tensor<256xf32> to tensor<4xf32>
    %extracted_slice_1 = tensor.extract_slice %8[%12] [4] [1] : tensor<256xf32> to tensor<4xf32>
    %extracted_slice_2 = tensor.extract_slice %arg1[%13] [4] [1] : tensor<256xf32> to tensor<4xf32>
    %14 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice, %extracted_slice_0, %extracted_slice_1 : tensor<4xf32>, tensor<4xf32>, tensor<4xf32>) outs(%extracted_slice_2 : tensor<4xf32>) attrs =  {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256]]>} {
    ^bb0(%in: f32, %in_3: f32, %in_4: f32, %out: f32):
      %16 = arith.addf %in, %in_3 : f32
      %17 = arith.addf %16, %in_4 : f32
      linalg.yield %17 : f32
    } -> tensor<4xf32>
    %15 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
    scf.foreach_thread.perform_concurrently {
      tensor.parallel_insert_slice %14 into %arg1[%15] [4] [1] : tensor<4xf32> into tensor<256xf32>
    }
  } {mapping = [#gpu.thread<x>]}
  flow.dispatch.tensor.store %9, %3, offsets = [%4], sizes = [256], strides = [1] : tensor<256xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
    %5 = flow.dispatch.tensor.load %3, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<131072xf32>> -> tensor<256xf32>
    %6 = flow.dispatch.tensor.load %0, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
    %7 = flow.dispatch.tensor.load %1, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
    %8 = flow.dispatch.tensor.load %2, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
    %9 = scf.foreach_thread (%arg0) in (%c64) shared_outs(%arg1 = %5) -> (tensor<256xf32>) {
      %10 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
      %11 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
      %12 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
      %13 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
      %extracted_slice = tensor.extract_slice %6[%10] [4] [1] : tensor<256xf32> to tensor<4xf32>
      %extracted_slice_0 = tensor.extract_slice %7[%11] [4] [1] : tensor<256xf32> to tensor<4xf32>
      %extracted_slice_1 = tensor.extract_slice %8[%12] [4] [1] : tensor<256xf32> to tensor<4xf32>
      %extracted_slice_2 = tensor.extract_slice %arg1[%13] [4] [1] : tensor<256xf32> to tensor<4xf32>
      %14 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice, %extracted_slice_0, %extracted_slice_1 : tensor<4xf32>, tensor<4xf32>, tensor<4xf32>) outs(%extracted_slice_2 : tensor<4xf32>) attrs =  {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256]]>} {
      ^bb0(%in: f32, %in_3: f32, %in_4: f32, %out: f32):
        %16 = arith.addf %in, %in_3 : f32
        %17 = arith.addf %16, %in_4 : f32
        linalg.yield %17 : f32
      } -> tensor<4xf32>
      %15 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
      scf.foreach_thread.perform_concurrently {
        tensor.parallel_insert_slice %14 into %arg1[%15] [4] [1] : tensor<4xf32> into tensor<256xf32>
      }
    } {mapping = [#gpu.thread<x>]}
    flow.dispatch.tensor.store %9, %3, offsets = [%4], sizes = [256], strides = [1] : tensor<256xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
    %5 = flow.dispatch.tensor.load %3, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<131072xf32>> -> tensor<256xf32>
    %6 = flow.dispatch.tensor.load %0, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
    %7 = flow.dispatch.tensor.load %1, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
    %8 = flow.dispatch.tensor.load %2, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
    %9 = scf.foreach_thread (%arg0) in (%c64) shared_outs(%arg1 = %5) -> (tensor<256xf32>) {
      %10 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
      %extracted_slice = tensor.extract_slice %6[%10] [4] [1] : tensor<256xf32> to tensor<4xf32>
      %extracted_slice_0 = tensor.extract_slice %7[%10] [4] [1] : tensor<256xf32> to tensor<4xf32>
      %extracted_slice_1 = tensor.extract_slice %8[%10] [4] [1] : tensor<256xf32> to tensor<4xf32>
      %extracted_slice_2 = tensor.extract_slice %arg1[%10] [4] [1] : tensor<256xf32> to tensor<4xf32>
      %11 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice, %extracted_slice_0, %extracted_slice_1 : tensor<4xf32>, tensor<4xf32>, tensor<4xf32>) outs(%extracted_slice_2 : tensor<4xf32>) attrs =  {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256]]>} {
      ^bb0(%in: f32, %in_3: f32, %in_4: f32, %out: f32):
        %12 = arith.addf %in, %in_3 : f32
        %13 = arith.addf %12, %in_4 : f32
        linalg.yield %13 : f32
      } -> tensor<4xf32>
      scf.foreach_thread.perform_concurrently {
        tensor.parallel_insert_slice %11 into %arg1[%10] [4] [1] : tensor<4xf32> into tensor<256xf32>
      }
    } {mapping = [#gpu.thread<x>]}
    flow.dispatch.tensor.store %9, %3, offsets = [%4], sizes = [256], strides = [1] : tensor<256xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    return
  }
}

// -----// IR Dump After RemoveSingleIterationLoop (iree-codegen-remove-single-iteration-loop) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
  %5 = flow.dispatch.tensor.load %3, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<131072xf32>> -> tensor<256xf32>
  %6 = flow.dispatch.tensor.load %0, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
  %7 = flow.dispatch.tensor.load %1, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
  %8 = flow.dispatch.tensor.load %2, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
  %9 = scf.foreach_thread (%arg0) in (%c64) shared_outs(%arg1 = %5) -> (tensor<256xf32>) {
    %10 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
    %extracted_slice = tensor.extract_slice %6[%10] [4] [1] : tensor<256xf32> to tensor<4xf32>
    %extracted_slice_0 = tensor.extract_slice %7[%10] [4] [1] : tensor<256xf32> to tensor<4xf32>
    %extracted_slice_1 = tensor.extract_slice %8[%10] [4] [1] : tensor<256xf32> to tensor<4xf32>
    %extracted_slice_2 = tensor.extract_slice %arg1[%10] [4] [1] : tensor<256xf32> to tensor<4xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%extracted_slice, %extracted_slice_0, %extracted_slice_1 : tensor<4xf32>, tensor<4xf32>, tensor<4xf32>) outs(%extracted_slice_2 : tensor<4xf32>) attrs =  {__internal_linalg_transform__ = "workgroup_k_tiled", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256]]>} {
    ^bb0(%in: f32, %in_3: f32, %in_4: f32, %out: f32):
      %12 = arith.addf %in, %in_3 : f32
      %13 = arith.addf %12, %in_4 : f32
      linalg.yield %13 : f32
    } -> tensor<4xf32>
    scf.foreach_thread.perform_concurrently {
      tensor.parallel_insert_slice %11 into %arg1[%10] [4] [1] : tensor<4xf32> into tensor<256xf32>
    }
  } {mapping = [#gpu.thread<x>]}
  flow.dispatch.tensor.store %9, %3, offsets = [%4], sizes = [256], strides = [1] : tensor<256xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  return
}

// -----// IR Dump After GPUVectorization (iree-codegen-gpu-vectorization) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
  %5 = flow.dispatch.tensor.load %3, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<131072xf32>> -> tensor<256xf32>
  %6 = flow.dispatch.tensor.load %0, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
  %7 = flow.dispatch.tensor.load %1, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
  %8 = flow.dispatch.tensor.load %2, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
  %9 = scf.foreach_thread (%arg0) in (%c64) shared_outs(%arg1 = %5) -> (tensor<256xf32>) {
    %10 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
    %extracted_slice = tensor.extract_slice %6[%10] [4] [1] : tensor<256xf32> to tensor<4xf32>
    %extracted_slice_0 = tensor.extract_slice %7[%10] [4] [1] : tensor<256xf32> to tensor<4xf32>
    %extracted_slice_1 = tensor.extract_slice %8[%10] [4] [1] : tensor<256xf32> to tensor<4xf32>
    %extracted_slice_2 = tensor.extract_slice %arg1[%10] [4] [1] : tensor<256xf32> to tensor<4xf32>
    %11 = vector.transfer_read %extracted_slice[%c0], %cst {in_bounds = [true]} : tensor<4xf32>, vector<4xf32>
    %12 = vector.transfer_read %extracted_slice_0[%c0], %cst {in_bounds = [true]} : tensor<4xf32>, vector<4xf32>
    %13 = vector.transfer_read %extracted_slice_1[%c0], %cst {in_bounds = [true]} : tensor<4xf32>, vector<4xf32>
    %14 = arith.addf %11, %12 : vector<4xf32>
    %15 = arith.addf %14, %13 : vector<4xf32>
    %16 = vector.transfer_write %15, %extracted_slice_2[%c0] {in_bounds = [true]} : vector<4xf32>, tensor<4xf32>
    scf.foreach_thread.perform_concurrently {
      tensor.parallel_insert_slice %16 into %arg1[%10] [4] [1] : tensor<4xf32> into tensor<256xf32>
    }
  } {mapping = [#gpu.thread<x>]}
  flow.dispatch.tensor.store %9, %3, offsets = [%4], sizes = [256], strides = [1] : tensor<256xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
  %5 = flow.dispatch.tensor.load %3, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<131072xf32>> -> tensor<256xf32>
  %6 = flow.dispatch.tensor.load %0, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
  %7 = flow.dispatch.tensor.load %1, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
  %8 = flow.dispatch.tensor.load %2, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
  %9 = scf.foreach_thread (%arg0) in (%c64) shared_outs(%arg1 = %5) -> (tensor<256xf32>) {
    %10 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
    %extracted_slice = tensor.extract_slice %arg1[%10] [4] [1] : tensor<256xf32> to tensor<4xf32>
    %11 = vector.transfer_read %6[%10], %cst {in_bounds = [true]} : tensor<256xf32>, vector<4xf32>
    %12 = vector.transfer_read %7[%10], %cst {in_bounds = [true]} : tensor<256xf32>, vector<4xf32>
    %13 = vector.transfer_read %8[%10], %cst {in_bounds = [true]} : tensor<256xf32>, vector<4xf32>
    %14 = arith.addf %11, %12 : vector<4xf32>
    %15 = arith.addf %14, %13 : vector<4xf32>
    %16 = vector.transfer_write %15, %extracted_slice[%c0] {in_bounds = [true]} : vector<4xf32>, tensor<4xf32>
    scf.foreach_thread.perform_concurrently {
      tensor.parallel_insert_slice %16 into %arg1[%10] [4] [1] : tensor<4xf32> into tensor<256xf32>
    }
  } {mapping = [#gpu.thread<x>]}
  flow.dispatch.tensor.store %9, %3, offsets = [%4], sizes = [256], strides = [1] : tensor<256xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
  %5 = flow.dispatch.tensor.load %3, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<131072xf32>> -> tensor<256xf32>
  %6 = flow.dispatch.tensor.load %0, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
  %7 = flow.dispatch.tensor.load %1, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
  %8 = flow.dispatch.tensor.load %2, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
  %9 = scf.foreach_thread (%arg0) in (%c64) shared_outs(%arg1 = %5) -> (tensor<256xf32>) {
    %10 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
    %extracted_slice = tensor.extract_slice %arg1[%10] [4] [1] : tensor<256xf32> to tensor<4xf32>
    %11 = vector.transfer_read %6[%10], %cst {in_bounds = [true]} : tensor<256xf32>, vector<4xf32>
    %12 = vector.transfer_read %7[%10], %cst {in_bounds = [true]} : tensor<256xf32>, vector<4xf32>
    %13 = vector.transfer_read %8[%10], %cst {in_bounds = [true]} : tensor<256xf32>, vector<4xf32>
    %14 = arith.addf %11, %12 : vector<4xf32>
    %15 = arith.addf %14, %13 : vector<4xf32>
    %16 = vector.transfer_write %15, %extracted_slice[%c0] {in_bounds = [true]} : vector<4xf32>, tensor<4xf32>
    scf.foreach_thread.perform_concurrently {
      tensor.parallel_insert_slice %16 into %arg1[%10] [4] [1] : tensor<4xf32> into tensor<256xf32>
    }
  } {mapping = [#gpu.thread<x>]}
  flow.dispatch.tensor.store %9, %3, offsets = [%4], sizes = [256], strides = [1] : tensor<256xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  return
}

// -----// IR Dump After EliminateEmptyTensors (iree-eliminate-empty-tensors) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %cst = arith.constant 0.000000e+00 : f32
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
    %5 = flow.dispatch.tensor.load %3, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<131072xf32>> -> tensor<256xf32>
    %6 = flow.dispatch.tensor.load %0, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
    %7 = flow.dispatch.tensor.load %1, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
    %8 = flow.dispatch.tensor.load %2, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
    %9 = scf.foreach_thread (%arg0) in (%c64) shared_outs(%arg1 = %5) -> (tensor<256xf32>) {
      %10 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
      %extracted_slice = tensor.extract_slice %arg1[%10] [4] [1] : tensor<256xf32> to tensor<4xf32>
      %11 = vector.transfer_read %6[%10], %cst {in_bounds = [true]} : tensor<256xf32>, vector<4xf32>
      %12 = vector.transfer_read %7[%10], %cst {in_bounds = [true]} : tensor<256xf32>, vector<4xf32>
      %13 = vector.transfer_read %8[%10], %cst {in_bounds = [true]} : tensor<256xf32>, vector<4xf32>
      %14 = arith.addf %11, %12 : vector<4xf32>
      %15 = arith.addf %14, %13 : vector<4xf32>
      %16 = vector.transfer_write %15, %extracted_slice[%c0] {in_bounds = [true]} : vector<4xf32>, tensor<4xf32>
      scf.foreach_thread.perform_concurrently {
        tensor.parallel_insert_slice %16 into %arg1[%10] [4] [1] : tensor<4xf32> into tensor<256xf32>
      }
    } {mapping = [#gpu.thread<x>]}
    flow.dispatch.tensor.store %9, %3, offsets = [%4], sizes = [256], strides = [1] : tensor<256xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    return
  }
}

// -----// IR Dump After EmptyTensorToAllocTensor (empty-tensor-to-alloc-tensor) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %cst = arith.constant 0.000000e+00 : f32
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
    %5 = flow.dispatch.tensor.load %3, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<131072xf32>> -> tensor<256xf32>
    %6 = flow.dispatch.tensor.load %0, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
    %7 = flow.dispatch.tensor.load %1, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
    %8 = flow.dispatch.tensor.load %2, offsets = [%4], sizes = [256], strides = [1] : !flow.dispatch.tensor<readonly:tensor<131072xf32>> -> tensor<256xf32>
    %9 = scf.foreach_thread (%arg0) in (%c64) shared_outs(%arg1 = %5) -> (tensor<256xf32>) {
      %10 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
      %extracted_slice = tensor.extract_slice %arg1[%10] [4] [1] : tensor<256xf32> to tensor<4xf32>
      %11 = vector.transfer_read %6[%10], %cst {in_bounds = [true]} : tensor<256xf32>, vector<4xf32>
      %12 = vector.transfer_read %7[%10], %cst {in_bounds = [true]} : tensor<256xf32>, vector<4xf32>
      %13 = vector.transfer_read %8[%10], %cst {in_bounds = [true]} : tensor<256xf32>, vector<4xf32>
      %14 = arith.addf %11, %12 : vector<4xf32>
      %15 = arith.addf %14, %13 : vector<4xf32>
      %16 = vector.transfer_write %15, %extracted_slice[%c0] {in_bounds = [true]} : vector<4xf32>, tensor<4xf32>
      scf.foreach_thread.perform_concurrently {
        tensor.parallel_insert_slice %16 into %arg1[%10] [4] [1] : tensor<4xf32> into tensor<256xf32>
      }
    } {mapping = [#gpu.thread<x>]}
    flow.dispatch.tensor.store %9, %3, offsets = [%4], sizes = [256], strides = [1] : tensor<256xf32> -> !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    return
  }
}

// -----// IR Dump After IREEComprehensiveBufferize (iree-codegen-iree-comprehensive-bufferize) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %cst = arith.constant 0.000000e+00 : f32
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %4, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %6 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %6, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    %7 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %8 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
    %subview = memref.subview %6[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_0 = memref.subview %0[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_1 = memref.subview %2[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_2 = memref.subview %4[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.foreach_thread (%arg0) in (%c64) {
      %9 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
      %subview_4 = memref.subview %subview[%9] [4] [1] : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %10 = vector.transfer_read %subview_0[%9], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
      %11 = vector.transfer_read %subview_1[%9], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
      %12 = vector.transfer_read %subview_2[%9], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
      %13 = arith.addf %10, %11 : vector<4xf32>
      %14 = arith.addf %13, %12 : vector<4xf32>
      vector.transfer_write %14, %subview_4[%c0] {in_bounds = [true]} : vector<4xf32>, memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_5 = memref.subview %subview[%9] [4] [1] : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      memref.copy %subview_4, %subview_5 : memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    } {mapping = [#gpu.thread<x>]}
    %subview_3 = memref.subview %6[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    memref.copy %subview, %subview_3 : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    return
  }
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %cst = arith.constant 0.000000e+00 : f32
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %4, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
    %6 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %6, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    %7 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %8 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
    %subview = memref.subview %6[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_0 = memref.subview %0[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_1 = memref.subview %2[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_2 = memref.subview %4[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.foreach_thread (%arg0) in (%c64) {
      %9 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
      %subview_4 = memref.subview %subview[%9] [4] [1] : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %10 = vector.transfer_read %subview_0[%9], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
      %11 = vector.transfer_read %subview_1[%9], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
      %12 = vector.transfer_read %subview_2[%9], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
      %13 = arith.addf %10, %11 : vector<4xf32>
      %14 = arith.addf %13, %12 : vector<4xf32>
      vector.transfer_write %14, %subview_4[%c0] {in_bounds = [true]} : vector<4xf32>, memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_5 = memref.subview %subview[%9] [4] [1] : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      memref.copy %subview_4, %subview_5 : memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    } {mapping = [#gpu.thread<x>]}
    %subview_3 = memref.subview %6[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    memref.copy %subview, %subview_3 : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %4, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %6 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %6, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  %7 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %8 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
  %subview = memref.subview %6[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_0 = memref.subview %0[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_1 = memref.subview %2[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_2 = memref.subview %4[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  scf.foreach_thread (%arg0) in (%c64) {
    %9 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
    %subview_4 = memref.subview %subview[%9] [4] [1] : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %10 = vector.transfer_read %subview_0[%9], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
    %11 = vector.transfer_read %subview_1[%9], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
    %12 = vector.transfer_read %subview_2[%9], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
    %13 = arith.addf %10, %11 : vector<4xf32>
    %14 = arith.addf %13, %12 : vector<4xf32>
    vector.transfer_write %14, %subview_4[%c0] {in_bounds = [true]} : vector<4xf32>, memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_5 = memref.subview %subview[%9] [4] [1] : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    memref.copy %subview_4, %subview_5 : memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  } {mapping = [#gpu.thread<x>]}
  %subview_3 = memref.subview %6[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  memref.copy %subview, %subview_3 : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %4, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %6 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %6, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  %7 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %8 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
  %subview = memref.subview %6[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_0 = memref.subview %0[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_1 = memref.subview %2[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_2 = memref.subview %4[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  scf.foreach_thread (%arg0) in (%c64) {
    %9 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
    %subview_3 = memref.subview %subview[%9] [4] [1] : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %10 = vector.transfer_read %subview_0[%9], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
    %11 = vector.transfer_read %subview_1[%9], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
    %12 = vector.transfer_read %subview_2[%9], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
    %13 = arith.addf %10, %11 : vector<4xf32>
    %14 = arith.addf %13, %12 : vector<4xf32>
    vector.transfer_write %14, %subview_3[%c0] {in_bounds = [true]} : vector<4xf32>, memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    memref.copy %subview_3, %subview_3 : memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  } {mapping = [#gpu.thread<x>]}
  memref.copy %subview, %subview : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %4, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<131072xf32>>
  %6 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %6, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  %7 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<131072xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %8 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
  %subview = memref.subview %6[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_0 = memref.subview %0[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_1 = memref.subview %2[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_2 = memref.subview %4[%8] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  scf.foreach_thread (%arg0) in (%c64) {
    %9 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
    %subview_3 = memref.subview %subview[%9] [4] [1] : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %10 = vector.transfer_read %subview_0[%9], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
    %11 = vector.transfer_read %subview_1[%9], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
    %12 = vector.transfer_read %subview_2[%9], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
    %13 = arith.addf %10, %11 : vector<4xf32>
    %14 = arith.addf %13, %12 : vector<4xf32>
    vector.transfer_write %14, %subview_3[%c0] {in_bounds = [true]} : vector<4xf32>, memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  } {mapping = [#gpu.thread<x>]}
  return
}

// -----// IR Dump After CleanupBufferAllocView (iree-codegen-cleanup-buffer-alloc-view) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %3, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
  %subview = memref.subview %3[%4] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_0 = memref.subview %0[%4] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_1 = memref.subview %1[%4] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %subview_2 = memref.subview %2[%4] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  scf.foreach_thread (%arg0) in (%c64) {
    %5 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
    %subview_3 = memref.subview %subview[%5] [4] [1] : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %6 = vector.transfer_read %subview_0[%5], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
    %7 = vector.transfer_read %subview_1[%5], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
    %8 = vector.transfer_read %subview_2[%5], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
    %9 = arith.addf %6, %7 : vector<4xf32>
    %10 = arith.addf %9, %8 : vector<4xf32>
    vector.transfer_write %10, %subview_3[%c0] {in_bounds = [true]} : vector<4xf32>, memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  } {mapping = [#gpu.thread<x>]}
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %cst = arith.constant 0.000000e+00 : f32
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %3, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
    %subview = memref.subview %3[%4] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_0 = memref.subview %0[%4] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_1 = memref.subview %1[%4] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_2 = memref.subview %2[%4] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.foreach_thread (%arg0) in (%c64) {
      %5 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
      %subview_3 = memref.subview %subview[%5] [4] [1] : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %6 = vector.transfer_read %subview_0[%5], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
      %7 = vector.transfer_read %subview_1[%5], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
      %8 = vector.transfer_read %subview_2[%5], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
      %9 = arith.addf %6, %7 : vector<4xf32>
      %10 = arith.addf %9, %8 : vector<4xf32>
      vector.transfer_write %10, %subview_3[%c0] {in_bounds = [true]} : vector<4xf32>, memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    } {mapping = [#gpu.thread<x>]}
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %cst = arith.constant 0.000000e+00 : f32
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %3, 64 : memref<131072xf32, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
    %subview = memref.subview %3[%4] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_0 = memref.subview %0[%4] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_1 = memref.subview %1[%4] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %subview_2 = memref.subview %2[%4] [256] [1] : memref<131072xf32, #hal.descriptor_type<storage_buffer>> to memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.foreach_thread (%arg0) in (%c64) {
      %5 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
      %subview_3 = memref.subview %subview[%5] [4] [1] : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %6 = vector.transfer_read %subview_0[%5], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
      %7 = vector.transfer_read %subview_1[%5], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
      %8 = vector.transfer_read %subview_2[%5], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<4xf32>
      %9 = arith.addf %6, %7 : vector<4xf32>
      %10 = arith.addf %9, %8 : vector<4xf32>
      vector.transfer_write %10, %subview_3[%c0] {in_bounds = [true]} : vector<4xf32>, memref<4xf32, strided<[1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    } {mapping = [#gpu.thread<x>]}
    return
  }
}

// -----// IR Dump After EraseHALDescriptorTypeFromMemRef (iree-codegen-erase-hal-descriptor-type-from-memref) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
  %subview = memref.subview %3[%4] [256] [1] : memref<131072xf32> to memref<256xf32, strided<[1], offset: ?>>
  %subview_0 = memref.subview %0[%4] [256] [1] : memref<131072xf32> to memref<256xf32, strided<[1], offset: ?>>
  %subview_1 = memref.subview %1[%4] [256] [1] : memref<131072xf32> to memref<256xf32, strided<[1], offset: ?>>
  %subview_2 = memref.subview %2[%4] [256] [1] : memref<131072xf32> to memref<256xf32, strided<[1], offset: ?>>
  scf.foreach_thread (%arg0) in (%c64) {
    %5 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
    %subview_3 = memref.subview %subview[%5] [4] [1] : memref<256xf32, strided<[1], offset: ?>> to memref<4xf32, strided<[1], offset: ?>>
    %6 = vector.transfer_read %subview_0[%5], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>>, vector<4xf32>
    %7 = vector.transfer_read %subview_1[%5], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>>, vector<4xf32>
    %8 = vector.transfer_read %subview_2[%5], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>>, vector<4xf32>
    %9 = arith.addf %6, %7 : vector<4xf32>
    %10 = arith.addf %9, %8 : vector<4xf32>
    vector.transfer_write %10, %subview_3[%c0] {in_bounds = [true]} : vector<4xf32>, memref<4xf32, strided<[1], offset: ?>>
  } {mapping = [#gpu.thread<x>]}
  return
}

// -----// IR Dump After LLVMGPUDistribute (iree-llvmgpu-distribute) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
  %subview = memref.subview %3[%4] [256] [1] : memref<131072xf32> to memref<256xf32, strided<[1], offset: ?>>
  %subview_0 = memref.subview %0[%4] [256] [1] : memref<131072xf32> to memref<256xf32, strided<[1], offset: ?>>
  %subview_1 = memref.subview %1[%4] [256] [1] : memref<131072xf32> to memref<256xf32, strided<[1], offset: ?>>
  %subview_2 = memref.subview %2[%4] [256] [1] : memref<131072xf32> to memref<256xf32, strided<[1], offset: ?>>
  %c1 = arith.constant 1 : index
  %5 = gpu.thread_id  x
  %6 = gpu.thread_id  y
  %7 = gpu.thread_id  z
  %c0_3 = arith.constant 0 : index
  %8 = affine.apply affine_map<(d0) -> (d0 * 4)>(%5)
  %subview_4 = memref.subview %subview[%8] [4] [1] : memref<256xf32, strided<[1], offset: ?>> to memref<4xf32, strided<[1], offset: ?>>
  %9 = vector.transfer_read %subview_0[%8], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>>, vector<4xf32>
  %10 = vector.transfer_read %subview_1[%8], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>>, vector<4xf32>
  %11 = vector.transfer_read %subview_2[%8], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>>, vector<4xf32>
  %12 = arith.addf %9, %10 : vector<4xf32>
  %13 = arith.addf %12, %11 : vector<4xf32>
  vector.transfer_write %13, %subview_4[%c0] {in_bounds = [true]} : vector<4xf32>, memref<4xf32, strided<[1], offset: ?>>
  return
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
  %subview = memref.subview %3[%4] [256] [1] : memref<131072xf32> to memref<256xf32, strided<[1], offset: ?>>
  %subview_0 = memref.subview %0[%4] [256] [1] : memref<131072xf32> to memref<256xf32, strided<[1], offset: ?>>
  %subview_1 = memref.subview %1[%4] [256] [1] : memref<131072xf32> to memref<256xf32, strided<[1], offset: ?>>
  %subview_2 = memref.subview %2[%4] [256] [1] : memref<131072xf32> to memref<256xf32, strided<[1], offset: ?>>
  %c1 = arith.constant 1 : index
  %5 = gpu.thread_id  x
  %6 = gpu.thread_id  y
  %7 = gpu.thread_id  z
  %c0_3 = arith.constant 0 : index
  %8 = affine.apply affine_map<(d0) -> (d0 * 4)>(%5)
  %subview_4 = memref.subview %subview[%8] [4] [1] : memref<256xf32, strided<[1], offset: ?>> to memref<4xf32, strided<[1], offset: ?>>
  %9 = vector.transfer_read %subview_0[%8], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>>, vector<4xf32>
  %10 = vector.transfer_read %subview_1[%8], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>>, vector<4xf32>
  %11 = vector.transfer_read %subview_2[%8], %cst {in_bounds = [true]} : memref<256xf32, strided<[1], offset: ?>>, vector<4xf32>
  %12 = arith.addf %9, %10 : vector<4xf32>
  %13 = arith.addf %12, %11 : vector<4xf32>
  vector.transfer_write %13, %subview_4[%c0] {in_bounds = [true]} : vector<4xf32>, memref<4xf32, strided<[1], offset: ?>>
  return
}

// -----// IR Dump After FoldMemRefAliasOps (fold-memref-alias-ops) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_x]
  %5 = gpu.thread_id  x
  %6 = affine.apply affine_map<(d0) -> (d0 * 4)>(%5)
  %7 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%6)[%4]
  %8 = vector.transfer_read %0[%7], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %9 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%6)[%4]
  %10 = vector.transfer_read %1[%9], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %11 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%6)[%4]
  %12 = vector.transfer_read %2[%11], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %13 = arith.addf %8, %10 : vector<4xf32>
  %14 = arith.addf %13, %12 : vector<4xf32>
  %15 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%c0)[%6]
  %16 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%15)[%4]
  vector.transfer_write %14, %3[%16] {in_bounds = [true]} : vector<4xf32>, memref<131072xf32>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = gpu.thread_id  x
  %5 = affine.apply affine_map<()[s0, s1] -> (s0 * 4 + s1 * 256)>()[%4, %workgroup_id_x]
  %6 = vector.transfer_read %0[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %7 = affine.apply affine_map<()[s0, s1] -> (s0 * 4 + s1 * 256)>()[%4, %workgroup_id_x]
  %8 = vector.transfer_read %1[%7], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %9 = affine.apply affine_map<()[s0, s1] -> (s0 * 4 + s1 * 256)>()[%4, %workgroup_id_x]
  %10 = vector.transfer_read %2[%9], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %11 = arith.addf %6, %8 : vector<4xf32>
  %12 = arith.addf %11, %10 : vector<4xf32>
  %13 = affine.apply affine_map<()[s0, s1] -> (s0 * 4 + s1 * 256)>()[%4, %workgroup_id_x]
  vector.transfer_write %12, %3[%13] {in_bounds = [true]} : vector<4xf32>, memref<131072xf32>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = gpu.thread_id  x
  %5 = affine.apply affine_map<()[s0, s1] -> (s0 * 4 + s1 * 256)>()[%4, %workgroup_id_x]
  %6 = vector.transfer_read %0[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %7 = vector.transfer_read %1[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %8 = vector.transfer_read %2[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %9 = arith.addf %6, %7 : vector<4xf32>
  %10 = arith.addf %9, %8 : vector<4xf32>
  vector.transfer_write %10, %3[%5] {in_bounds = [true]} : vector<4xf32>, memref<131072xf32>
  return
}

// -----// IR Dump After OptimizeVectorTransfer (iree-codegen-optimize-vector-transfer) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = gpu.thread_id  x
  %5 = affine.apply affine_map<()[s0, s1] -> (s0 * 4 + s1 * 256)>()[%4, %workgroup_id_x]
  %6 = vector.transfer_read %0[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %7 = vector.transfer_read %1[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %8 = vector.transfer_read %2[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %9 = arith.addf %6, %7 : vector<4xf32>
  %10 = arith.addf %9, %8 : vector<4xf32>
  vector.transfer_write %10, %3[%5] {in_bounds = [true]} : vector<4xf32>, memref<131072xf32>
  return
}

// -----// IR Dump After LLVMGPULowerExecutableTarget (iree-llvmgpu-lower-executable-target) //----- //
hal.executable.variant public @cuda_nvptx_fb, target = <"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}> {
  hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<LLVMGPUVectorize>, workgroup_size = [64 : index, 1 : index, 1 : index]} {
  ^bb0(%arg0: !hal.device, %arg1: index):
    %c512 = arith.constant 512 : index
    %c1 = arith.constant 1 : index
    hal.return %c512, %c1, %c1 : index, index, index
  }
  builtin.module {
    func.func @twoadd_static_dispatch_0_generic_131072() {
      %cst = arith.constant 0.000000e+00 : f32
      %c0 = arith.constant 0 : index
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
      memref.assume_alignment %0, 64 : memref<131072xf32>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
      memref.assume_alignment %1, 64 : memref<131072xf32>
      %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
      memref.assume_alignment %2, 64 : memref<131072xf32>
      %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
      memref.assume_alignment %3, 64 : memref<131072xf32>
      %workgroup_id_x = hal.interface.workgroup.id[0] : index
      %4 = gpu.thread_id  x
      %5 = affine.apply affine_map<()[s0, s1] -> (s0 * 4 + s1 * 256)>()[%4, %workgroup_id_x]
      %6 = vector.transfer_read %0[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
      %7 = vector.transfer_read %1[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
      %8 = vector.transfer_read %2[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
      %9 = arith.addf %6, %7 : vector<4xf32>
      %10 = arith.addf %9, %8 : vector<4xf32>
      vector.transfer_write %10, %3[%5] {in_bounds = [true]} : vector<4xf32>, memref<131072xf32>
      return
    }
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %cst = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %0, 64 : memref<131072xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %1, 64 : memref<131072xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %2, 64 : memref<131072xf32>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
    memref.assume_alignment %3, 64 : memref<131072xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = gpu.thread_id  x
    %5 = affine.apply affine_map<()[s0, s1] -> (s0 * 4 + s1 * 256)>()[%4, %workgroup_id_x]
    %6 = vector.transfer_read %0[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %7 = vector.transfer_read %1[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %8 = vector.transfer_read %2[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %9 = arith.addf %6, %7 : vector<4xf32>
    %10 = arith.addf %9, %8 : vector<4xf32>
    vector.transfer_write %10, %3[%5] {in_bounds = [true]} : vector<4xf32>, memref<131072xf32>
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %cst = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %0, 64 : memref<131072xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %1, 64 : memref<131072xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %2, 64 : memref<131072xf32>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
    memref.assume_alignment %3, 64 : memref<131072xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = gpu.thread_id  x
    %5 = affine.apply affine_map<()[s0, s1] -> (s0 * 4 + s1 * 256)>()[%4, %workgroup_id_x]
    %6 = vector.transfer_read %0[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %7 = vector.transfer_read %1[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %8 = vector.transfer_read %2[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %9 = arith.addf %6, %7 : vector<4xf32>
    %10 = arith.addf %9, %8 : vector<4xf32>
    vector.transfer_write %10, %3[%5] {in_bounds = [true]} : vector<4xf32>, memref<131072xf32>
    return
  }
}

// -----// IR Dump After LinalgExtToLoops (iree-linalg-ext-to-loops) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = gpu.thread_id  x
  %5 = affine.apply affine_map<()[s0, s1] -> (s0 * 4 + s1 * 256)>()[%4, %workgroup_id_x]
  %6 = vector.transfer_read %0[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %7 = vector.transfer_read %1[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %8 = vector.transfer_read %2[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %9 = arith.addf %6, %7 : vector<4xf32>
  %10 = arith.addf %9, %8 : vector<4xf32>
  vector.transfer_write %10, %3[%5] {in_bounds = [true]} : vector<4xf32>, memref<131072xf32>
  return
}

// -----// IR Dump After MemrefCopyToLinalgPass (iree-codegen-memrefcopy-to-linalg) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = gpu.thread_id  x
  %5 = affine.apply affine_map<()[s0, s1] -> (s0 * 4 + s1 * 256)>()[%4, %workgroup_id_x]
  %6 = vector.transfer_read %0[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %7 = vector.transfer_read %1[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %8 = vector.transfer_read %2[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %9 = arith.addf %6, %7 : vector<4xf32>
  %10 = arith.addf %9, %8 : vector<4xf32>
  vector.transfer_write %10, %3[%5] {in_bounds = [true]} : vector<4xf32>, memref<131072xf32>
  return
}

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = gpu.thread_id  x
  %5 = affine.apply affine_map<()[s0, s1] -> (s0 * 4 + s1 * 256)>()[%4, %workgroup_id_x]
  %6 = vector.transfer_read %0[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %7 = vector.transfer_read %1[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %8 = vector.transfer_read %2[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %9 = arith.addf %6, %7 : vector<4xf32>
  %10 = arith.addf %9, %8 : vector<4xf32>
  vector.transfer_write %10, %3[%5] {in_bounds = [true]} : vector<4xf32>, memref<131072xf32>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = gpu.thread_id  x
  %5 = affine.apply affine_map<()[s0, s1] -> (s0 * 4 + s1 * 256)>()[%4, %workgroup_id_x]
  %6 = vector.transfer_read %0[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %7 = vector.transfer_read %1[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %8 = vector.transfer_read %2[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %9 = arith.addf %6, %7 : vector<4xf32>
  %10 = arith.addf %9, %8 : vector<4xf32>
  vector.transfer_write %10, %3[%5] {in_bounds = [true]} : vector<4xf32>, memref<131072xf32>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = gpu.thread_id  x
  %5 = affine.apply affine_map<()[s0, s1] -> (s0 * 4 + s1 * 256)>()[%4, %workgroup_id_x]
  %6 = vector.transfer_read %0[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %7 = vector.transfer_read %1[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %8 = vector.transfer_read %2[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %9 = arith.addf %6, %7 : vector<4xf32>
  %10 = arith.addf %9, %8 : vector<4xf32>
  vector.transfer_write %10, %3[%5] {in_bounds = [true]} : vector<4xf32>, memref<131072xf32>
  return
}

// -----// IR Dump After PadDynamicAlloc (iree-codegen-pad-dynamic-alloc) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %cst = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = gpu.thread_id  x
  %5 = affine.apply affine_map<()[s0, s1] -> (s0 * 4 + s1 * 256)>()[%4, %workgroup_id_x]
  %6 = vector.transfer_read %0[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %7 = vector.transfer_read %1[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %8 = vector.transfer_read %2[%5], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
  %9 = arith.addf %6, %7 : vector<4xf32>
  %10 = arith.addf %9, %8 : vector<4xf32>
  vector.transfer_write %10, %3[%5] {in_bounds = [true]} : vector<4xf32>, memref<131072xf32>
  return
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %cst = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %0, 64 : memref<131072xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %1, 64 : memref<131072xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %2, 64 : memref<131072xf32>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
    memref.assume_alignment %3, 64 : memref<131072xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = gpu.thread_id  x
    %c4 = arith.constant 4 : index
    %5 = arith.muli %4, %c4 : index
    %c256 = arith.constant 256 : index
    %6 = arith.muli %workgroup_id_x, %c256 : index
    %7 = arith.addi %5, %6 : index
    %8 = vector.transfer_read %0[%7], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %9 = vector.transfer_read %1[%7], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %10 = vector.transfer_read %2[%7], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %11 = arith.addf %8, %9 : vector<4xf32>
    %12 = arith.addf %11, %10 : vector<4xf32>
    vector.transfer_write %12, %3[%7] {in_bounds = [true]} : vector<4xf32>, memref<131072xf32>
    return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %c256 = arith.constant 256 : index
    %c4 = arith.constant 4 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %0, 64 : memref<131072xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %1, 64 : memref<131072xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %2, 64 : memref<131072xf32>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
    memref.assume_alignment %3, 64 : memref<131072xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = gpu.thread_id  x
    %5 = arith.muli %4, %c4 : index
    %6 = arith.muli %workgroup_id_x, %c256 : index
    %7 = arith.addi %5, %6 : index
    %8 = vector.transfer_read %0[%7], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %9 = vector.transfer_read %1[%7], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %10 = vector.transfer_read %2[%7], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %11 = arith.addf %8, %9 : vector<4xf32>
    %12 = arith.addf %11, %10 : vector<4xf32>
    vector.transfer_write %12, %3[%7] {in_bounds = [true]} : vector<4xf32>, memref<131072xf32>
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %c256 = arith.constant 256 : index
    %c4 = arith.constant 4 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %0, 64 : memref<131072xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %1, 64 : memref<131072xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %2, 64 : memref<131072xf32>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
    memref.assume_alignment %3, 64 : memref<131072xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = gpu.thread_id  x
    %5 = arith.muli %4, %c4 : index
    %6 = arith.muli %workgroup_id_x, %c256 : index
    %7 = arith.addi %5, %6 : index
    %8 = vector.transfer_read %0[%7], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %9 = vector.transfer_read %1[%7], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %10 = vector.transfer_read %2[%7], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %11 = arith.addf %8, %9 : vector<4xf32>
    %12 = arith.addf %11, %10 : vector<4xf32>
    vector.transfer_write %12, %3[%7] {in_bounds = [true]} : vector<4xf32>, memref<131072xf32>
    return
  }
}

// -----// IR Dump After ArithBufferize (arith-bufferize) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %c256 = arith.constant 256 : index
    %c4 = arith.constant 4 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %0, 64 : memref<131072xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %1, 64 : memref<131072xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %2, 64 : memref<131072xf32>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
    memref.assume_alignment %3, 64 : memref<131072xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = gpu.thread_id  x
    %5 = arith.muli %4, %c4 : index
    %6 = arith.muli %workgroup_id_x, %c256 : index
    %7 = arith.addi %5, %6 : index
    %8 = vector.transfer_read %0[%7], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %9 = vector.transfer_read %1[%7], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %10 = vector.transfer_read %2[%7], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %11 = arith.addf %8, %9 : vector<4xf32>
    %12 = arith.addf %11, %10 : vector<4xf32>
    vector.transfer_write %12, %3[%7] {in_bounds = [true]} : vector<4xf32>, memref<131072xf32>
    return
  }
}

// -----// IR Dump After FoldTensorExtractOp (iree-codegen-fold-tensor-extract-op) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %c256 = arith.constant 256 : index
    %c4 = arith.constant 4 : index
    %cst = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %0, 64 : memref<131072xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %1, 64 : memref<131072xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %2, 64 : memref<131072xf32>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
    memref.assume_alignment %3, 64 : memref<131072xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = gpu.thread_id  x
    %5 = arith.muli %4, %c4 : index
    %6 = arith.muli %workgroup_id_x, %c256 : index
    %7 = arith.addi %5, %6 : index
    %8 = vector.transfer_read %0[%7], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %9 = vector.transfer_read %1[%7], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %10 = vector.transfer_read %2[%7], %cst {in_bounds = [true]} : memref<131072xf32>, vector<4xf32>
    %11 = arith.addf %8, %9 : vector<4xf32>
    %12 = arith.addf %11, %10 : vector<4xf32>
    vector.transfer_write %12, %3[%7] {in_bounds = [true]} : vector<4xf32>, memref<131072xf32>
    return
  }
}

// -----// IR Dump After LLVMGPUVectorLowering (iree-llvmgpu-vector-lowering) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %c256 = arith.constant 256 : index
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = gpu.thread_id  x
  %5 = arith.muli %4, %c4 : index
  %6 = arith.muli %workgroup_id_x, %c256 : index
  %7 = arith.addi %5, %6 : index
  %8 = vector.load %0[%7] : memref<131072xf32>, vector<4xf32>
  %9 = vector.load %1[%7] : memref<131072xf32>, vector<4xf32>
  %10 = vector.load %2[%7] : memref<131072xf32>, vector<4xf32>
  %11 = arith.addf %8, %9 : vector<4xf32>
  %12 = arith.addf %11, %10 : vector<4xf32>
  vector.store %12, %3[%7] : memref<131072xf32>, vector<4xf32>
  return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %c256 = arith.constant 256 : index
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = gpu.thread_id  x
  %5 = arith.muli %4, %c4 : index
  %6 = arith.muli %workgroup_id_x, %c256 : index
  %7 = arith.addi %5, %6 : index
  %8 = vector.load %0[%7] : memref<131072xf32>, vector<4xf32>
  %9 = vector.load %1[%7] : memref<131072xf32>, vector<4xf32>
  %10 = vector.load %2[%7] : memref<131072xf32>, vector<4xf32>
  %11 = arith.addf %8, %9 : vector<4xf32>
  %12 = arith.addf %11, %10 : vector<4xf32>
  vector.store %12, %3[%7] : memref<131072xf32>, vector<4xf32>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %c256 = arith.constant 256 : index
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = gpu.thread_id  x
  %5 = arith.muli %4, %c4 : index
  %6 = arith.muli %workgroup_id_x, %c256 : index
  %7 = arith.addi %5, %6 : index
  %8 = vector.load %0[%7] : memref<131072xf32>, vector<4xf32>
  %9 = vector.load %1[%7] : memref<131072xf32>, vector<4xf32>
  %10 = vector.load %2[%7] : memref<131072xf32>, vector<4xf32>
  %11 = arith.addf %8, %9 : vector<4xf32>
  %12 = arith.addf %11, %10 : vector<4xf32>
  vector.store %12, %3[%7] : memref<131072xf32>, vector<4xf32>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %c256 = arith.constant 256 : index
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = gpu.thread_id  x
  %5 = arith.muli %4, %c4 : index
  %6 = arith.muli %workgroup_id_x, %c256 : index
  %7 = arith.addi %5, %6 : index
  %8 = vector.load %0[%7] : memref<131072xf32>, vector<4xf32>
  %9 = vector.load %1[%7] : memref<131072xf32>, vector<4xf32>
  %10 = vector.load %2[%7] : memref<131072xf32>, vector<4xf32>
  %11 = arith.addf %8, %9 : vector<4xf32>
  %12 = arith.addf %11, %10 : vector<4xf32>
  vector.store %12, %3[%7] : memref<131072xf32>, vector<4xf32>
  return
}

// -----// IR Dump After PolynomialApproximationPass (iree-codegen-polynomial-approximation) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %c256 = arith.constant 256 : index
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = gpu.thread_id  x
  %5 = arith.muli %4, %c4 : index
  %6 = arith.muli %workgroup_id_x, %c256 : index
  %7 = arith.addi %5, %6 : index
  %8 = vector.load %0[%7] : memref<131072xf32>, vector<4xf32>
  %9 = vector.load %1[%7] : memref<131072xf32>, vector<4xf32>
  %10 = vector.load %2[%7] : memref<131072xf32>, vector<4xf32>
  %11 = arith.addf %8, %9 : vector<4xf32>
  %12 = arith.addf %11, %10 : vector<4xf32>
  vector.store %12, %3[%7] : memref<131072xf32>, vector<4xf32>
  return
}

// -----// IR Dump After ArithExpandOps (arith-expand) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %c256 = arith.constant 256 : index
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = gpu.thread_id  x
  %5 = arith.muli %4, %c4 : index
  %6 = arith.muli %workgroup_id_x, %c256 : index
  %7 = arith.addi %5, %6 : index
  %8 = vector.load %0[%7] : memref<131072xf32>, vector<4xf32>
  %9 = vector.load %1[%7] : memref<131072xf32>, vector<4xf32>
  %10 = vector.load %2[%7] : memref<131072xf32>, vector<4xf32>
  %11 = arith.addf %8, %9 : vector<4xf32>
  %12 = arith.addf %11, %10 : vector<4xf32>
  vector.store %12, %3[%7] : memref<131072xf32>, vector<4xf32>
  return
}

// -----// IR Dump After ExpandOps (memref-expand) //----- //
func.func @twoadd_static_dispatch_0_generic_131072() {
  %c256 = arith.constant 256 : index
  %c4 = arith.constant 4 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %0, 64 : memref<131072xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %1, 64 : memref<131072xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
  memref.assume_alignment %2, 64 : memref<131072xf32>
  %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
  memref.assume_alignment %3, 64 : memref<131072xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %4 = gpu.thread_id  x
  %5 = arith.muli %4, %c4 : index
  %6 = arith.muli %workgroup_id_x, %c256 : index
  %7 = arith.addi %5, %6 : index
  %8 = vector.load %0[%7] : memref<131072xf32>, vector<4xf32>
  %9 = vector.load %1[%7] : memref<131072xf32>, vector<4xf32>
  %10 = vector.load %2[%7] : memref<131072xf32>, vector<4xf32>
  %11 = arith.addf %8, %9 : vector<4xf32>
  %12 = arith.addf %11, %10 : vector<4xf32>
  vector.store %12, %3[%7] : memref<131072xf32>, vector<4xf32>
  return
}

// -----// IR Dump After ExpandStridedMetadata (expand-strided-metadata) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %c256 = arith.constant 256 : index
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %0, 64 : memref<131072xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %1, 64 : memref<131072xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %2, 64 : memref<131072xf32>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
    memref.assume_alignment %3, 64 : memref<131072xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = gpu.thread_id  x
    %5 = arith.muli %4, %c4 : index
    %6 = arith.muli %workgroup_id_x, %c256 : index
    %7 = arith.addi %5, %6 : index
    %8 = vector.load %0[%7] : memref<131072xf32>, vector<4xf32>
    %9 = vector.load %1[%7] : memref<131072xf32>, vector<4xf32>
    %10 = vector.load %2[%7] : memref<131072xf32>, vector<4xf32>
    %11 = arith.addf %8, %9 : vector<4xf32>
    %12 = arith.addf %11, %10 : vector<4xf32>
    vector.store %12, %3[%7] : memref<131072xf32>, vector<4xf32>
    return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %c256 = arith.constant 256 : index
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %0, 64 : memref<131072xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %1, 64 : memref<131072xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %2, 64 : memref<131072xf32>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
    memref.assume_alignment %3, 64 : memref<131072xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = gpu.thread_id  x
    %5 = arith.muli %4, %c4 : index
    %6 = arith.muli %workgroup_id_x, %c256 : index
    %7 = arith.addi %5, %6 : index
    %8 = vector.load %0[%7] : memref<131072xf32>, vector<4xf32>
    %9 = vector.load %1[%7] : memref<131072xf32>, vector<4xf32>
    %10 = vector.load %2[%7] : memref<131072xf32>, vector<4xf32>
    %11 = arith.addf %8, %9 : vector<4xf32>
    %12 = arith.addf %11, %10 : vector<4xf32>
    vector.store %12, %3[%7] : memref<131072xf32>, vector<4xf32>
    return
  }
}

// -----// IR Dump After GPULowerMemorySpaceAttributesPass (gpu-lower-memory-space-attributes) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %c256 = arith.constant 256 : index
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %0, 64 : memref<131072xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %1, 64 : memref<131072xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %2, 64 : memref<131072xf32>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
    memref.assume_alignment %3, 64 : memref<131072xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = gpu.thread_id  x
    %5 = arith.muli %4, %c4 : index
    %6 = arith.muli %workgroup_id_x, %c256 : index
    %7 = arith.addi %5, %6 : index
    %8 = vector.load %0[%7] : memref<131072xf32>, vector<4xf32>
    %9 = vector.load %1[%7] : memref<131072xf32>, vector<4xf32>
    %10 = vector.load %2[%7] : memref<131072xf32>, vector<4xf32>
    %11 = arith.addf %8, %9 : vector<4xf32>
    %12 = arith.addf %11, %10 : vector<4xf32>
    vector.store %12, %3[%7] : memref<131072xf32>, vector<4xf32>
    return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
module {
  func.func @twoadd_static_dispatch_0_generic_131072() {
    %c256 = arith.constant 256 : index
    %c4 = arith.constant 4 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %0, 64 : memref<131072xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %1, 64 : memref<131072xf32>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<131072xf32>
    memref.assume_alignment %2, 64 : memref<131072xf32>
    %3 = hal.interface.binding.subspan set(0) binding(3) type(storage_buffer) alignment(64) offset(%c0) : memref<131072xf32>
    memref.assume_alignment %3, 64 : memref<131072xf32>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %4 = gpu.thread_id  x
    %5 = arith.muli %4, %c4 : index
    %6 = arith.muli %workgroup_id_x, %c256 : index
    %7 = arith.addi %5, %6 : index
    %8 = vector.load %0[%7] : memref<131072xf32>, vector<4xf32>
    %9 = vector.load %1[%7] : memref<131072xf32>, vector<4xf32>
    %10 = vector.load %2[%7] : memref<131072xf32>, vector<4xf32>
    %11 = arith.addf %8, %9 : vector<4xf32>
    %12 = arith.addf %11, %10 : vector<4xf32>
    vector.store %12, %3[%7] : memref<131072xf32>, vector<4xf32>
    return
  }
}

// -----// IR Dump After ConvertToNVVM (iree-convert-to-nvvm) //----- //
module {
  llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
    %0 = llvm.mlir.constant(256 : index) : i64
    %1 = llvm.mlir.constant(4 : index) : i64
    %2 = llvm.mlir.constant(0 : index) : i64
    %3 = llvm.bitcast %arg0 : !llvm.ptr<f32> to !llvm.ptr<i8>
    %4 = llvm.getelementptr %3[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
    %5 = llvm.bitcast %4 : !llvm.ptr<i8> to !llvm.ptr<f32>
    %6 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
    %7 = llvm.insertvalue %5, %6[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %8 = llvm.insertvalue %5, %7[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %9 = llvm.mlir.constant(0 : index) : i64
    %10 = llvm.insertvalue %9, %8[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %11 = llvm.mlir.constant(131072 : index) : i64
    %12 = llvm.insertvalue %11, %10[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %13 = llvm.mlir.constant(1 : index) : i64
    %14 = llvm.insertvalue %13, %12[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %15 = llvm.extractvalue %14[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %16 = llvm.mlir.constant(0 : index) : i64
    %17 = llvm.mlir.constant(63 : index) : i64
    %18 = llvm.ptrtoint %15 : !llvm.ptr<f32> to i64
    %19 = llvm.and %18, %17  : i64
    %20 = llvm.icmp "eq" %19, %16 : i64
    "llvm.intr.assume"(%20) : (i1) -> ()
    %21 = llvm.bitcast %arg1 : !llvm.ptr<f32> to !llvm.ptr<i8>
    %22 = llvm.getelementptr %21[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
    %23 = llvm.bitcast %22 : !llvm.ptr<i8> to !llvm.ptr<f32>
    %24 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
    %25 = llvm.insertvalue %23, %24[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %26 = llvm.insertvalue %23, %25[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %27 = llvm.mlir.constant(0 : index) : i64
    %28 = llvm.insertvalue %27, %26[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %29 = llvm.mlir.constant(131072 : index) : i64
    %30 = llvm.insertvalue %29, %28[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %31 = llvm.mlir.constant(1 : index) : i64
    %32 = llvm.insertvalue %31, %30[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %33 = llvm.extractvalue %32[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %34 = llvm.mlir.constant(0 : index) : i64
    %35 = llvm.mlir.constant(63 : index) : i64
    %36 = llvm.ptrtoint %33 : !llvm.ptr<f32> to i64
    %37 = llvm.and %36, %35  : i64
    %38 = llvm.icmp "eq" %37, %34 : i64
    "llvm.intr.assume"(%38) : (i1) -> ()
    %39 = llvm.bitcast %arg2 : !llvm.ptr<f32> to !llvm.ptr<i8>
    %40 = llvm.getelementptr %39[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
    %41 = llvm.bitcast %40 : !llvm.ptr<i8> to !llvm.ptr<f32>
    %42 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
    %43 = llvm.insertvalue %41, %42[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %44 = llvm.insertvalue %41, %43[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %45 = llvm.mlir.constant(0 : index) : i64
    %46 = llvm.insertvalue %45, %44[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %47 = llvm.mlir.constant(131072 : index) : i64
    %48 = llvm.insertvalue %47, %46[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %49 = llvm.mlir.constant(1 : index) : i64
    %50 = llvm.insertvalue %49, %48[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %51 = llvm.extractvalue %50[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %52 = llvm.mlir.constant(0 : index) : i64
    %53 = llvm.mlir.constant(63 : index) : i64
    %54 = llvm.ptrtoint %51 : !llvm.ptr<f32> to i64
    %55 = llvm.and %54, %53  : i64
    %56 = llvm.icmp "eq" %55, %52 : i64
    "llvm.intr.assume"(%56) : (i1) -> ()
    %57 = llvm.bitcast %arg3 : !llvm.ptr<f32> to !llvm.ptr<i8>
    %58 = llvm.getelementptr %57[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
    %59 = llvm.bitcast %58 : !llvm.ptr<i8> to !llvm.ptr<f32>
    %60 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
    %61 = llvm.insertvalue %59, %60[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %62 = llvm.insertvalue %59, %61[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %63 = llvm.mlir.constant(0 : index) : i64
    %64 = llvm.insertvalue %63, %62[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %65 = llvm.mlir.constant(131072 : index) : i64
    %66 = llvm.insertvalue %65, %64[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %67 = llvm.mlir.constant(1 : index) : i64
    %68 = llvm.insertvalue %67, %66[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %69 = llvm.extractvalue %68[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %70 = llvm.mlir.constant(0 : index) : i64
    %71 = llvm.mlir.constant(63 : index) : i64
    %72 = llvm.ptrtoint %69 : !llvm.ptr<f32> to i64
    %73 = llvm.and %72, %71  : i64
    %74 = llvm.icmp "eq" %73, %70 : i64
    "llvm.intr.assume"(%74) : (i1) -> ()
    %75 = nvvm.read.ptx.sreg.ctaid.x : i32
    %76 = llvm.sext %75 : i32 to i64
    %77 = nvvm.read.ptx.sreg.tid.x : i32
    %78 = llvm.sext %77 : i32 to i64
    %79 = llvm.mul %78, %1  : i64
    %80 = llvm.mul %76, %0  : i64
    %81 = llvm.add %79, %80  : i64
    %82 = llvm.extractvalue %14[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %83 = llvm.getelementptr %82[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
    %84 = llvm.bitcast %83 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
    %85 = llvm.load %84 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
    %86 = llvm.extractvalue %32[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %87 = llvm.getelementptr %86[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
    %88 = llvm.bitcast %87 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
    %89 = llvm.load %88 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
    %90 = llvm.extractvalue %50[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %91 = llvm.getelementptr %90[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
    %92 = llvm.bitcast %91 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
    %93 = llvm.load %92 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
    %94 = llvm.fadd %85, %89  : vector<4xf32>
    %95 = llvm.fadd %94, %93  : vector<4xf32>
    %96 = llvm.extractvalue %68[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %97 = llvm.getelementptr %96[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
    %98 = llvm.bitcast %97 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
    llvm.store %95, %98 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
    llvm.return
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::TranslateTargetExecutableVariantsPass (iree-hal-translate-target-executable-variants) //----- //
hal.executable.variant public @cuda_nvptx_fb, target = <"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}> {
  hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<LLVMGPUVectorize>, workgroup_size = [64 : index, 1 : index, 1 : index]} {
  ^bb0(%arg0: !hal.device, %arg1: index):
    %c512 = arith.constant 512 : index
    %c1 = arith.constant 1 : index
    hal.return %c512, %c1, %c1 : index, index, index
  }
  builtin.module {
    llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
      %0 = llvm.mlir.constant(256 : index) : i64
      %1 = llvm.mlir.constant(4 : index) : i64
      %2 = llvm.mlir.constant(0 : index) : i64
      %3 = llvm.bitcast %arg0 : !llvm.ptr<f32> to !llvm.ptr<i8>
      %4 = llvm.getelementptr %3[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
      %5 = llvm.bitcast %4 : !llvm.ptr<i8> to !llvm.ptr<f32>
      %6 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
      %7 = llvm.insertvalue %5, %6[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %8 = llvm.insertvalue %5, %7[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %9 = llvm.mlir.constant(0 : index) : i64
      %10 = llvm.insertvalue %9, %8[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %11 = llvm.mlir.constant(131072 : index) : i64
      %12 = llvm.insertvalue %11, %10[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %13 = llvm.mlir.constant(1 : index) : i64
      %14 = llvm.insertvalue %13, %12[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %15 = llvm.extractvalue %14[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %16 = llvm.mlir.constant(0 : index) : i64
      %17 = llvm.mlir.constant(63 : index) : i64
      %18 = llvm.ptrtoint %15 : !llvm.ptr<f32> to i64
      %19 = llvm.and %18, %17  : i64
      %20 = llvm.icmp "eq" %19, %16 : i64
      "llvm.intr.assume"(%20) : (i1) -> ()
      %21 = llvm.bitcast %arg1 : !llvm.ptr<f32> to !llvm.ptr<i8>
      %22 = llvm.getelementptr %21[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
      %23 = llvm.bitcast %22 : !llvm.ptr<i8> to !llvm.ptr<f32>
      %24 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
      %25 = llvm.insertvalue %23, %24[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %26 = llvm.insertvalue %23, %25[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %27 = llvm.mlir.constant(0 : index) : i64
      %28 = llvm.insertvalue %27, %26[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %29 = llvm.mlir.constant(131072 : index) : i64
      %30 = llvm.insertvalue %29, %28[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %31 = llvm.mlir.constant(1 : index) : i64
      %32 = llvm.insertvalue %31, %30[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %33 = llvm.extractvalue %32[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %34 = llvm.mlir.constant(0 : index) : i64
      %35 = llvm.mlir.constant(63 : index) : i64
      %36 = llvm.ptrtoint %33 : !llvm.ptr<f32> to i64
      %37 = llvm.and %36, %35  : i64
      %38 = llvm.icmp "eq" %37, %34 : i64
      "llvm.intr.assume"(%38) : (i1) -> ()
      %39 = llvm.bitcast %arg2 : !llvm.ptr<f32> to !llvm.ptr<i8>
      %40 = llvm.getelementptr %39[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
      %41 = llvm.bitcast %40 : !llvm.ptr<i8> to !llvm.ptr<f32>
      %42 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
      %43 = llvm.insertvalue %41, %42[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %44 = llvm.insertvalue %41, %43[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %45 = llvm.mlir.constant(0 : index) : i64
      %46 = llvm.insertvalue %45, %44[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %47 = llvm.mlir.constant(131072 : index) : i64
      %48 = llvm.insertvalue %47, %46[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %49 = llvm.mlir.constant(1 : index) : i64
      %50 = llvm.insertvalue %49, %48[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %51 = llvm.extractvalue %50[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %52 = llvm.mlir.constant(0 : index) : i64
      %53 = llvm.mlir.constant(63 : index) : i64
      %54 = llvm.ptrtoint %51 : !llvm.ptr<f32> to i64
      %55 = llvm.and %54, %53  : i64
      %56 = llvm.icmp "eq" %55, %52 : i64
      "llvm.intr.assume"(%56) : (i1) -> ()
      %57 = llvm.bitcast %arg3 : !llvm.ptr<f32> to !llvm.ptr<i8>
      %58 = llvm.getelementptr %57[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
      %59 = llvm.bitcast %58 : !llvm.ptr<i8> to !llvm.ptr<f32>
      %60 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
      %61 = llvm.insertvalue %59, %60[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %62 = llvm.insertvalue %59, %61[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %63 = llvm.mlir.constant(0 : index) : i64
      %64 = llvm.insertvalue %63, %62[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %65 = llvm.mlir.constant(131072 : index) : i64
      %66 = llvm.insertvalue %65, %64[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %67 = llvm.mlir.constant(1 : index) : i64
      %68 = llvm.insertvalue %67, %66[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %69 = llvm.extractvalue %68[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %70 = llvm.mlir.constant(0 : index) : i64
      %71 = llvm.mlir.constant(63 : index) : i64
      %72 = llvm.ptrtoint %69 : !llvm.ptr<f32> to i64
      %73 = llvm.and %72, %71  : i64
      %74 = llvm.icmp "eq" %73, %70 : i64
      "llvm.intr.assume"(%74) : (i1) -> ()
      %75 = nvvm.read.ptx.sreg.ctaid.x : i32
      %76 = llvm.sext %75 : i32 to i64
      %77 = nvvm.read.ptx.sreg.tid.x : i32
      %78 = llvm.sext %77 : i32 to i64
      %79 = llvm.mul %78, %1  : i64
      %80 = llvm.mul %76, %0  : i64
      %81 = llvm.add %79, %80  : i64
      %82 = llvm.extractvalue %14[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %83 = llvm.getelementptr %82[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
      %84 = llvm.bitcast %83 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
      %85 = llvm.load %84 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
      %86 = llvm.extractvalue %32[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %87 = llvm.getelementptr %86[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
      %88 = llvm.bitcast %87 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
      %89 = llvm.load %88 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
      %90 = llvm.extractvalue %50[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %91 = llvm.getelementptr %90[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
      %92 = llvm.bitcast %91 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
      %93 = llvm.load %92 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
      %94 = llvm.fadd %85, %89  : vector<4xf32>
      %95 = llvm.fadd %94, %93  : vector<4xf32>
      %96 = llvm.extractvalue %68[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
      %97 = llvm.getelementptr %96[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
      %98 = llvm.bitcast %97 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
      llvm.store %95, %98 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
      llvm.return
    }
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::TranslateExecutablesPass (iree-hal-translate-executables) //----- //
hal.executable private @twoadd_static_dispatch_0 {
  hal.executable.variant public @cuda_nvptx_fb, target = <"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}> {
    hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<LLVMGPUVectorize>, workgroup_size = [64 : index, 1 : index, 1 : index]} {
    ^bb0(%arg0: !hal.device, %arg1: index):
      %c512 = arith.constant 512 : index
      %c1 = arith.constant 1 : index
      hal.return %c512, %c1, %c1 : index, index, index
    }
    builtin.module {
      llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
        %0 = llvm.mlir.constant(256 : index) : i64
        %1 = llvm.mlir.constant(4 : index) : i64
        %2 = llvm.mlir.constant(0 : index) : i64
        %3 = llvm.bitcast %arg0 : !llvm.ptr<f32> to !llvm.ptr<i8>
        %4 = llvm.getelementptr %3[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
        %5 = llvm.bitcast %4 : !llvm.ptr<i8> to !llvm.ptr<f32>
        %6 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
        %7 = llvm.insertvalue %5, %6[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %8 = llvm.insertvalue %5, %7[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %9 = llvm.mlir.constant(0 : index) : i64
        %10 = llvm.insertvalue %9, %8[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %11 = llvm.mlir.constant(131072 : index) : i64
        %12 = llvm.insertvalue %11, %10[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %13 = llvm.mlir.constant(1 : index) : i64
        %14 = llvm.insertvalue %13, %12[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %15 = llvm.extractvalue %14[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %16 = llvm.mlir.constant(0 : index) : i64
        %17 = llvm.mlir.constant(63 : index) : i64
        %18 = llvm.ptrtoint %15 : !llvm.ptr<f32> to i64
        %19 = llvm.and %18, %17  : i64
        %20 = llvm.icmp "eq" %19, %16 : i64
        "llvm.intr.assume"(%20) : (i1) -> ()
        %21 = llvm.bitcast %arg1 : !llvm.ptr<f32> to !llvm.ptr<i8>
        %22 = llvm.getelementptr %21[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
        %23 = llvm.bitcast %22 : !llvm.ptr<i8> to !llvm.ptr<f32>
        %24 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
        %25 = llvm.insertvalue %23, %24[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %26 = llvm.insertvalue %23, %25[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %27 = llvm.mlir.constant(0 : index) : i64
        %28 = llvm.insertvalue %27, %26[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %29 = llvm.mlir.constant(131072 : index) : i64
        %30 = llvm.insertvalue %29, %28[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %31 = llvm.mlir.constant(1 : index) : i64
        %32 = llvm.insertvalue %31, %30[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %33 = llvm.extractvalue %32[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %34 = llvm.mlir.constant(0 : index) : i64
        %35 = llvm.mlir.constant(63 : index) : i64
        %36 = llvm.ptrtoint %33 : !llvm.ptr<f32> to i64
        %37 = llvm.and %36, %35  : i64
        %38 = llvm.icmp "eq" %37, %34 : i64
        "llvm.intr.assume"(%38) : (i1) -> ()
        %39 = llvm.bitcast %arg2 : !llvm.ptr<f32> to !llvm.ptr<i8>
        %40 = llvm.getelementptr %39[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
        %41 = llvm.bitcast %40 : !llvm.ptr<i8> to !llvm.ptr<f32>
        %42 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
        %43 = llvm.insertvalue %41, %42[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %44 = llvm.insertvalue %41, %43[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %45 = llvm.mlir.constant(0 : index) : i64
        %46 = llvm.insertvalue %45, %44[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %47 = llvm.mlir.constant(131072 : index) : i64
        %48 = llvm.insertvalue %47, %46[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %49 = llvm.mlir.constant(1 : index) : i64
        %50 = llvm.insertvalue %49, %48[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %51 = llvm.extractvalue %50[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %52 = llvm.mlir.constant(0 : index) : i64
        %53 = llvm.mlir.constant(63 : index) : i64
        %54 = llvm.ptrtoint %51 : !llvm.ptr<f32> to i64
        %55 = llvm.and %54, %53  : i64
        %56 = llvm.icmp "eq" %55, %52 : i64
        "llvm.intr.assume"(%56) : (i1) -> ()
        %57 = llvm.bitcast %arg3 : !llvm.ptr<f32> to !llvm.ptr<i8>
        %58 = llvm.getelementptr %57[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
        %59 = llvm.bitcast %58 : !llvm.ptr<i8> to !llvm.ptr<f32>
        %60 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
        %61 = llvm.insertvalue %59, %60[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %62 = llvm.insertvalue %59, %61[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %63 = llvm.mlir.constant(0 : index) : i64
        %64 = llvm.insertvalue %63, %62[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %65 = llvm.mlir.constant(131072 : index) : i64
        %66 = llvm.insertvalue %65, %64[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %67 = llvm.mlir.constant(1 : index) : i64
        %68 = llvm.insertvalue %67, %66[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %69 = llvm.extractvalue %68[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %70 = llvm.mlir.constant(0 : index) : i64
        %71 = llvm.mlir.constant(63 : index) : i64
        %72 = llvm.ptrtoint %69 : !llvm.ptr<f32> to i64
        %73 = llvm.and %72, %71  : i64
        %74 = llvm.icmp "eq" %73, %70 : i64
        "llvm.intr.assume"(%74) : (i1) -> ()
        %75 = nvvm.read.ptx.sreg.ctaid.x : i32
        %76 = llvm.sext %75 : i32 to i64
        %77 = nvvm.read.ptx.sreg.tid.x : i32
        %78 = llvm.sext %77 : i32 to i64
        %79 = llvm.mul %78, %1  : i64
        %80 = llvm.mul %76, %0  : i64
        %81 = llvm.add %79, %80  : i64
        %82 = llvm.extractvalue %14[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %83 = llvm.getelementptr %82[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
        %84 = llvm.bitcast %83 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
        %85 = llvm.load %84 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
        %86 = llvm.extractvalue %32[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %87 = llvm.getelementptr %86[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
        %88 = llvm.bitcast %87 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
        %89 = llvm.load %88 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
        %90 = llvm.extractvalue %50[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %91 = llvm.getelementptr %90[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
        %92 = llvm.bitcast %91 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
        %93 = llvm.load %92 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
        %94 = llvm.fadd %85, %89  : vector<4xf32>
        %95 = llvm.fadd %94, %93  : vector<4xf32>
        %96 = llvm.extractvalue %68[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
        %97 = llvm.getelementptr %96[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
        %98 = llvm.bitcast %97 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
        llvm.store %95, %98 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
        llvm.return
      }
    }
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::(anonymous namespace)::ConvertToHALPass (iree-hal-conversion) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(256 : index) : i64
          %1 = llvm.mlir.constant(4 : index) : i64
          %2 = llvm.mlir.constant(0 : index) : i64
          %3 = llvm.bitcast %arg0 : !llvm.ptr<f32> to !llvm.ptr<i8>
          %4 = llvm.getelementptr %3[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
          %5 = llvm.bitcast %4 : !llvm.ptr<i8> to !llvm.ptr<f32>
          %6 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
          %7 = llvm.insertvalue %5, %6[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %8 = llvm.insertvalue %5, %7[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %9 = llvm.mlir.constant(0 : index) : i64
          %10 = llvm.insertvalue %9, %8[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %11 = llvm.mlir.constant(131072 : index) : i64
          %12 = llvm.insertvalue %11, %10[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %13 = llvm.mlir.constant(1 : index) : i64
          %14 = llvm.insertvalue %13, %12[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %15 = llvm.extractvalue %14[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %16 = llvm.mlir.constant(0 : index) : i64
          %17 = llvm.mlir.constant(63 : index) : i64
          %18 = llvm.ptrtoint %15 : !llvm.ptr<f32> to i64
          %19 = llvm.and %18, %17  : i64
          %20 = llvm.icmp "eq" %19, %16 : i64
          "llvm.intr.assume"(%20) : (i1) -> ()
          %21 = llvm.bitcast %arg1 : !llvm.ptr<f32> to !llvm.ptr<i8>
          %22 = llvm.getelementptr %21[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
          %23 = llvm.bitcast %22 : !llvm.ptr<i8> to !llvm.ptr<f32>
          %24 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
          %25 = llvm.insertvalue %23, %24[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %26 = llvm.insertvalue %23, %25[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %27 = llvm.mlir.constant(0 : index) : i64
          %28 = llvm.insertvalue %27, %26[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %29 = llvm.mlir.constant(131072 : index) : i64
          %30 = llvm.insertvalue %29, %28[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %31 = llvm.mlir.constant(1 : index) : i64
          %32 = llvm.insertvalue %31, %30[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %33 = llvm.extractvalue %32[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %34 = llvm.mlir.constant(0 : index) : i64
          %35 = llvm.mlir.constant(63 : index) : i64
          %36 = llvm.ptrtoint %33 : !llvm.ptr<f32> to i64
          %37 = llvm.and %36, %35  : i64
          %38 = llvm.icmp "eq" %37, %34 : i64
          "llvm.intr.assume"(%38) : (i1) -> ()
          %39 = llvm.bitcast %arg2 : !llvm.ptr<f32> to !llvm.ptr<i8>
          %40 = llvm.getelementptr %39[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
          %41 = llvm.bitcast %40 : !llvm.ptr<i8> to !llvm.ptr<f32>
          %42 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
          %43 = llvm.insertvalue %41, %42[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %44 = llvm.insertvalue %41, %43[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %45 = llvm.mlir.constant(0 : index) : i64
          %46 = llvm.insertvalue %45, %44[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %47 = llvm.mlir.constant(131072 : index) : i64
          %48 = llvm.insertvalue %47, %46[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %49 = llvm.mlir.constant(1 : index) : i64
          %50 = llvm.insertvalue %49, %48[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %51 = llvm.extractvalue %50[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %52 = llvm.mlir.constant(0 : index) : i64
          %53 = llvm.mlir.constant(63 : index) : i64
          %54 = llvm.ptrtoint %51 : !llvm.ptr<f32> to i64
          %55 = llvm.and %54, %53  : i64
          %56 = llvm.icmp "eq" %55, %52 : i64
          "llvm.intr.assume"(%56) : (i1) -> ()
          %57 = llvm.bitcast %arg3 : !llvm.ptr<f32> to !llvm.ptr<i8>
          %58 = llvm.getelementptr %57[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
          %59 = llvm.bitcast %58 : !llvm.ptr<i8> to !llvm.ptr<f32>
          %60 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
          %61 = llvm.insertvalue %59, %60[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %62 = llvm.insertvalue %59, %61[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %63 = llvm.mlir.constant(0 : index) : i64
          %64 = llvm.insertvalue %63, %62[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %65 = llvm.mlir.constant(131072 : index) : i64
          %66 = llvm.insertvalue %65, %64[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %67 = llvm.mlir.constant(1 : index) : i64
          %68 = llvm.insertvalue %67, %66[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %69 = llvm.extractvalue %68[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %70 = llvm.mlir.constant(0 : index) : i64
          %71 = llvm.mlir.constant(63 : index) : i64
          %72 = llvm.ptrtoint %69 : !llvm.ptr<f32> to i64
          %73 = llvm.and %72, %71  : i64
          %74 = llvm.icmp "eq" %73, %70 : i64
          "llvm.intr.assume"(%74) : (i1) -> ()
          %75 = nvvm.read.ptx.sreg.ctaid.x : i32
          %76 = llvm.sext %75 : i32 to i64
          %77 = nvvm.read.ptx.sreg.tid.x : i32
          %78 = llvm.sext %77 : i32 to i64
          %79 = llvm.mul %78, %1  : i64
          %80 = llvm.mul %76, %0  : i64
          %81 = llvm.add %79, %80  : i64
          %82 = llvm.extractvalue %14[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %83 = llvm.getelementptr %82[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %84 = llvm.bitcast %83 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %85 = llvm.load %84 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %86 = llvm.extractvalue %32[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %87 = llvm.getelementptr %86[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %88 = llvm.bitcast %87 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %89 = llvm.load %88 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %90 = llvm.extractvalue %50[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %91 = llvm.getelementptr %90[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %92 = llvm.bitcast %91 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %93 = llvm.load %92 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %94 = llvm.fadd %85, %89  : vector<4xf32>
          %95 = llvm.fadd %94, %93  : vector<4xf32>
          %96 = llvm.extractvalue %68[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %97 = llvm.getelementptr %96[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %98 = llvm.bitcast %97 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %95, %98 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    %device_1 = hal.ex.shared_device : !hal.device
    %allocator_2 = hal.device.allocator<%device_1 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator_2 : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_3 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    %device_4 = hal.ex.shared_device : !hal.device
    %allocator_5 = hal.device.allocator<%device_4 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_3 : !hal.buffer> message("tensor") allocator(%allocator_5 : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %device_6 = hal.ex.shared_device : !hal.device
    %allocator_7 = hal.device.allocator<%device_6 : !hal.device> : !hal.allocator
    %buffer_8 = hal.allocator.allocate<%allocator_7 : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %device_9 = hal.ex.shared_device : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %cmd = hal.command_buffer.create device(%device_9 : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %0 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    hal.device.switch<%0 : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%0 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      %c0_15 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c2 = arith.constant 2 : index
      %c3 = arith.constant 3 : index
      %c0_16 = arith.constant 0 : index
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0_16] bindings([
        %c0_15 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
        %c2 = (%buffer_3 : !hal.buffer)[%c0, %c524288], 
        %c3 = (%buffer_8 : !hal.buffer)[%c0, %c524288]
      ])
      %c512_17 = arith.constant 512 : index
      %c1_18 = arith.constant 1 : index
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@twoadd_static_dispatch_0::@cuda_nvptx_fb::@twoadd_static_dispatch_0_generic_131072) workgroups([%c512_17, %c1_18, %c1_18])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %1 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_9 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_9 : !hal.device> affinity(%c-1_i64) wait(%1) signal(%fence) commands([%cmd])
    %c-1_i32 = arith.constant -1 : i32
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %c512_10 = arith.constant 512 : index
    %c256_11 = arith.constant 256 : index
    %c0_12 = arith.constant 0 : index
    %c553648160_i32_13 = arith.constant 553648160 : i32
    %c1_i32_14 = arith.constant 1 : i32
    %view = hal.buffer_view.create buffer(%buffer_8 : !hal.buffer)[%c0_12, %c524288] shape([%c512_10, %c256_11]) type(%c553648160_i32_13) encoding(%c1_i32_14) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::FixupLegacySyncPass (iree-hal-fixup-legacy-sync) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(256 : index) : i64
          %1 = llvm.mlir.constant(4 : index) : i64
          %2 = llvm.mlir.constant(0 : index) : i64
          %3 = llvm.bitcast %arg0 : !llvm.ptr<f32> to !llvm.ptr<i8>
          %4 = llvm.getelementptr %3[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
          %5 = llvm.bitcast %4 : !llvm.ptr<i8> to !llvm.ptr<f32>
          %6 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
          %7 = llvm.insertvalue %5, %6[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %8 = llvm.insertvalue %5, %7[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %9 = llvm.mlir.constant(0 : index) : i64
          %10 = llvm.insertvalue %9, %8[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %11 = llvm.mlir.constant(131072 : index) : i64
          %12 = llvm.insertvalue %11, %10[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %13 = llvm.mlir.constant(1 : index) : i64
          %14 = llvm.insertvalue %13, %12[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %15 = llvm.extractvalue %14[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %16 = llvm.mlir.constant(0 : index) : i64
          %17 = llvm.mlir.constant(63 : index) : i64
          %18 = llvm.ptrtoint %15 : !llvm.ptr<f32> to i64
          %19 = llvm.and %18, %17  : i64
          %20 = llvm.icmp "eq" %19, %16 : i64
          "llvm.intr.assume"(%20) : (i1) -> ()
          %21 = llvm.bitcast %arg1 : !llvm.ptr<f32> to !llvm.ptr<i8>
          %22 = llvm.getelementptr %21[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
          %23 = llvm.bitcast %22 : !llvm.ptr<i8> to !llvm.ptr<f32>
          %24 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
          %25 = llvm.insertvalue %23, %24[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %26 = llvm.insertvalue %23, %25[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %27 = llvm.mlir.constant(0 : index) : i64
          %28 = llvm.insertvalue %27, %26[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %29 = llvm.mlir.constant(131072 : index) : i64
          %30 = llvm.insertvalue %29, %28[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %31 = llvm.mlir.constant(1 : index) : i64
          %32 = llvm.insertvalue %31, %30[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %33 = llvm.extractvalue %32[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %34 = llvm.mlir.constant(0 : index) : i64
          %35 = llvm.mlir.constant(63 : index) : i64
          %36 = llvm.ptrtoint %33 : !llvm.ptr<f32> to i64
          %37 = llvm.and %36, %35  : i64
          %38 = llvm.icmp "eq" %37, %34 : i64
          "llvm.intr.assume"(%38) : (i1) -> ()
          %39 = llvm.bitcast %arg2 : !llvm.ptr<f32> to !llvm.ptr<i8>
          %40 = llvm.getelementptr %39[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
          %41 = llvm.bitcast %40 : !llvm.ptr<i8> to !llvm.ptr<f32>
          %42 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
          %43 = llvm.insertvalue %41, %42[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %44 = llvm.insertvalue %41, %43[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %45 = llvm.mlir.constant(0 : index) : i64
          %46 = llvm.insertvalue %45, %44[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %47 = llvm.mlir.constant(131072 : index) : i64
          %48 = llvm.insertvalue %47, %46[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %49 = llvm.mlir.constant(1 : index) : i64
          %50 = llvm.insertvalue %49, %48[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %51 = llvm.extractvalue %50[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %52 = llvm.mlir.constant(0 : index) : i64
          %53 = llvm.mlir.constant(63 : index) : i64
          %54 = llvm.ptrtoint %51 : !llvm.ptr<f32> to i64
          %55 = llvm.and %54, %53  : i64
          %56 = llvm.icmp "eq" %55, %52 : i64
          "llvm.intr.assume"(%56) : (i1) -> ()
          %57 = llvm.bitcast %arg3 : !llvm.ptr<f32> to !llvm.ptr<i8>
          %58 = llvm.getelementptr %57[%2] : (!llvm.ptr<i8>, i64) -> !llvm.ptr<i8>
          %59 = llvm.bitcast %58 : !llvm.ptr<i8> to !llvm.ptr<f32>
          %60 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
          %61 = llvm.insertvalue %59, %60[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %62 = llvm.insertvalue %59, %61[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %63 = llvm.mlir.constant(0 : index) : i64
          %64 = llvm.insertvalue %63, %62[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %65 = llvm.mlir.constant(131072 : index) : i64
          %66 = llvm.insertvalue %65, %64[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %67 = llvm.mlir.constant(1 : index) : i64
          %68 = llvm.insertvalue %67, %66[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %69 = llvm.extractvalue %68[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %70 = llvm.mlir.constant(0 : index) : i64
          %71 = llvm.mlir.constant(63 : index) : i64
          %72 = llvm.ptrtoint %69 : !llvm.ptr<f32> to i64
          %73 = llvm.and %72, %71  : i64
          %74 = llvm.icmp "eq" %73, %70 : i64
          "llvm.intr.assume"(%74) : (i1) -> ()
          %75 = nvvm.read.ptx.sreg.ctaid.x : i32
          %76 = llvm.sext %75 : i32 to i64
          %77 = nvvm.read.ptx.sreg.tid.x : i32
          %78 = llvm.sext %77 : i32 to i64
          %79 = llvm.mul %78, %1  : i64
          %80 = llvm.mul %76, %0  : i64
          %81 = llvm.add %79, %80  : i64
          %82 = llvm.extractvalue %14[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %83 = llvm.getelementptr %82[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %84 = llvm.bitcast %83 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %85 = llvm.load %84 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %86 = llvm.extractvalue %32[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %87 = llvm.getelementptr %86[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %88 = llvm.bitcast %87 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %89 = llvm.load %88 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %90 = llvm.extractvalue %50[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %91 = llvm.getelementptr %90[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %92 = llvm.bitcast %91 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %93 = llvm.load %92 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %94 = llvm.fadd %85, %89  : vector<4xf32>
          %95 = llvm.fadd %94, %93  : vector<4xf32>
          %96 = llvm.extractvalue %68[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
          %97 = llvm.getelementptr %96[%81] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %98 = llvm.bitcast %97 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %95, %98 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c131072 = arith.constant 131072 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    %device_1 = hal.ex.shared_device : !hal.device
    %allocator_2 = hal.device.allocator<%device_1 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator_2 : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_3 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    %device_4 = hal.ex.shared_device : !hal.device
    %allocator_5 = hal.device.allocator<%device_4 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_3 : !hal.buffer> message("tensor") allocator(%allocator_5 : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %device_6 = hal.ex.shared_device : !hal.device
    %allocator_7 = hal.device.allocator<%device_6 : !hal.device> : !hal.allocator
    %buffer_8 = hal.allocator.allocate<%allocator_7 : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %device_9 = hal.ex.shared_device : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %cmd = hal.command_buffer.create device(%device_9 : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %0 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    hal.device.switch<%0 : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%0 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      %c0_17 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c2 = arith.constant 2 : index
      %c3 = arith.constant 3 : index
      %c0_18 = arith.constant 0 : index
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0_18] bindings([
        %c0_17 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
        %c2 = (%buffer_3 : !hal.buffer)[%c0, %c524288], 
        %c3 = (%buffer_8 : !hal.buffer)[%c0, %c524288]
      ])
      %c512_19 = arith.constant 512 : index
      %c1_20 = arith.constant 1 : index
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@twoadd_static_dispatch_0::@cuda_nvptx_fb::@twoadd_static_dispatch_0_generic_131072) workgroups([%c512_19, %c1_20, %c1_20])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %1 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_9 : !hal.device) flags("None") : !hal.fence
    %c-1_i32 = arith.constant -1 : i32
    %status = hal.fence.await until([%1]) timeout_millis(%c-1_i32) : i32
    hal.device.queue.execute<%device_9 : !hal.device> affinity(%c-1_i64) wait(%1) signal(%fence) commands([%cmd])
    %c-1_i32_10 = arith.constant -1 : i32
    %status_11 = hal.fence.await until([%fence]) timeout_millis(%c-1_i32_10) : i32
    util.status.check_ok %status_11, "failed to wait on timepoint"
    %c512_12 = arith.constant 512 : index
    %c256_13 = arith.constant 256 : index
    %c0_14 = arith.constant 0 : index
    %c553648160_i32_15 = arith.constant 553648160 : i32
    %c1_i32_16 = arith.constant 1 : i32
    %view = hal.buffer_view.create buffer(%buffer_8 : !hal.buffer)[%c0_14, %c524288] shape([%c512_12, %c256_13]) type(%c553648160_i32_15) encoding(%c1_i32_16) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(256 : index) : i64
          %2 = llvm.mlir.constant(4 : index) : i64
          %3 = llvm.mlir.constant(0 : index) : i64
          %4 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %5 = llvm.and %4, %0  : i64
          %6 = llvm.icmp "eq" %5, %3 : i64
          "llvm.intr.assume"(%6) : (i1) -> ()
          %7 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %8 = llvm.and %7, %0  : i64
          %9 = llvm.icmp "eq" %8, %3 : i64
          "llvm.intr.assume"(%9) : (i1) -> ()
          %10 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %11 = llvm.and %10, %0  : i64
          %12 = llvm.icmp "eq" %11, %3 : i64
          "llvm.intr.assume"(%12) : (i1) -> ()
          %13 = llvm.ptrtoint %arg3 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %3 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = nvvm.read.ptx.sreg.ctaid.x : i32
          %17 = llvm.sext %16 : i32 to i64
          %18 = nvvm.read.ptx.sreg.tid.x : i32
          %19 = llvm.sext %18 : i32 to i64
          %20 = llvm.mul %19, %2  : i64
          %21 = llvm.mul %17, %1  : i64
          %22 = llvm.add %20, %21  : i64
          %23 = llvm.getelementptr %arg0[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %24 = llvm.bitcast %23 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %25 = llvm.load %24 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %26 = llvm.getelementptr %arg1[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %27 = llvm.bitcast %26 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %28 = llvm.load %27 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %29 = llvm.getelementptr %arg2[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %30 = llvm.bitcast %29 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %32 = llvm.fadd %25, %28  : vector<4xf32>
          %33 = llvm.fadd %32, %31  : vector<4xf32>
          %34 = llvm.getelementptr %arg3[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %35 = llvm.bitcast %34 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %33, %35 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    %device_1 = hal.ex.shared_device : !hal.device
    %allocator_2 = hal.device.allocator<%device_1 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator_2 : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_3 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    %device_4 = hal.ex.shared_device : !hal.device
    %allocator_5 = hal.device.allocator<%device_4 : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer_3 : !hal.buffer> message("tensor") allocator(%allocator_5 : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %device_6 = hal.ex.shared_device : !hal.device
    %allocator_7 = hal.device.allocator<%device_6 : !hal.device> : !hal.allocator
    %buffer_8 = hal.allocator.allocate<%allocator_7 : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %device_9 = hal.ex.shared_device : !hal.device
    %cmd = hal.command_buffer.create device(%device_9 : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device_9 : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device_9 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
        %c2 = (%buffer_3 : !hal.buffer)[%c0, %c524288], 
        %c3 = (%buffer_8 : !hal.buffer)[%c0, %c524288]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@twoadd_static_dispatch_0::@cuda_nvptx_fb::@twoadd_static_dispatch_0_generic_131072) workgroups([%c512, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_9 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_9 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_8 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(256 : index) : i64
          %2 = llvm.mlir.constant(4 : index) : i64
          %3 = llvm.mlir.constant(0 : index) : i64
          %4 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %5 = llvm.and %4, %0  : i64
          %6 = llvm.icmp "eq" %5, %3 : i64
          "llvm.intr.assume"(%6) : (i1) -> ()
          %7 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %8 = llvm.and %7, %0  : i64
          %9 = llvm.icmp "eq" %8, %3 : i64
          "llvm.intr.assume"(%9) : (i1) -> ()
          %10 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %11 = llvm.and %10, %0  : i64
          %12 = llvm.icmp "eq" %11, %3 : i64
          "llvm.intr.assume"(%12) : (i1) -> ()
          %13 = llvm.ptrtoint %arg3 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %3 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = nvvm.read.ptx.sreg.ctaid.x : i32
          %17 = llvm.sext %16 : i32 to i64
          %18 = nvvm.read.ptx.sreg.tid.x : i32
          %19 = llvm.sext %18 : i32 to i64
          %20 = llvm.mul %19, %2  : i64
          %21 = llvm.mul %17, %1  : i64
          %22 = llvm.add %20, %21  : i64
          %23 = llvm.getelementptr %arg0[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %24 = llvm.bitcast %23 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %25 = llvm.load %24 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %26 = llvm.getelementptr %arg1[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %27 = llvm.bitcast %26 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %28 = llvm.load %27 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %29 = llvm.getelementptr %arg2[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %30 = llvm.bitcast %29 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %32 = llvm.fadd %25, %28  : vector<4xf32>
          %33 = llvm.fadd %32, %31  : vector<4xf32>
          %34 = llvm.getelementptr %arg3[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %35 = llvm.bitcast %34 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %33, %35 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
        %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
        %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@twoadd_static_dispatch_0::@cuda_nvptx_fb::@twoadd_static_dispatch_0_generic_131072) workgroups([%c512, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
    %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>) : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@twoadd_static_dispatch_0::@cuda_nvptx_fb::@twoadd_static_dispatch_0_generic_131072) workgroups([%c512, %c1, %c1])
    hal.return
  }
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(256 : index) : i64
          %2 = llvm.mlir.constant(4 : index) : i64
          %3 = llvm.mlir.constant(0 : index) : i64
          %4 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %5 = llvm.and %4, %0  : i64
          %6 = llvm.icmp "eq" %5, %3 : i64
          "llvm.intr.assume"(%6) : (i1) -> ()
          %7 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %8 = llvm.and %7, %0  : i64
          %9 = llvm.icmp "eq" %8, %3 : i64
          "llvm.intr.assume"(%9) : (i1) -> ()
          %10 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %11 = llvm.and %10, %0  : i64
          %12 = llvm.icmp "eq" %11, %3 : i64
          "llvm.intr.assume"(%12) : (i1) -> ()
          %13 = llvm.ptrtoint %arg3 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %3 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = nvvm.read.ptx.sreg.ctaid.x : i32
          %17 = llvm.sext %16 : i32 to i64
          %18 = nvvm.read.ptx.sreg.tid.x : i32
          %19 = llvm.sext %18 : i32 to i64
          %20 = llvm.mul %19, %2  : i64
          %21 = llvm.mul %17, %1  : i64
          %22 = llvm.add %20, %21  : i64
          %23 = llvm.getelementptr %arg0[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %24 = llvm.bitcast %23 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %25 = llvm.load %24 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %26 = llvm.getelementptr %arg1[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %27 = llvm.bitcast %26 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %28 = llvm.load %27 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %29 = llvm.getelementptr %arg2[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %30 = llvm.bitcast %29 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %32 = llvm.fadd %25, %28  : vector<4xf32>
          %33 = llvm.fadd %32, %31  : vector<4xf32>
          %34 = llvm.getelementptr %arg3[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %35 = llvm.bitcast %34 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %33, %35 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
        %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
        %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@twoadd_static_dispatch_0::@cuda_nvptx_fb::@twoadd_static_dispatch_0_generic_131072) workgroups([%c512, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(256 : index) : i64
          %2 = llvm.mlir.constant(4 : index) : i64
          %3 = llvm.mlir.constant(0 : index) : i64
          %4 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %5 = llvm.and %4, %0  : i64
          %6 = llvm.icmp "eq" %5, %3 : i64
          "llvm.intr.assume"(%6) : (i1) -> ()
          %7 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %8 = llvm.and %7, %0  : i64
          %9 = llvm.icmp "eq" %8, %3 : i64
          "llvm.intr.assume"(%9) : (i1) -> ()
          %10 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %11 = llvm.and %10, %0  : i64
          %12 = llvm.icmp "eq" %11, %3 : i64
          "llvm.intr.assume"(%12) : (i1) -> ()
          %13 = llvm.ptrtoint %arg3 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %3 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = nvvm.read.ptx.sreg.ctaid.x : i32
          %17 = llvm.sext %16 : i32 to i64
          %18 = nvvm.read.ptx.sreg.tid.x : i32
          %19 = llvm.sext %18 : i32 to i64
          %20 = llvm.mul %19, %2  : i64
          %21 = llvm.mul %17, %1  : i64
          %22 = llvm.add %20, %21  : i64
          %23 = llvm.getelementptr %arg0[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %24 = llvm.bitcast %23 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %25 = llvm.load %24 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %26 = llvm.getelementptr %arg1[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %27 = llvm.bitcast %26 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %28 = llvm.load %27 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %29 = llvm.getelementptr %arg2[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %30 = llvm.bitcast %29 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %32 = llvm.fadd %25, %28  : vector<4xf32>
          %33 = llvm.fadd %32, %31  : vector<4xf32>
          %34 = llvm.getelementptr %arg3[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %35 = llvm.bitcast %34 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %33, %35 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
        %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
        %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@twoadd_static_dispatch_0::@cuda_nvptx_fb::@twoadd_static_dispatch_0_generic_131072) workgroups([%c512, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(256 : index) : i64
          %2 = llvm.mlir.constant(4 : index) : i64
          %3 = llvm.mlir.constant(0 : index) : i64
          %4 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %5 = llvm.and %4, %0  : i64
          %6 = llvm.icmp "eq" %5, %3 : i64
          "llvm.intr.assume"(%6) : (i1) -> ()
          %7 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %8 = llvm.and %7, %0  : i64
          %9 = llvm.icmp "eq" %8, %3 : i64
          "llvm.intr.assume"(%9) : (i1) -> ()
          %10 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %11 = llvm.and %10, %0  : i64
          %12 = llvm.icmp "eq" %11, %3 : i64
          "llvm.intr.assume"(%12) : (i1) -> ()
          %13 = llvm.ptrtoint %arg3 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %3 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = nvvm.read.ptx.sreg.ctaid.x : i32
          %17 = llvm.sext %16 : i32 to i64
          %18 = nvvm.read.ptx.sreg.tid.x : i32
          %19 = llvm.sext %18 : i32 to i64
          %20 = llvm.mul %19, %2  : i64
          %21 = llvm.mul %17, %1  : i64
          %22 = llvm.add %20, %21  : i64
          %23 = llvm.getelementptr %arg0[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %24 = llvm.bitcast %23 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %25 = llvm.load %24 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %26 = llvm.getelementptr %arg1[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %27 = llvm.bitcast %26 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %28 = llvm.load %27 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %29 = llvm.getelementptr %arg2[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %30 = llvm.bitcast %29 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %32 = llvm.fadd %25, %28  : vector<4xf32>
          %33 = llvm.fadd %32, %31  : vector<4xf32>
          %34 = llvm.getelementptr %arg3[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %35 = llvm.bitcast %34 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %33, %35 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
        %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
        %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@twoadd_static_dispatch_0::@cuda_nvptx_fb::@twoadd_static_dispatch_0_generic_131072) workgroups([%c512, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::LinkTargetExecutablesPass (iree-hal-link-target-executables) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(256 : index) : i64
          %2 = llvm.mlir.constant(4 : index) : i64
          %3 = llvm.mlir.constant(0 : index) : i64
          %4 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %5 = llvm.and %4, %0  : i64
          %6 = llvm.icmp "eq" %5, %3 : i64
          "llvm.intr.assume"(%6) : (i1) -> ()
          %7 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %8 = llvm.and %7, %0  : i64
          %9 = llvm.icmp "eq" %8, %3 : i64
          "llvm.intr.assume"(%9) : (i1) -> ()
          %10 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %11 = llvm.and %10, %0  : i64
          %12 = llvm.icmp "eq" %11, %3 : i64
          "llvm.intr.assume"(%12) : (i1) -> ()
          %13 = llvm.ptrtoint %arg3 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %3 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = nvvm.read.ptx.sreg.ctaid.x : i32
          %17 = llvm.sext %16 : i32 to i64
          %18 = nvvm.read.ptx.sreg.tid.x : i32
          %19 = llvm.sext %18 : i32 to i64
          %20 = llvm.mul %19, %2  : i64
          %21 = llvm.mul %17, %1  : i64
          %22 = llvm.add %20, %21  : i64
          %23 = llvm.getelementptr %arg0[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %24 = llvm.bitcast %23 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %25 = llvm.load %24 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %26 = llvm.getelementptr %arg1[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %27 = llvm.bitcast %26 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %28 = llvm.load %27 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %29 = llvm.getelementptr %arg2[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %30 = llvm.bitcast %29 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %32 = llvm.fadd %25, %28  : vector<4xf32>
          %33 = llvm.fadd %32, %31  : vector<4xf32>
          %34 = llvm.getelementptr %arg3[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %35 = llvm.bitcast %34 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %33, %35 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
        %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
        %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@twoadd_static_dispatch_0::@cuda_nvptx_fb::@twoadd_static_dispatch_0_generic_131072) workgroups([%c512, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(256 : index) : i64
          %2 = llvm.mlir.constant(4 : index) : i64
          %3 = llvm.mlir.constant(0 : index) : i64
          %4 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %5 = llvm.and %4, %0  : i64
          %6 = llvm.icmp "eq" %5, %3 : i64
          "llvm.intr.assume"(%6) : (i1) -> ()
          %7 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %8 = llvm.and %7, %0  : i64
          %9 = llvm.icmp "eq" %8, %3 : i64
          "llvm.intr.assume"(%9) : (i1) -> ()
          %10 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %11 = llvm.and %10, %0  : i64
          %12 = llvm.icmp "eq" %11, %3 : i64
          "llvm.intr.assume"(%12) : (i1) -> ()
          %13 = llvm.ptrtoint %arg3 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %3 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = nvvm.read.ptx.sreg.ctaid.x : i32
          %17 = llvm.sext %16 : i32 to i64
          %18 = nvvm.read.ptx.sreg.tid.x : i32
          %19 = llvm.sext %18 : i32 to i64
          %20 = llvm.mul %19, %2  : i64
          %21 = llvm.mul %17, %1  : i64
          %22 = llvm.add %20, %21  : i64
          %23 = llvm.getelementptr %arg0[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %24 = llvm.bitcast %23 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %25 = llvm.load %24 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %26 = llvm.getelementptr %arg1[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %27 = llvm.bitcast %26 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %28 = llvm.load %27 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %29 = llvm.getelementptr %arg2[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %30 = llvm.bitcast %29 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %32 = llvm.fadd %25, %28  : vector<4xf32>
          %33 = llvm.fadd %32, %31  : vector<4xf32>
          %34 = llvm.getelementptr %arg3[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %35 = llvm.bitcast %34 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %33, %35 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
        %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
        %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@twoadd_static_dispatch_0::@cuda_nvptx_fb::@twoadd_static_dispatch_0_generic_131072) workgroups([%c512, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::LinkExecutablesPass (iree-hal-link-executables) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(256 : index) : i64
          %2 = llvm.mlir.constant(4 : index) : i64
          %3 = llvm.mlir.constant(0 : index) : i64
          %4 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %5 = llvm.and %4, %0  : i64
          %6 = llvm.icmp "eq" %5, %3 : i64
          "llvm.intr.assume"(%6) : (i1) -> ()
          %7 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %8 = llvm.and %7, %0  : i64
          %9 = llvm.icmp "eq" %8, %3 : i64
          "llvm.intr.assume"(%9) : (i1) -> ()
          %10 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %11 = llvm.and %10, %0  : i64
          %12 = llvm.icmp "eq" %11, %3 : i64
          "llvm.intr.assume"(%12) : (i1) -> ()
          %13 = llvm.ptrtoint %arg3 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %3 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = nvvm.read.ptx.sreg.ctaid.x : i32
          %17 = llvm.sext %16 : i32 to i64
          %18 = nvvm.read.ptx.sreg.tid.x : i32
          %19 = llvm.sext %18 : i32 to i64
          %20 = llvm.mul %19, %2  : i64
          %21 = llvm.mul %17, %1  : i64
          %22 = llvm.add %20, %21  : i64
          %23 = llvm.getelementptr %arg0[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %24 = llvm.bitcast %23 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %25 = llvm.load %24 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %26 = llvm.getelementptr %arg1[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %27 = llvm.bitcast %26 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %28 = llvm.load %27 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %29 = llvm.getelementptr %arg2[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %30 = llvm.bitcast %29 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %32 = llvm.fadd %25, %28  : vector<4xf32>
          %33 = llvm.fadd %32, %31  : vector<4xf32>
          %34 = llvm.getelementptr %arg3[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %35 = llvm.bitcast %34 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %33, %35 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
        %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
        %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@twoadd_static_dispatch_0::@cuda_nvptx_fb::@twoadd_static_dispatch_0_generic_131072) workgroups([%c512, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ResolveExportOrdinalsPass (iree-hal-resolve-export-ordinals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(256 : index) : i64
          %2 = llvm.mlir.constant(4 : index) : i64
          %3 = llvm.mlir.constant(0 : index) : i64
          %4 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %5 = llvm.and %4, %0  : i64
          %6 = llvm.icmp "eq" %5, %3 : i64
          "llvm.intr.assume"(%6) : (i1) -> ()
          %7 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %8 = llvm.and %7, %0  : i64
          %9 = llvm.icmp "eq" %8, %3 : i64
          "llvm.intr.assume"(%9) : (i1) -> ()
          %10 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %11 = llvm.and %10, %0  : i64
          %12 = llvm.icmp "eq" %11, %3 : i64
          "llvm.intr.assume"(%12) : (i1) -> ()
          %13 = llvm.ptrtoint %arg3 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %3 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = nvvm.read.ptx.sreg.ctaid.x : i32
          %17 = llvm.sext %16 : i32 to i64
          %18 = nvvm.read.ptx.sreg.tid.x : i32
          %19 = llvm.sext %18 : i32 to i64
          %20 = llvm.mul %19, %2  : i64
          %21 = llvm.mul %17, %1  : i64
          %22 = llvm.add %20, %21  : i64
          %23 = llvm.getelementptr %arg0[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %24 = llvm.bitcast %23 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %25 = llvm.load %24 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %26 = llvm.getelementptr %arg1[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %27 = llvm.bitcast %26 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %28 = llvm.load %27 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %29 = llvm.getelementptr %arg2[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %30 = llvm.bitcast %29 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %32 = llvm.fadd %25, %28  : vector<4xf32>
          %33 = llvm.fadd %32, %31  : vector<4xf32>
          %34 = llvm.getelementptr %arg3[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %35 = llvm.bitcast %34 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %33, %35 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
        %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
        %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
      ])
      %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
      %exe = hal.executable.lookup device(%1 : !hal.device) executable(@twoadd_static_dispatch_0) : !hal.executable
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[0] workgroups([%c512, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::MaterializeResourceCachesPass (iree-hal-materialize-resource-caches) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %0 = hal.device.switch<%device : !hal.device> -> !hal.executable
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
      hal.return %exe : !hal.executable
    },
    #hal.match.always {
      %1 = util.null : !hal.executable
      hal.return %1 : !hal.executable
    }
    util.global.store %0, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(256 : index) : i64
          %2 = llvm.mlir.constant(4 : index) : i64
          %3 = llvm.mlir.constant(0 : index) : i64
          %4 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %5 = llvm.and %4, %0  : i64
          %6 = llvm.icmp "eq" %5, %3 : i64
          "llvm.intr.assume"(%6) : (i1) -> ()
          %7 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %8 = llvm.and %7, %0  : i64
          %9 = llvm.icmp "eq" %8, %3 : i64
          "llvm.intr.assume"(%9) : (i1) -> ()
          %10 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %11 = llvm.and %10, %0  : i64
          %12 = llvm.icmp "eq" %11, %3 : i64
          "llvm.intr.assume"(%12) : (i1) -> ()
          %13 = llvm.ptrtoint %arg3 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %3 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = nvvm.read.ptx.sreg.ctaid.x : i32
          %17 = llvm.sext %16 : i32 to i64
          %18 = nvvm.read.ptx.sreg.tid.x : i32
          %19 = llvm.sext %18 : i32 to i64
          %20 = llvm.mul %19, %2  : i64
          %21 = llvm.mul %17, %1  : i64
          %22 = llvm.add %20, %21  : i64
          %23 = llvm.getelementptr %arg0[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %24 = llvm.bitcast %23 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %25 = llvm.load %24 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %26 = llvm.getelementptr %arg1[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %27 = llvm.bitcast %26 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %28 = llvm.load %27 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %29 = llvm.getelementptr %arg2[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %30 = llvm.bitcast %29 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %32 = llvm.fadd %25, %28  : vector<4xf32>
          %33 = llvm.fadd %32, %31  : vector<4xf32>
          %34 = llvm.getelementptr %arg3[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %35 = llvm.bitcast %34 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %33, %35 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"cuda-nvptx-fb"> {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
        %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
        %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
      ])
      %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
      %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::InlineDeviceSwitchesPass (iree-hal-inline-device-switches) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::InlineDeviceSwitchesPass (iree-hal-inline-device-switches) //----- //
util.initializer {
  %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  %device = hal.ex.shared_device : !hal.device
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::InlineDeviceSwitchesPass (iree-hal-inline-device-switches) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb5(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %true = arith.constant true
  cf.cond_br %true, ^bb3, ^bb4
^bb3:  // pred: ^bb2
  %0 = util.null : !hal.executable
  cf.br ^bb5(%0 : !hal.executable)
^bb4:  // pred: ^bb2
  util.unreachable "device not supported in the compiled configuration"
^bb5(%1: !hal.executable):  // 2 preds: ^bb1, ^bb3
  util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::InlineDeviceSwitchesPass (iree-hal-inline-device-switches) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
    %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
  ])
  %0 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
  %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
  cf.br ^bb3
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
^bb3:  // pred: ^bb1
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %1 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%1) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::MemoizeDeviceQueriesPass (iree-hal-memoize-device-queries) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %ok, @_device_query_0_ok : i1
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %_device_query_0_ok = util.global.load @_device_query_0_ok : i1
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb5(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %true = arith.constant true
    cf.cond_br %true, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %0 = util.null : !hal.executable
    cf.br ^bb5(%0 : !hal.executable)
  ^bb4:  // pred: ^bb2
    util.unreachable "device not supported in the compiled configuration"
  ^bb5(%1: !hal.executable):  // 2 preds: ^bb1, ^bb3
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(256 : index) : i64
          %2 = llvm.mlir.constant(4 : index) : i64
          %3 = llvm.mlir.constant(0 : index) : i64
          %4 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %5 = llvm.and %4, %0  : i64
          %6 = llvm.icmp "eq" %5, %3 : i64
          "llvm.intr.assume"(%6) : (i1) -> ()
          %7 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %8 = llvm.and %7, %0  : i64
          %9 = llvm.icmp "eq" %8, %3 : i64
          "llvm.intr.assume"(%9) : (i1) -> ()
          %10 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %11 = llvm.and %10, %0  : i64
          %12 = llvm.icmp "eq" %11, %3 : i64
          "llvm.intr.assume"(%12) : (i1) -> ()
          %13 = llvm.ptrtoint %arg3 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %3 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = nvvm.read.ptx.sreg.ctaid.x : i32
          %17 = llvm.sext %16 : i32 to i64
          %18 = nvvm.read.ptx.sreg.tid.x : i32
          %19 = llvm.sext %18 : i32 to i64
          %20 = llvm.mul %19, %2  : i64
          %21 = llvm.mul %17, %1  : i64
          %22 = llvm.add %20, %21  : i64
          %23 = llvm.getelementptr %arg0[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %24 = llvm.bitcast %23 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %25 = llvm.load %24 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %26 = llvm.getelementptr %arg1[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %27 = llvm.bitcast %26 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %28 = llvm.load %27 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %29 = llvm.getelementptr %arg2[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %30 = llvm.bitcast %29 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %32 = llvm.fadd %25, %28  : vector<4xf32>
          %33 = llvm.fadd %32, %31  : vector<4xf32>
          %34 = llvm.getelementptr %arg3[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %35 = llvm.bitcast %34 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %33, %35 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %_device_query_0_ok = util.global.load @_device_query_0_ok : i1
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    %0 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    cf.br ^bb3
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  ^bb3:  // pred: ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %1 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%1) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %ok, @_device_query_0_ok : i1
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(256 : index) : i64
          %2 = llvm.mlir.constant(4 : index) : i64
          %3 = llvm.mlir.constant(0 : index) : i64
          %4 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %5 = llvm.and %4, %0  : i64
          %6 = llvm.icmp "eq" %5, %3 : i64
          "llvm.intr.assume"(%6) : (i1) -> ()
          %7 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %8 = llvm.and %7, %0  : i64
          %9 = llvm.icmp "eq" %8, %3 : i64
          "llvm.intr.assume"(%9) : (i1) -> ()
          %10 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %11 = llvm.and %10, %0  : i64
          %12 = llvm.icmp "eq" %11, %3 : i64
          "llvm.intr.assume"(%12) : (i1) -> ()
          %13 = llvm.ptrtoint %arg3 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %3 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = nvvm.read.ptx.sreg.ctaid.x : i32
          %17 = llvm.sext %16 : i32 to i64
          %18 = nvvm.read.ptx.sreg.tid.x : i32
          %19 = llvm.sext %18 : i32 to i64
          %20 = llvm.mul %19, %2  : i64
          %21 = llvm.mul %17, %1  : i64
          %22 = llvm.add %20, %21  : i64
          %23 = llvm.getelementptr %arg0[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %24 = llvm.bitcast %23 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %25 = llvm.load %24 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %26 = llvm.getelementptr %arg1[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %27 = llvm.bitcast %26 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %28 = llvm.load %27 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %29 = llvm.getelementptr %arg2[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %30 = llvm.bitcast %29 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %32 = llvm.fadd %25, %28  : vector<4xf32>
          %33 = llvm.fadd %32, %31  : vector<4xf32>
          %34 = llvm.getelementptr %arg3[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %35 = llvm.bitcast %34 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %33, %35 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %ok, @_device_query_0_ok : i1
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(256 : index) : i64
          %2 = llvm.mlir.constant(4 : index) : i64
          %3 = llvm.mlir.constant(0 : index) : i64
          %4 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %5 = llvm.and %4, %0  : i64
          %6 = llvm.icmp "eq" %5, %3 : i64
          "llvm.intr.assume"(%6) : (i1) -> ()
          %7 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %8 = llvm.and %7, %0  : i64
          %9 = llvm.icmp "eq" %8, %3 : i64
          "llvm.intr.assume"(%9) : (i1) -> ()
          %10 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %11 = llvm.and %10, %0  : i64
          %12 = llvm.icmp "eq" %11, %3 : i64
          "llvm.intr.assume"(%12) : (i1) -> ()
          %13 = llvm.ptrtoint %arg3 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %3 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = nvvm.read.ptx.sreg.ctaid.x : i32
          %17 = llvm.sext %16 : i32 to i64
          %18 = nvvm.read.ptx.sreg.tid.x : i32
          %19 = llvm.sext %18 : i32 to i64
          %20 = llvm.mul %19, %2  : i64
          %21 = llvm.mul %17, %1  : i64
          %22 = llvm.add %20, %21  : i64
          %23 = llvm.getelementptr %arg0[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %24 = llvm.bitcast %23 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %25 = llvm.load %24 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %26 = llvm.getelementptr %arg1[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %27 = llvm.bitcast %26 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %28 = llvm.load %27 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %29 = llvm.getelementptr %arg2[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %30 = llvm.bitcast %29 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %32 = llvm.fadd %25, %28  : vector<4xf32>
          %33 = llvm.fadd %32, %31  : vector<4xf32>
          %34 = llvm.getelementptr %arg3[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %35 = llvm.bitcast %34 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %33, %35 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @_device_query_0 : i1
  util.global.store %ok, @_device_query_0_ok : i1
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  %device = hal.ex.shared_device : !hal.device
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %device = hal.ex.shared_device : !hal.device
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
  %c-1_i32 = arith.constant -1 : i32
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
    %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.global.store %ok, @_device_query_0_ok : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(256 : index) : i64
          %2 = llvm.mlir.constant(4 : index) : i64
          %3 = llvm.mlir.constant(0 : index) : i64
          %4 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %5 = llvm.and %4, %0  : i64
          %6 = llvm.icmp "eq" %5, %3 : i64
          "llvm.intr.assume"(%6) : (i1) -> ()
          %7 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %8 = llvm.and %7, %0  : i64
          %9 = llvm.icmp "eq" %8, %3 : i64
          "llvm.intr.assume"(%9) : (i1) -> ()
          %10 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %11 = llvm.and %10, %0  : i64
          %12 = llvm.icmp "eq" %11, %3 : i64
          "llvm.intr.assume"(%12) : (i1) -> ()
          %13 = llvm.ptrtoint %arg3 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %3 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = nvvm.read.ptx.sreg.ctaid.x : i32
          %17 = llvm.sext %16 : i32 to i64
          %18 = nvvm.read.ptx.sreg.tid.x : i32
          %19 = llvm.sext %18 : i32 to i64
          %20 = llvm.mul %19, %2  : i64
          %21 = llvm.mul %17, %1  : i64
          %22 = llvm.add %20, %21  : i64
          %23 = llvm.getelementptr %arg0[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %24 = llvm.bitcast %23 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %25 = llvm.load %24 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %26 = llvm.getelementptr %arg1[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %27 = llvm.bitcast %26 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %28 = llvm.load %27 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %29 = llvm.getelementptr %arg2[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %30 = llvm.bitcast %29 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %32 = llvm.fadd %25, %28  : vector<4xf32>
          %33 = llvm.fadd %32, %31  : vector<4xf32>
          %34 = llvm.getelementptr %arg3[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %35 = llvm.bitcast %34 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %33, %35 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c512 = arith.constant 512 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(256 : index) : i64
          %2 = llvm.mlir.constant(4 : index) : i64
          %3 = llvm.mlir.constant(0 : index) : i64
          %4 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %5 = llvm.and %4, %0  : i64
          %6 = llvm.icmp "eq" %5, %3 : i64
          "llvm.intr.assume"(%6) : (i1) -> ()
          %7 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %8 = llvm.and %7, %0  : i64
          %9 = llvm.icmp "eq" %8, %3 : i64
          "llvm.intr.assume"(%9) : (i1) -> ()
          %10 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %11 = llvm.and %10, %0  : i64
          %12 = llvm.icmp "eq" %11, %3 : i64
          "llvm.intr.assume"(%12) : (i1) -> ()
          %13 = llvm.ptrtoint %arg3 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %3 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = nvvm.read.ptx.sreg.ctaid.x : i32
          %17 = llvm.sext %16 : i32 to i64
          %18 = nvvm.read.ptx.sreg.tid.x : i32
          %19 = llvm.sext %18 : i32 to i64
          %20 = llvm.mul %19, %2  : i64
          %21 = llvm.mul %17, %1  : i64
          %22 = llvm.add %20, %21  : i64
          %23 = llvm.getelementptr %arg0[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %24 = llvm.bitcast %23 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %25 = llvm.load %24 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %26 = llvm.getelementptr %arg1[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %27 = llvm.bitcast %26 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %28 = llvm.load %27 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %29 = llvm.getelementptr %arg2[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %30 = llvm.bitcast %29 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %32 = llvm.fadd %25, %28  : vector<4xf32>
          %33 = llvm.fadd %32, %31  : vector<4xf32>
          %34 = llvm.getelementptr %arg3[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %35 = llvm.bitcast %34 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %33, %35 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c512 = arith.constant 512 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(256 : index) : i64
          %2 = llvm.mlir.constant(4 : index) : i64
          %3 = llvm.mlir.constant(0 : index) : i64
          %4 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %5 = llvm.and %4, %0  : i64
          %6 = llvm.icmp "eq" %5, %3 : i64
          "llvm.intr.assume"(%6) : (i1) -> ()
          %7 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %8 = llvm.and %7, %0  : i64
          %9 = llvm.icmp "eq" %8, %3 : i64
          "llvm.intr.assume"(%9) : (i1) -> ()
          %10 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %11 = llvm.and %10, %0  : i64
          %12 = llvm.icmp "eq" %11, %3 : i64
          "llvm.intr.assume"(%12) : (i1) -> ()
          %13 = llvm.ptrtoint %arg3 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %3 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = nvvm.read.ptx.sreg.ctaid.x : i32
          %17 = llvm.sext %16 : i32 to i64
          %18 = nvvm.read.ptx.sreg.tid.x : i32
          %19 = llvm.sext %18 : i32 to i64
          %20 = llvm.mul %19, %2  : i64
          %21 = llvm.mul %17, %1  : i64
          %22 = llvm.add %20, %21  : i64
          %23 = llvm.getelementptr %arg0[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %24 = llvm.bitcast %23 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %25 = llvm.load %24 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %26 = llvm.getelementptr %arg1[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %27 = llvm.bitcast %26 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %28 = llvm.load %27 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %29 = llvm.getelementptr %arg2[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %30 = llvm.bitcast %29 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %32 = llvm.fadd %25, %28  : vector<4xf32>
          %33 = llvm.fadd %32, %31  : vector<4xf32>
          %34 = llvm.getelementptr %arg3[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %35 = llvm.bitcast %34 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %33, %35 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c512 = arith.constant 512 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @_device_query_0 : i1
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  %device = hal.ex.shared_device : !hal.device
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %device = hal.ex.shared_device : !hal.device
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c256 = arith.constant 256 : index
  %c512 = arith.constant 512 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c-1_i64 = arith.constant -1 : i64
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c-1_i32 = arith.constant -1 : i32
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
    %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(256 : index) : i64
          %2 = llvm.mlir.constant(4 : index) : i64
          %3 = llvm.mlir.constant(0 : index) : i64
          %4 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %5 = llvm.and %4, %0  : i64
          %6 = llvm.icmp "eq" %5, %3 : i64
          "llvm.intr.assume"(%6) : (i1) -> ()
          %7 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %8 = llvm.and %7, %0  : i64
          %9 = llvm.icmp "eq" %8, %3 : i64
          "llvm.intr.assume"(%9) : (i1) -> ()
          %10 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %11 = llvm.and %10, %0  : i64
          %12 = llvm.icmp "eq" %11, %3 : i64
          "llvm.intr.assume"(%12) : (i1) -> ()
          %13 = llvm.ptrtoint %arg3 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %3 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = nvvm.read.ptx.sreg.ctaid.x : i32
          %17 = llvm.sext %16 : i32 to i64
          %18 = nvvm.read.ptx.sreg.tid.x : i32
          %19 = llvm.sext %18 : i32 to i64
          %20 = llvm.mul %19, %2  : i64
          %21 = llvm.mul %17, %1  : i64
          %22 = llvm.add %20, %21  : i64
          %23 = llvm.getelementptr %arg0[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %24 = llvm.bitcast %23 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %25 = llvm.load %24 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %26 = llvm.getelementptr %arg1[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %27 = llvm.bitcast %26 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %28 = llvm.load %27 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %29 = llvm.getelementptr %arg2[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %30 = llvm.bitcast %29 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %32 = llvm.fadd %25, %28  : vector<4xf32>
          %33 = llvm.fadd %32, %31  : vector<4xf32>
          %34 = llvm.getelementptr %arg3[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %35 = llvm.bitcast %34 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %33, %35 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c512 = arith.constant 512 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  util.global.store %value, @_device_query_0 : i1
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  %device = hal.ex.shared_device : !hal.device
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %device = hal.ex.shared_device : !hal.device
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c256 = arith.constant 256 : index
  %c512 = arith.constant 512 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c-1_i64 = arith.constant -1 : i64
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c-1_i32 = arith.constant -1 : i32
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
    %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After CombineInitializers (iree-util-combine-initializers) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer, ReadOnly>, <3, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<LLVMGPUVectorize>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %device_0 = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_1 = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_1 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device_2 = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_2 : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    cf.br ^bb4
  ^bb4:  // pred: ^bb3
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.variant public @cuda_nvptx_fb, target = #executable_target_cuda_nvptx_fb {
      hal.executable.export public @twoadd_static_dispatch_0_generic_131072 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation, workgroup_size = [64 : index, 1 : index, 1 : index]} {
      ^bb0(%arg0: !hal.device, %arg1: index):
        %c512 = arith.constant 512 : index
        %c1 = arith.constant 1 : index
        hal.return %c512, %c1, %c1 : index, index, index
      }
      builtin.module {
        llvm.func @twoadd_static_dispatch_0_generic_131072(%arg0: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg1: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg2: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias, llvm.readonly}, %arg3: !llvm.ptr<f32> {llvm.align = 16 : i32, llvm.noalias}) {
          %0 = llvm.mlir.constant(63 : index) : i64
          %1 = llvm.mlir.constant(256 : index) : i64
          %2 = llvm.mlir.constant(4 : index) : i64
          %3 = llvm.mlir.constant(0 : index) : i64
          %4 = llvm.ptrtoint %arg0 : !llvm.ptr<f32> to i64
          %5 = llvm.and %4, %0  : i64
          %6 = llvm.icmp "eq" %5, %3 : i64
          "llvm.intr.assume"(%6) : (i1) -> ()
          %7 = llvm.ptrtoint %arg1 : !llvm.ptr<f32> to i64
          %8 = llvm.and %7, %0  : i64
          %9 = llvm.icmp "eq" %8, %3 : i64
          "llvm.intr.assume"(%9) : (i1) -> ()
          %10 = llvm.ptrtoint %arg2 : !llvm.ptr<f32> to i64
          %11 = llvm.and %10, %0  : i64
          %12 = llvm.icmp "eq" %11, %3 : i64
          "llvm.intr.assume"(%12) : (i1) -> ()
          %13 = llvm.ptrtoint %arg3 : !llvm.ptr<f32> to i64
          %14 = llvm.and %13, %0  : i64
          %15 = llvm.icmp "eq" %14, %3 : i64
          "llvm.intr.assume"(%15) : (i1) -> ()
          %16 = nvvm.read.ptx.sreg.ctaid.x : i32
          %17 = llvm.sext %16 : i32 to i64
          %18 = nvvm.read.ptx.sreg.tid.x : i32
          %19 = llvm.sext %18 : i32 to i64
          %20 = llvm.mul %19, %2  : i64
          %21 = llvm.mul %17, %1  : i64
          %22 = llvm.add %20, %21  : i64
          %23 = llvm.getelementptr %arg0[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %24 = llvm.bitcast %23 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %25 = llvm.load %24 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %26 = llvm.getelementptr %arg1[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %27 = llvm.bitcast %26 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %28 = llvm.load %27 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %29 = llvm.getelementptr %arg2[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %30 = llvm.bitcast %29 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          %32 = llvm.fadd %25, %28  : vector<4xf32>
          %33 = llvm.fadd %32, %31  : vector<4xf32>
          %34 = llvm.getelementptr %arg3[%22] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
          %35 = llvm.bitcast %34 : !llvm.ptr<f32> to !llvm.ptr<vector<4xf32>>
          llvm.store %33, %35 {alignment = 4 : i64} : !llvm.ptr<vector<4xf32>>
          llvm.return
        }
      }
    }
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c512 = arith.constant 512 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::SerializeTargetExecutablesPass (iree-hal-serialize-target-executables) //----- //
hal.executable private @twoadd_static_dispatch_0 {
  hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<1836xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::SerializeExecutablesPass (iree-hal-serialize-executables) //----- //
hal.executable private @twoadd_static_dispatch_0 {
  hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<1836xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %device_0 = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_1 = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_1 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device_2 = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_2 : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    cf.br ^bb4
  ^bb4:  // pred: ^bb3
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<1836xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c512 = arith.constant 512 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %device_0 = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_1 = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_1 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device_2 = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_2 : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<1836xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c512 = arith.constant 512 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<1836xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c512 = arith.constant 512 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
  %c256 = arith.constant 256 : index
  %c512 = arith.constant 512 : index
  %c1_i32 = arith.constant 1 : i32
  %c553648160_i32 = arith.constant 553648160 : i32
  %c524288 = arith.constant 524288 : index
  %c0 = arith.constant 0 : index
  %c-1_i64 = arith.constant -1 : i64
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c3 = arith.constant 3 : index
  %c-1_i32 = arith.constant -1 : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
    %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<1836xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<1836xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<1836xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<1836xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<1836xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
    %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After Inliner (inline) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<1836xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"0x0800000043554441E8F8FFFF10000000480000003C0000005000000001000000040000002700000074776F6164645F7374617469635F64697370617463685F305F67656E657269635F31333130373200010000000000000001000000400000000100000001000000B40600002F2F0A2F2F2047656E657261746564206279204C4C564D204E56505458204261636B2D456E640A2F2F0A0A2E76657273696F6E20362E300A2E74617267657420736D5F37300A2E616464726573735F73697A652036340A0A092F2F202E676C6F626C0974776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037320A0A2E76697369626C65202E656E7472792074776F6164645F7374617469635F64697370617463685F305F67656E657269635F313331303732280A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F302C0A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F312C0A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F322C0A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F330A290A2E6D61786E7469642036342C20312C20310A7B0A092E726567202E623332200925723C343E3B0A092E726567202E663332200925663C32313E3B0A092E726567202E62363420092572643C31373E3B0A0A096C642E706172616D2E7536342009257264312C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F305D3B0A096C642E706172616D2E7536342009257264322C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F335D3B0A09637674612E746F2E676C6F62616C2E7536342009257264332C20257264323B0A096C642E706172616D2E7536342009257264342C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F315D3B0A096C642E706172616D2E7536342009257264352C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F325D3B0A09637674612E746F2E676C6F62616C2E7536342009257264362C20257264353B0A09637674612E746F2E676C6F62616C2E7536342009257264372C20257264343B0A09637674612E746F2E676C6F62616C2E7536342009257264382C20257264313B0A096D6F762E75333220092572312C202563746169642E783B0A096D6F762E75333220092572322C20257469642E783B0A0973686C2E62333220092572332C202572322C20323B0A096376742E7536342E7533322009257264392C202572333B0A096D756C2E776964652E753332200925726431302C202572312C203235363B0A096164642E733634200925726431312C2025726431302C20257264393B0A0973686C2E623634200925726431322C2025726431312C20323B0A096164642E733634200925726431332C20257264382C2025726431323B0A096C642E676C6F62616C2E6E632E76342E66333220097B2566312C202566322C202566332C202566347D2C205B25726431335D3B0A096164642E733634200925726431342C20257264372C2025726431323B0A096C642E676C6F62616C2E6E632E76342E66333220097B2566352C202566362C202566372C202566387D2C205B25726431345D3B0A096164642E733634200925726431352C20257264362C2025726431323B0A096C642E676C6F62616C2E6E632E76342E66333220097B2566392C20256631302C20256631312C20256631327D2C205B25726431355D3B0A096164642E726E2E6633322009256631332C202566312C202566353B0A096164642E726E2E6633322009256631342C202566322C202566363B0A096164642E726E2E6633322009256631352C202566332C202566373B0A096164642E726E2E6633322009256631362C202566342C202566383B0A096164642E726E2E6633322009256631372C20256631362C20256631323B0A096164642E726E2E6633322009256631382C20256631352C20256631313B0A096164642E726E2E6633322009256631392C20256631342C20256631303B0A096164642E726E2E6633322009256632302C20256631332C202566393B0A096164642E733634200925726431362C20257264332C2025726431323B0A0973742E676C6F62616C2E76342E66333220095B25726431365D2C207B256632302C20256631392C20256631382C20256631377D3B0A097265743B0A0A7D0A0C001400040008000C001000"> : vector<1836xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After SCFForLoopCanonicalization (scf-for-loop-canonicalization) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SCFForLoopCanonicalization (scf-for-loop-canonicalization) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
    %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After LoopCoalescing (affine-loop-coalescing) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
    %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
    %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
    %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
    %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ArithUnsignedWhenEquivalent (arith-unsigned-when-equivalent) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After ArithUnsignedWhenEquivalent (arith-unsigned-when-equivalent) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
    %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After PropagateSubranges (iree-util-propagate-subranges) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"0x0800000043554441E8F8FFFF10000000480000003C0000005000000001000000040000002700000074776F6164645F7374617469635F64697370617463685F305F67656E657269635F31333130373200010000000000000001000000400000000100000001000000B40600002F2F0A2F2F2047656E657261746564206279204C4C564D204E56505458204261636B2D456E640A2F2F0A0A2E76657273696F6E20362E300A2E74617267657420736D5F37300A2E616464726573735F73697A652036340A0A092F2F202E676C6F626C0974776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037320A0A2E76697369626C65202E656E7472792074776F6164645F7374617469635F64697370617463685F305F67656E657269635F313331303732280A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F302C0A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F312C0A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F322C0A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F330A290A2E6D61786E7469642036342C20312C20310A7B0A092E726567202E623332200925723C343E3B0A092E726567202E663332200925663C32313E3B0A092E726567202E62363420092572643C31373E3B0A0A096C642E706172616D2E7536342009257264312C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F305D3B0A096C642E706172616D2E7536342009257264322C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F335D3B0A09637674612E746F2E676C6F62616C2E7536342009257264332C20257264323B0A096C642E706172616D2E7536342009257264342C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F315D3B0A096C642E706172616D2E7536342009257264352C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F325D3B0A09637674612E746F2E676C6F62616C2E7536342009257264362C20257264353B0A09637674612E746F2E676C6F62616C2E7536342009257264372C20257264343B0A09637674612E746F2E676C6F62616C2E7536342009257264382C20257264313B0A096D6F762E75333220092572312C202563746169642E783B0A096D6F762E75333220092572322C20257469642E783B0A0973686C2E62333220092572332C202572322C20323B0A096376742E7536342E7533322009257264392C202572333B0A096D756C2E776964652E753332200925726431302C202572312C203235363B0A096164642E733634200925726431312C2025726431302C20257264393B0A0973686C2E623634200925726431322C2025726431312C20323B0A096164642E733634200925726431332C20257264382C2025726431323B0A096C642E676C6F62616C2E6E632E76342E66333220097B2566312C202566322C202566332C202566347D2C205B25726431335D3B0A096164642E733634200925726431342C20257264372C2025726431323B0A096C642E676C6F62616C2E6E632E76342E66333220097B2566352C202566362C202566372C202566387D2C205B25726431345D3B0A096164642E733634200925726431352C20257264362C2025726431323B0A096C642E676C6F62616C2E6E632E76342E66333220097B2566392C20256631302C20256631312C20256631327D2C205B25726431355D3B0A096164642E726E2E6633322009256631332C202566312C202566353B0A096164642E726E2E6633322009256631342C202566322C202566363B0A096164642E726E2E6633322009256631352C202566332C202566373B0A096164642E726E2E6633322009256631362C202566342C202566383B0A096164642E726E2E6633322009256631372C20256631362C20256631323B0A096164642E726E2E6633322009256631382C20256631352C20256631313B0A096164642E726E2E6633322009256631392C20256631342C20256631303B0A096164642E726E2E6633322009256632302C20256631332C202566393B0A096164642E733634200925726431362C20257264332C2025726431323B0A0973742E676C6F62616C2E76342E66333220095B25726431365D2C207B256632302C20256631392C20256631382C20256631377D3B0A097265743B0A0A7D0A0C001400040008000C001000"> : vector<1836xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<1836xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<1836xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c3 = arith.constant 3 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0 = arith.constant 0 : index
    %c524288 = arith.constant 524288 : index
    %c553648160_i32 = arith.constant 553648160 : i32
    %c1_i32 = arith.constant 1 : i32
    %c512 = arith.constant 512 : index
    %c256 = arith.constant 256 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
  %c-1_i32 = arith.constant -1 : i32
  %c3 = arith.constant 3 : index
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0 = arith.constant 0 : index
  %c524288 = arith.constant 524288 : index
  %c553648160_i32 = arith.constant 553648160 : i32
  %c1_i32 = arith.constant 1 : i32
  %c512 = arith.constant 512 : index
  %c256 = arith.constant 256 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
  %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
  hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
    %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
    %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
  %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"0x0800000043554441E8F8FFFF10000000480000003C0000005000000001000000040000002700000074776F6164645F7374617469635F64697370617463685F305F67656E657269635F31333130373200010000000000000001000000400000000100000001000000B40600002F2F0A2F2F2047656E657261746564206279204C4C564D204E56505458204261636B2D456E640A2F2F0A0A2E76657273696F6E20362E300A2E74617267657420736D5F37300A2E616464726573735F73697A652036340A0A092F2F202E676C6F626C0974776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037320A0A2E76697369626C65202E656E7472792074776F6164645F7374617469635F64697370617463685F305F67656E657269635F313331303732280A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F302C0A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F312C0A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F322C0A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F330A290A2E6D61786E7469642036342C20312C20310A7B0A092E726567202E623332200925723C343E3B0A092E726567202E663332200925663C32313E3B0A092E726567202E62363420092572643C31373E3B0A0A096C642E706172616D2E7536342009257264312C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F305D3B0A096C642E706172616D2E7536342009257264322C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F335D3B0A09637674612E746F2E676C6F62616C2E7536342009257264332C20257264323B0A096C642E706172616D2E7536342009257264342C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F315D3B0A096C642E706172616D2E7536342009257264352C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F325D3B0A09637674612E746F2E676C6F62616C2E7536342009257264362C20257264353B0A09637674612E746F2E676C6F62616C2E7536342009257264372C20257264343B0A09637674612E746F2E676C6F62616C2E7536342009257264382C20257264313B0A096D6F762E75333220092572312C202563746169642E783B0A096D6F762E75333220092572322C20257469642E783B0A0973686C2E62333220092572332C202572322C20323B0A096376742E7536342E7533322009257264392C202572333B0A096D756C2E776964652E753332200925726431302C202572312C203235363B0A096164642E733634200925726431312C2025726431302C20257264393B0A0973686C2E623634200925726431322C2025726431312C20323B0A096164642E733634200925726431332C20257264382C2025726431323B0A096C642E676C6F62616C2E6E632E76342E66333220097B2566312C202566322C202566332C202566347D2C205B25726431335D3B0A096164642E733634200925726431342C20257264372C2025726431323B0A096C642E676C6F62616C2E6E632E76342E66333220097B2566352C202566362C202566372C202566387D2C205B25726431345D3B0A096164642E733634200925726431352C20257264362C2025726431323B0A096C642E676C6F62616C2E6E632E76342E66333220097B2566392C20256631302C20256631312C20256631327D2C205B25726431355D3B0A096164642E726E2E6633322009256631332C202566312C202566353B0A096164642E726E2E6633322009256631342C202566322C202566363B0A096164642E726E2E6633322009256631352C202566332C202566373B0A096164642E726E2E6633322009256631362C202566342C202566383B0A096164642E726E2E6633322009256631372C20256631362C20256631323B0A096164642E726E2E6633322009256631382C20256631352C20256631313B0A096164642E726E2E6633322009256631392C20256631342C20256631303B0A096164642E726E2E6633322009256632302C20256631332C202566393B0A096164642E733634200925726431362C20257264332C2025726431323B0A0973742E676C6F62616C2E76342E66333220095B25726431365D2C207B256632302C20256631392C20256631382C20256631377D3B0A097265743B0A0A7D0A0C001400040008000C001000"> : vector<1836xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c512 = arith.constant 512 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"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vector<1836xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c512 = arith.constant 512 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_twoadd_static_dispatch_0 : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "cuda-nvptx-fb") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<2, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<3, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@twoadd_static_dispatch_0::@cuda_nvptx_fb) layouts([%_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_twoadd_static_dispatch_0 : !hal.executable
    util.initializer.return
  }
  hal.executable private @twoadd_static_dispatch_0 {
    hal.executable.binary public @cuda_nvptx_fb attributes {data = dense<"0x0800000043554441E8F8FFFF10000000480000003C0000005000000001000000040000002700000074776F6164645F7374617469635F64697370617463685F305F67656E657269635F31333130373200010000000000000001000000400000000100000001000000B40600002F2F0A2F2F2047656E657261746564206279204C4C564D204E56505458204261636B2D456E640A2F2F0A0A2E76657273696F6E20362E300A2E74617267657420736D5F37300A2E616464726573735F73697A652036340A0A092F2F202E676C6F626C0974776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037320A0A2E76697369626C65202E656E7472792074776F6164645F7374617469635F64697370617463685F305F67656E657269635F313331303732280A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F302C0A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F312C0A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F322C0A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F330A290A2E6D61786E7469642036342C20312C20310A7B0A092E726567202E623332200925723C343E3B0A092E726567202E663332200925663C32313E3B0A092E726567202E62363420092572643C31373E3B0A0A096C642E706172616D2E7536342009257264312C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F305D3B0A096C642E706172616D2E7536342009257264322C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F335D3B0A09637674612E746F2E676C6F62616C2E7536342009257264332C20257264323B0A096C642E706172616D2E7536342009257264342C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F315D3B0A096C642E706172616D2E7536342009257264352C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F325D3B0A09637674612E746F2E676C6F62616C2E7536342009257264362C20257264353B0A09637674612E746F2E676C6F62616C2E7536342009257264372C20257264343B0A09637674612E746F2E676C6F62616C2E7536342009257264382C20257264313B0A096D6F762E75333220092572312C202563746169642E783B0A096D6F762E75333220092572322C20257469642E783B0A0973686C2E62333220092572332C202572322C20323B0A096376742E7536342E7533322009257264392C202572333B0A096D756C2E776964652E753332200925726431302C202572312C203235363B0A096164642E733634200925726431312C2025726431302C20257264393B0A0973686C2E623634200925726431322C2025726431312C20323B0A096164642E733634200925726431332C20257264382C2025726431323B0A096C642E676C6F62616C2E6E632E76342E66333220097B2566312C202566322C202566332C202566347D2C205B25726431335D3B0A096164642E733634200925726431342C20257264372C2025726431323B0A096C642E676C6F62616C2E6E632E76342E66333220097B2566352C202566362C202566372C202566387D2C205B25726431345D3B0A096164642E733634200925726431352C20257264362C2025726431323B0A096C642E676C6F62616C2E6E632E76342E66333220097B2566392C20256631302C20256631312C20256631327D2C205B25726431355D3B0A096164642E726E2E6633322009256631332C202566312C202566353B0A096164642E726E2E6633322009256631342C202566322C202566363B0A096164642E726E2E6633322009256631352C202566332C202566373B0A096164642E726E2E6633322009256631362C202566342C202566383B0A096164642E726E2E6633322009256631372C20256631362C20256631323B0A096164642E726E2E6633322009256631382C20256631352C20256631313B0A096164642E726E2E6633322009256631392C20256631342C20256631303B0A096164642E726E2E6633322009256632302C20256631332C202566393B0A096164642E733634200925726431362C20257264332C2025726431323B0A0973742E676C6F62616C2E76342E66333220095B25726431365D2C207B256632302C20256631392C20256631382C20256631377D3B0A097265743B0A0A7D0A0C001400040008000C001000"> : vector<1836xi8>, format = "cuda-nvptx-fb", mime_type = "application/x-flatbuffers"}
  }
  func.func @twoadd_static(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub} {
    %c256 = arith.constant 256 : index
    %c512 = arith.constant 512 : index
    %c1_i32 = arith.constant 1 : i32
    %c553648160_i32 = arith.constant 553648160 : i32
    %c524288 = arith.constant 524288 : index
    %c0 = arith.constant 0 : index
    %c-1_i64 = arith.constant -1 : i64
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_twoadd_static_dispatch_0 = util.global.load @_executable_twoadd_static_dispatch_0 : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_0 = hal.buffer_view.buffer<%arg1 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_0 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    hal.buffer_view.assert<%arg2 : !hal.buffer_view> message("tensor") shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32)
    %buffer_1 = hal.buffer_view.buffer<%arg2 : !hal.buffer_view> : !hal.buffer
    hal.buffer.assert<%buffer_1 : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c524288) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_2 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c524288}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode("OneShot|AllowInlineExecution") categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c524288], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c524288], 
      %c2 = (%buffer_1 : !hal.buffer)[%c0, %c524288], 
      %c3 = (%buffer_2 : !hal.buffer)[%c0, %c524288]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_twoadd_static_dispatch_0 : !hal.executable)[0] workgroups([%c512, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) commands([%cmd])
    %status = hal.fence.await until([%fence]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0, %c524288] shape([%c512, %c256]) type(%c553648160_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::ConversionPass (iree-vm-conversion) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
    vm.initializer {
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %buffer = vm.rodata.inline "_utf8_hal_executable_format_EAB228F999C2D3A1" {alignment = 1 : i64} : !vm.buffer = "hal.executable.format"
      %buffer_0 = vm.rodata.inline "_utf8_cuda_nvptx_fb_B15B42B96FDBACC" {alignment = 1 : i64} : !vm.buffer = "cuda-nvptx-fb"
      %0:2 = vm.call @hal.device.query.i64(%ref, %buffer, %buffer_0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %c1 = vm.const.i32 1
      %2 = vm.and.i32 %1, %c1 : i32
      %zero = vm.const.i32.zero
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %c1_1 = vm.const.i32 1
      %zero_2 = vm.const.i32.zero
      %zero_3 = vm.const.i32.zero
      %c7 = vm.const.i32 7
      %c1_4 = vm.const.i32 1
      %c1_5 = vm.const.i32 1
      %c7_6 = vm.const.i32 7
      %c1_7 = vm.const.i32 1
      %c2 = vm.const.i32 2
      %c7_8 = vm.const.i32 7
      %c1_9 = vm.const.i32 1
      %c3 = vm.const.i32 3
      %c7_10 = vm.const.i32 7
      %zero_11 = vm.const.i32.zero
      %ref_12 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_2, [(%zero_3, %c7, %c1_4), (%c1_5, %c7_6, %c1_7), (%c2, %c7_8, %c1_9), (%c3, %c7_10, %zero_11)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %zero_13 = vm.const.i32.zero
      %ref_14 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_13, [%ref_12]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_14, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %buffer_15 = vm.rodata.inline "_utf8_cuda_nvptx_fb_B15B42B96FDBACC" {alignment = 1 : i64} : !vm.buffer = "cuda-nvptx-fb"
      %null = vm.const.ref.zero : !vm.buffer
      %ref_16 = vm.call.variadic @hal.executable.create(%ref, %buffer_15, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_16 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      %null_17 = vm.const.ref.zero : !vm.ref<!hal.executable>
      vm.br ^bb3(%null_17 : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
    vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
    vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c256 = vm.const.i64 256
      %c512 = vm.const.i64 512
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c524288 = vm.const.i64 524288
      %zero = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c1_0 = vm.const.i64 1
      %c2 = vm.const.i64 2
      %c3 = vm.const.i64 3
      %c-1_1 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      %buffer = vm.rodata.inline "_utf8_tensor_3C6209B4FD120BDC" {alignment = 1 : i64} : !vm.buffer = "tensor"
      vm.call.variadic @hal.buffer_view.assert(%arg0, %buffer, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_2 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_3 = vm.call @hal.device.allocator(%ref_2) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %buffer_4 = vm.rodata.inline "_utf8_tensor_3C6209B4FD120BDC" {alignment = 1 : i64} : !vm.buffer = "tensor"
      %c16 = vm.const.i32 16
      %c3075 = vm.const.i32 3075
      vm.call @hal.buffer.assert(%ref, %buffer_4, %ref_3, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %buffer_5 = vm.rodata.inline "_utf8_tensor_3C6209B4FD120BDC" {alignment = 1 : i64} : !vm.buffer = "tensor"
      vm.call.variadic @hal.buffer_view.assert(%arg1, %buffer_5, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %buffer_7 = vm.rodata.inline "_utf8_tensor_3C6209B4FD120BDC" {alignment = 1 : i64} : !vm.buffer = "tensor"
      %c16_8 = vm.const.i32 16
      %c3075_9 = vm.const.i32 3075
      vm.call @hal.buffer.assert(%ref_6, %buffer_7, %ref_3, %c524288, %c16_8, %c3075_9) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %buffer_10 = vm.rodata.inline "_utf8_tensor_3C6209B4FD120BDC" {alignment = 1 : i64} : !vm.buffer = "tensor"
      vm.call.variadic @hal.buffer_view.assert(%arg2, %buffer_10, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_11 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %buffer_12 = vm.rodata.inline "_utf8_tensor_3C6209B4FD120BDC" {alignment = 1 : i64} : !vm.buffer = "tensor"
      %c16_13 = vm.const.i32 16
      %c3075_14 = vm.const.i32 3075
      vm.call @hal.buffer.assert(%ref_11, %buffer_12, %ref_3, %c524288, %c16_13, %c3075_14) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %c50 = vm.const.i32 50
      %c150998019 = vm.const.i32 150998019
      %ref_15 = vm.call @hal.allocator.allocate(%ref_3, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %c17 = vm.const.i32 17
      %c3_16 = vm.const.i32 3
      %zero_17 = vm.const.i32.zero
      %ref_18 = vm.call @hal.command_buffer.create(%ref_2, %c17, %c3_16, %zero_17) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %zero_19 = vm.const.i32.zero
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_18, %_pipeline_layout_0, %zero, [(%zero, %zero_19, %ref, %zero, %c524288), (%c1_0, %zero_19, %ref_6, %zero, %c524288), (%c2, %zero_19, %ref_11, %zero, %c524288), (%c3, %zero_19, %ref_15, %zero, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      %zero_20 = vm.const.i32.zero
      %c512_21 = vm.const.i32 512
      %c1_22 = vm.const.i32 1
      %c1_23 = vm.const.i32 1
      vm.call @hal.command_buffer.dispatch(%ref_18, %_executable_twoadd_static_dispatch_0, %zero_20, %c512_21, %c1_22, %c1_23) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      %c28 = vm.const.i32 28
      %c13 = vm.const.i32 13
      %zero_24 = vm.const.i32.zero
      vm.call @hal.command_buffer.execution_barrier(%ref_18, %c28, %c13, %zero_24) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_18) : (!vm.ref<!hal.command_buffer>) -> ()
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %zero_25 = vm.const.i32.zero
      %ref_26 = vm.call @hal.fence.create(%ref_2, %zero_25) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_2, %c-1, %null, %ref_26, [%ref_18]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_26]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_fail %0, "failed to wait on timepoint"
      %ref_27 = vm.call.variadic @hal.buffer_view.create(%ref_15, %zero, %c524288, %c553648160, %c1, [%c512, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_27 : !vm.ref<!hal.buffer_view>
    ^bb2:  // pred: ^bb0
      %c2_28 = vm.const.i32 2
      vm.fail %c2_28, "device not supported in the compiled configuration"
    }
    vm.export @twoadd_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::HoistInlinedRodataPass (iree-vm-hoist-inlined-rodata) //----- //
vm.module public @module {
  vm.global.i32 private @_device_query_0 : i32
  vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC_0 {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.initializer {
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %c1 = vm.const.i32 1
    %2 = vm.and.i32 %1, %c1 : i32
    %zero = vm.const.i32.zero
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %c1_0 = vm.const.i32 1
    %zero_1 = vm.const.i32.zero
    %zero_2 = vm.const.i32.zero
    %c7 = vm.const.i32 7
    %c1_3 = vm.const.i32 1
    %c1_4 = vm.const.i32 1
    %c7_5 = vm.const.i32 7
    %c1_6 = vm.const.i32 1
    %c2 = vm.const.i32 2
    %c7_7 = vm.const.i32 7
    %c1_8 = vm.const.i32 1
    %c3 = vm.const.i32 3
    %c7_9 = vm.const.i32 7
    %zero_10 = vm.const.i32.zero
    %ref_11 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_1, [(%zero_2, %c7, %c1_3), (%c1_4, %c7_5, %c1_6), (%c2, %c7_7, %c1_8), (%c3, %c7_9, %zero_10)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %zero_12 = vm.const.i32.zero
    %ref_13 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_12, [%ref_11]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_13, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC_0 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC_0 : !vm.buffer
    %null = vm.const.ref.zero : !vm.buffer
    %ref_14 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC_0, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_14 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    %null_15 = vm.const.ref.zero : !vm.ref<!hal.executable>
    vm.br ^bb3(%null_15 : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
  vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
  vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
  vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
  vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC_1 {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC_2 {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC_3 {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC_4 {alignment = 1 : i64} "tensor"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC_5 {alignment = 1 : i64} "tensor"
  vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c256 = vm.const.i64 256
    %c512 = vm.const.i64 512
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c524288 = vm.const.i64 524288
    %zero = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c1_0 = vm.const.i64 1
    %c2 = vm.const.i64 2
    %c3 = vm.const.i64 3
    %c-1_1 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_2 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_3 = vm.call @hal.device.allocator(%ref_2) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC_1 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC_1 : !vm.buffer
    %c16 = vm.const.i32 16
    %c3075 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC_1, %ref_3, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_tensor_3C6209B4FD120BDC_2 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC_2 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC_2, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_4 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_3C6209B4FD120BDC_3 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC_3 : !vm.buffer
    %c16_5 = vm.const.i32 16
    %c3075_6 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref_4, %_utf8_tensor_3C6209B4FD120BDC_3, %ref_3, %c524288, %c16_5, %c3075_6) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_tensor_3C6209B4FD120BDC_4 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC_4 : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC_4, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_7 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_3C6209B4FD120BDC_5 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC_5 : !vm.buffer
    %c16_8 = vm.const.i32 16
    %c3075_9 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref_7, %_utf8_tensor_3C6209B4FD120BDC_5, %ref_3, %c524288, %c16_8, %c3075_9) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %c50 = vm.const.i32 50
    %c150998019 = vm.const.i32 150998019
    %ref_10 = vm.call @hal.allocator.allocate(%ref_3, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %c17 = vm.const.i32 17
    %c3_11 = vm.const.i32 3
    %zero_12 = vm.const.i32.zero
    %ref_13 = vm.call @hal.command_buffer.create(%ref_2, %c17, %c3_11, %zero_12) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %zero_14 = vm.const.i32.zero
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_13, %_pipeline_layout_0, %zero, [(%zero, %zero_14, %ref, %zero, %c524288), (%c1_0, %zero_14, %ref_4, %zero, %c524288), (%c2, %zero_14, %ref_7, %zero, %c524288), (%c3, %zero_14, %ref_10, %zero, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    %zero_15 = vm.const.i32.zero
    %c512_16 = vm.const.i32 512
    %c1_17 = vm.const.i32 1
    %c1_18 = vm.const.i32 1
    vm.call @hal.command_buffer.dispatch(%ref_13, %_executable_twoadd_static_dispatch_0, %zero_15, %c512_16, %c1_17, %c1_18) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    %c28 = vm.const.i32 28
    %c13 = vm.const.i32 13
    %zero_19 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_13, %c28, %c13, %zero_19) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_13) : (!vm.ref<!hal.command_buffer>) -> ()
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %zero_20 = vm.const.i32.zero
    %ref_21 = vm.call @hal.fence.create(%ref_2, %zero_20) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_2, %c-1, %null, %ref_21, [%ref_13]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_21]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %0, "failed to wait on timepoint"
    %ref_22 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero, %c524288, %c553648160, %c1, [%c512, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_22 : !vm.ref<!hal.buffer_view>
  ^bb2:  // pred: ^bb0
    %c2_23 = vm.const.i32 2
    vm.fail %c2_23, "device not supported in the compiled configuration"
  }
  vm.export @twoadd_static attributes {iree.abi.stub}
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::DeduplicateRodataPass (iree-vm-deduplicate-rodata) //----- //
vm.module public @module {
  vm.global.i32 private @_device_query_0 : i32
  vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.initializer {
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %c1 = vm.const.i32 1
    %2 = vm.and.i32 %1, %c1 : i32
    %zero = vm.const.i32.zero
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %c1_0 = vm.const.i32 1
    %zero_1 = vm.const.i32.zero
    %zero_2 = vm.const.i32.zero
    %c7 = vm.const.i32 7
    %c1_3 = vm.const.i32 1
    %c1_4 = vm.const.i32 1
    %c7_5 = vm.const.i32 7
    %c1_6 = vm.const.i32 1
    %c2 = vm.const.i32 2
    %c7_7 = vm.const.i32 7
    %c1_8 = vm.const.i32 1
    %c3 = vm.const.i32 3
    %c7_9 = vm.const.i32 7
    %zero_10 = vm.const.i32.zero
    %ref_11 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_1, [(%zero_2, %c7, %c1_3), (%c1_4, %c7_5, %c1_6), (%c2, %c7_7, %c1_8), (%c3, %c7_9, %zero_10)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %zero_12 = vm.const.i32.zero
    %ref_13 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_12, [%ref_11]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_13, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC_14 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %null = vm.const.ref.zero : !vm.buffer
    %ref_15 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC_14, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_15 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    %null_16 = vm.const.ref.zero : !vm.ref<!hal.executable>
    vm.br ^bb3(%null_16 : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
  vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
  vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
  vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
  vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c256 = vm.const.i64 256
    %c512 = vm.const.i64 512
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c524288 = vm.const.i64 524288
    %zero = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c1_0 = vm.const.i64 1
    %c2 = vm.const.i64 2
    %c3 = vm.const.i64 3
    %c-1_1 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_2 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_3 = vm.call @hal.device.allocator(%ref_2) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC_4 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    %c16 = vm.const.i32 16
    %c3075 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC_4, %ref_3, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_tensor_3C6209B4FD120BDC_5 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC_5, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_6 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_3C6209B4FD120BDC_7 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    %c16_8 = vm.const.i32 16
    %c3075_9 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref_6, %_utf8_tensor_3C6209B4FD120BDC_7, %ref_3, %c524288, %c16_8, %c3075_9) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %_utf8_tensor_3C6209B4FD120BDC_10 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC_10, %c553648160, %c1, [%c512, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_11 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %_utf8_tensor_3C6209B4FD120BDC_12 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    %c16_13 = vm.const.i32 16
    %c3075_14 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref_11, %_utf8_tensor_3C6209B4FD120BDC_12, %ref_3, %c524288, %c16_13, %c3075_14) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %c50 = vm.const.i32 50
    %c150998019 = vm.const.i32 150998019
    %ref_15 = vm.call @hal.allocator.allocate(%ref_3, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %c17 = vm.const.i32 17
    %c3_16 = vm.const.i32 3
    %zero_17 = vm.const.i32.zero
    %ref_18 = vm.call @hal.command_buffer.create(%ref_2, %c17, %c3_16, %zero_17) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %zero_19 = vm.const.i32.zero
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_18, %_pipeline_layout_0, %zero, [(%zero, %zero_19, %ref, %zero, %c524288), (%c1_0, %zero_19, %ref_6, %zero, %c524288), (%c2, %zero_19, %ref_11, %zero, %c524288), (%c3, %zero_19, %ref_15, %zero, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    %zero_20 = vm.const.i32.zero
    %c512_21 = vm.const.i32 512
    %c1_22 = vm.const.i32 1
    %c1_23 = vm.const.i32 1
    vm.call @hal.command_buffer.dispatch(%ref_18, %_executable_twoadd_static_dispatch_0, %zero_20, %c512_21, %c1_22, %c1_23) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    %c28 = vm.const.i32 28
    %c13 = vm.const.i32 13
    %zero_24 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_18, %c28, %c13, %zero_24) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_18) : (!vm.ref<!hal.command_buffer>) -> ()
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %zero_25 = vm.const.i32.zero
    %ref_26 = vm.call @hal.fence.create(%ref_2, %zero_25) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_2, %c-1, %null, %ref_26, [%ref_18]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_1, [%ref_26]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %0, "failed to wait on timepoint"
    %ref_27 = vm.call.variadic @hal.buffer_view.create(%ref_15, %zero, %c524288, %c553648160, %c1, [%c512, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_27 : !vm.ref<!hal.buffer_view>
  ^bb2:  // pred: ^bb0
    %c2_28 = vm.const.i32 2
    vm.fail %c2_28, "device not supported in the compiled configuration"
  }
  vm.export @twoadd_static attributes {iree.abi.stub}
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c3 = vm.const.i32 3
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC_3 = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC_3, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
    vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
    vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c512 = vm.const.i32 512
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c512_0 = vm.const.i64 512
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c1_2 = vm.const.i64 1
      %c2_3 = vm.const.i64 2
      %c3_4 = vm.const.i64 3
      %c-1_5 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC_8 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC_8, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_tensor_3C6209B4FD120BDC_9 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC_9, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_10 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_3C6209B4FD120BDC_11 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref_10, %_utf8_tensor_3C6209B4FD120BDC_11, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %_utf8_tensor_3C6209B4FD120BDC_12 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC_12, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_13 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %_utf8_tensor_3C6209B4FD120BDC_14 = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref_13, %_utf8_tensor_3C6209B4FD120BDC_14, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_15 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_16 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_16, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_10, %zero_1, %c524288), (%c2_3, %zero, %ref_13, %zero_1, %c524288), (%c3_4, %zero, %ref_15, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_16, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_16, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_16) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_17 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_17, [%ref_16]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_17]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3(%0 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %ref_18 = vm.call.variadic @hal.buffer_view.create(%ref_15, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_18 : !vm.ref<!hal.buffer_view>
    ^bb3(%1: i32):  // pred: ^bb1
      vm.fail %1, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @twoadd_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c3 = vm.const.i32 3
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
    vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
    vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c512 = vm.const.i32 512
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c512_0 = vm.const.i64 512
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c1_2 = vm.const.i64 1
      %c2_3 = vm.const.i64 2
      %c3_4 = vm.const.i64 3
      %c-1_5 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3(%0 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_13 : !vm.ref<!hal.buffer_view>
    ^bb3(%1: i32):  // pred: ^bb1
      vm.fail %1, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @twoadd_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"0x0800000043554441E8F8FFFF10000000480000003C0000005000000001000000040000002700000074776F6164645F7374617469635F64697370617463685F305F67656E657269635F31333130373200010000000000000001000000400000000100000001000000B40600002F2F0A2F2F2047656E657261746564206279204C4C564D204E56505458204261636B2D456E640A2F2F0A0A2E76657273696F6E20362E300A2E74617267657420736D5F37300A2E616464726573735F73697A652036340A0A092F2F202E676C6F626C0974776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037320A0A2E76697369626C65202E656E7472792074776F6164645F7374617469635F64697370617463685F305F67656E657269635F313331303732280A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F302C0A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F312C0A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F322C0A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F330A290A2E6D61786E7469642036342C20312C20310A7B0A092E726567202E623332200925723C343E3B0A092E726567202E663332200925663C32313E3B0A092E726567202E62363420092572643C31373E3B0A0A096C642E706172616D2E7536342009257264312C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F305D3B0A096C642E706172616D2E7536342009257264322C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F335D3B0A09637674612E746F2E676C6F62616C2E7536342009257264332C20257264323B0A096C642E706172616D2E7536342009257264342C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F315D3B0A096C642E706172616D2E7536342009257264352C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F325D3B0A09637674612E746F2E676C6F62616C2E7536342009257264362C20257264353B0A09637674612E746F2E676C6F62616C2E7536342009257264372C20257264343B0A09637674612E746F2E676C6F62616C2E7536342009257264382C20257264313B0A096D6F762E75333220092572312C202563746169642E783B0A096D6F762E75333220092572322C20257469642E783B0A0973686C2E62333220092572332C202572322C20323B0A096376742E7536342E7533322009257264392C202572333B0A096D756C2E776964652E753332200925726431302C202572312C203235363B0A096164642E733634200925726431312C2025726431302C20257264393B0A0973686C2E623634200925726431322C2025726431312C20323B0A096164642E733634200925726431332C20257264382C2025726431323B0A096C642E676C6F62616C2E6E632E76342E66333220097B2566312C202566322C202566332C202566347D2C205B25726431335D3B0A096164642E733634200925726431342C20257264372C2025726431323B0A096C642E676C6F62616C2E6E632E76342E66333220097B2566352C202566362C202566372C202566387D2C205B25726431345D3B0A096164642E733634200925726431352C20257264362C2025726431323B0A096C642E676C6F62616C2E6E632E76342E66333220097B2566392C20256631302C20256631312C20256631327D2C205B25726431355D3B0A096164642E726E2E6633322009256631332C202566312C202566353B0A096164642E726E2E6633322009256631342C202566322C202566363B0A096164642E726E2E6633322009256631352C202566332C202566373B0A096164642E726E2E6633322009256631362C202566342C202566383B0A096164642E726E2E6633322009256631372C20256631362C20256631323B0A096164642E726E2E6633322009256631382C20256631352C20256631313B0A096164642E726E2E6633322009256631392C20256631342C20256631303B0A096164642E726E2E6633322009256632302C20256631332C202566393B0A096164642E733634200925726431362C20257264332C2025726431323B0A0973742E676C6F62616C2E76342E66333220095B25726431365D2C207B256632302C20256631392C20256631382C20256631377D3B0A097265743B0A0A7D0A0C001400040008000C001000"> : vector<1836xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c3 = vm.const.i32 3
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
    vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
    vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c512 = vm.const.i32 512
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c512_0 = vm.const.i64 512
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c1_2 = vm.const.i64 1
      %c2_3 = vm.const.i64 2
      %c3_4 = vm.const.i64 3
      %c-1_5 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_13 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @twoadd_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c3 = vm.const.i32 3
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
    vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
    vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c512 = vm.const.i32 512
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c512_0 = vm.const.i64 512
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c1_2 = vm.const.i64 1
      %c2_3 = vm.const.i64 2
      %c3_4 = vm.const.i64 3
      %c-1_5 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_13 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @twoadd_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c3 = vm.const.i32 3
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
    vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
    vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c512 = vm.const.i32 512
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c512_0 = vm.const.i64 512
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c1_2 = vm.const.i64 1
      %c2_3 = vm.const.i64 2
      %c3_4 = vm.const.i64 3
      %c-1_5 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_13 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @twoadd_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::ResolveRodataLoadsPass (iree-vm-resolve-rodata-loads) //----- //
vm.module public @module {
  vm.global.i32 private @_device_query_0 : i32
  vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.initializer {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c3 = vm.const.i32 3
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
  vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
  vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
  vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
  vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c512 = vm.const.i32 512
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c512_0 = vm.const.i64 512
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c1_2 = vm.const.i64 1
    %c2_3 = vm.const.i64 2
    %c3_4 = vm.const.i64 3
    %c-1_5 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_13 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @twoadd_static attributes {iree.abi.stub}
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.initializer {
  %null = vm.const.ref.zero : !vm.ref<!hal.executable>
  %null_0 = vm.const.ref.zero : !vm.buffer
  %c3 = vm.const.i32 3
  %c2 = vm.const.i32 2
  %c7 = vm.const.i32 7
  %zero = vm.const.i32.zero
  %c1 = vm.const.i32 1
  %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
  %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
  %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
  %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
  %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
  %2 = vm.and.i32 %1, %c1 : i32
  %3 = vm.select.i32 %0#0, %2, %zero : i32
  %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
  %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
  vm.global.store.i32 %3, @_device_query_0 : i32
  vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.cond_br %3, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
  %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
  vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
^bb2:  // pred: ^bb0
  vm.br ^bb3(%null : !vm.ref<!hal.executable>)
^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
  vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
  %c2 = vm.const.i32 2
  %null = vm.const.ref.zero : !vm.ref<!hal.fence>
  %c13 = vm.const.i32 13
  %c28 = vm.const.i32 28
  %c512 = vm.const.i32 512
  %zero = vm.const.i32.zero
  %c3 = vm.const.i32 3
  %c17 = vm.const.i32 17
  %c150998019 = vm.const.i32 150998019
  %c50 = vm.const.i32 50
  %c3075 = vm.const.i32 3075
  %c16 = vm.const.i32 16
  %c256 = vm.const.i64 256
  %c512_0 = vm.const.i64 512
  %c1 = vm.const.i32 1
  %c553648160 = vm.const.i32 553648160
  %c524288 = vm.const.i64 524288
  %zero_1 = vm.const.i64.zero
  %c-1 = vm.const.i64 -1
  %c1_2 = vm.const.i64 1
  %c2_3 = vm.const.i64 2
  %c3_4 = vm.const.i64 3
  %c-1_5 = vm.const.i32 -1
  %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
  %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
  vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
  %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
  vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
  vm.cond_br %_device_query_0, ^bb1, ^bb4
^bb1:  // pred: ^bb0
  vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
  %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
  %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %0, ^bb3, ^bb2
^bb2:  // pred: ^bb1
  %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
  vm.return %ref_13 : !vm.ref<!hal.buffer_view>
^bb3:  // pred: ^bb1
  vm.fail %0, "failed to wait on timepoint"
^bb4:  // pred: ^bb0
  vm.fail %c2, "device not supported in the compiled configuration"
}

// -----// IR Dump After Inliner (inline) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c3 = vm.const.i32 3
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.ex.submit_and_wait(%device : !vm.ref<!hal.device>, %command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...) attributes {sym_visibility = "private"}
    vm.import @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32 attributes {sym_visibility = "private"}
    vm.import @hal.fence.signal(%fence : !vm.ref<!hal.fence>) attributes {sym_visibility = "private"}
    vm.import @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32) attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c512 = vm.const.i32 512
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c512_0 = vm.const.i64 512
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c1_2 = vm.const.i64 1
      %c2_3 = vm.const.i64 2
      %c3_4 = vm.const.i64 3
      %c-1_5 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_13 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @twoadd_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c3 = vm.const.i32 3
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c512 = vm.const.i32 512
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c512_0 = vm.const.i64 512
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c1_2 = vm.const.i64 1
      %c2_3 = vm.const.i64 2
      %c3_4 = vm.const.i64 3
      %c-1_5 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_13 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @twoadd_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c3 = vm.const.i32 3
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c512 = vm.const.i32 512
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c512_0 = vm.const.i64 512
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c1_2 = vm.const.i64 1
      %c2_3 = vm.const.i64 2
      %c3_4 = vm.const.i64 3
      %c-1_5 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_13 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @twoadd_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c3 = vm.const.i32 3
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c512 = vm.const.i32 512
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c512_0 = vm.const.i64 512
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c1_2 = vm.const.i64 1
      %c2_3 = vm.const.i64 2
      %c3_4 = vm.const.i64 3
      %c-1_5 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_13 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @twoadd_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c3 = vm.const.i32 3
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c512 = vm.const.i32 512
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c512_0 = vm.const.i64 512
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c1_2 = vm.const.i64 1
      %c2_3 = vm.const.i64 2
      %c3_4 = vm.const.i64 3
      %c-1_5 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_13 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @twoadd_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c3 = vm.const.i32 3
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c512 = vm.const.i32 512
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c512_0 = vm.const.i64 512
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c1_2 = vm.const.i64 1
      %c2_3 = vm.const.i64 2
      %c3_4 = vm.const.i64 3
      %c-1_5 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_13 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @twoadd_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c3 = vm.const.i32 3
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c512 = vm.const.i32 512
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c512_0 = vm.const.i64 512
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c1_2 = vm.const.i64 1
      %c2_3 = vm.const.i64 2
      %c3_4 = vm.const.i64 3
      %c-1_5 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_13 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @twoadd_static attributes {iree.abi.stub}
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::GlobalInitializationPass (iree-vm-global-initialization) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c512 = vm.const.i32 512
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c512_0 = vm.const.i64 512
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c1_2 = vm.const.i64 1
    %c2_3 = vm.const.i64 2
    %c3_4 = vm.const.i64 3
    %c-1_5 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_13 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @twoadd_static attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c3 = vm.const.i32 3
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.br ^bb4
  ^bb4:  // pred: ^bb3
    vm.return
  }
  vm.export @__deinit
  vm.func private @__deinit() {
    vm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c512 = vm.const.i32 512
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c512_0 = vm.const.i64 512
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c1_2 = vm.const.i64 1
      %c2_3 = vm.const.i64 2
      %c3_4 = vm.const.i64 3
      %c-1_5 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_13 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @twoadd_static attributes {iree.abi.stub}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c3 = vm.const.i32 3
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.export @__deinit
    vm.func private @__deinit() {
      vm.return
    }
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c512 = vm.const.i32 512
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c512_0 = vm.const.i64 512
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c1_2 = vm.const.i64 1
      %c2_3 = vm.const.i64 2
      %c3_4 = vm.const.i64 3
      %c-1_5 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_13 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @twoadd_static attributes {iree.abi.stub}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c3 = vm.const.i32 3
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.export @__deinit
    vm.func private @__deinit() {
      vm.return
    }
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c512 = vm.const.i32 512
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c512_0 = vm.const.i64 512
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c1_2 = vm.const.i64 1
      %c2_3 = vm.const.i64 2
      %c3_4 = vm.const.i64 3
      %c-1_5 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_13 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @twoadd_static attributes {iree.abi.stub}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c3 = vm.const.i32 3
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
    vm.export @__deinit
    vm.func private @__deinit() {
      vm.return
    }
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::DropEmptyModuleInitializersPass (iree-vm-drop-empty-module-initializers) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c512 = vm.const.i32 512
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c512_0 = vm.const.i64 512
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c1_2 = vm.const.i64 1
    %c2_3 = vm.const.i64 2
    %c3_4 = vm.const.i64 3
    %c-1_5 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_13 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @twoadd_static attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c3 = vm.const.i32 3
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After DropCompilerHints (iree-util-drop-compiler-hints) //----- //
#executable_target_cuda_nvptx_fb = #hal.executable.target<"cuda", "cuda-nvptx-fb", {target_arch = "sm_70"}>
#device_target_cuda = #hal.device.target<"cuda", {executable_targets = [#executable_target_cuda_nvptx_fb], legacy_sync}>
module attributes {hal.device.targets = [#device_target_cuda], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
    vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
    vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
    vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
    vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
    vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
    vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
    vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
    vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
    vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c512 = vm.const.i32 512
      %zero = vm.const.i32.zero
      %c3 = vm.const.i32 3
      %c17 = vm.const.i32 17
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c256 = vm.const.i64 256
      %c512_0 = vm.const.i64 512
      %c1 = vm.const.i32 1
      %c553648160 = vm.const.i32 553648160
      %c524288 = vm.const.i64 524288
      %zero_1 = vm.const.i64.zero
      %c-1 = vm.const.i64 -1
      %c1_2 = vm.const.i64 1
      %c2_3 = vm.const.i64 2
      %c3_4 = vm.const.i64 3
      %c-1_5 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_13 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @twoadd_static attributes {iree.abi.stub}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c3 = vm.const.i32 3
      %c2 = vm.const.i32 2
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
      vm.return
    }
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::GlobalInitializationPass (iree-vm-global-initialization) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c512 = vm.const.i32 512
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c512_0 = vm.const.i64 512
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c1_2 = vm.const.i64 1
    %c2_3 = vm.const.i64 2
    %c3_4 = vm.const.i64 3
    %c-1_5 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_13 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @twoadd_static attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c3 = vm.const.i32 3
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.br ^bb4
  ^bb4:  // pred: ^bb3
    vm.return
  }
  vm.export @__deinit
  vm.func private @__deinit() {
    vm.return
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::DropEmptyModuleInitializersPass (iree-vm-drop-empty-module-initializers) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c512 = vm.const.i32 512
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c512_0 = vm.const.i64 512
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c1_2 = vm.const.i64 1
    %c2_3 = vm.const.i64 2
    %c3_4 = vm.const.i64 3
    %c-1_5 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_13 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @twoadd_static attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c3 = vm.const.i32 3
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.br ^bb4
  ^bb4:  // pred: ^bb3
    vm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
  %c2 = vm.const.i32 2
  %null = vm.const.ref.zero : !vm.ref<!hal.fence>
  %c13 = vm.const.i32 13
  %c28 = vm.const.i32 28
  %c512 = vm.const.i32 512
  %zero = vm.const.i32.zero
  %c3 = vm.const.i32 3
  %c17 = vm.const.i32 17
  %c150998019 = vm.const.i32 150998019
  %c50 = vm.const.i32 50
  %c3075 = vm.const.i32 3075
  %c16 = vm.const.i32 16
  %c256 = vm.const.i64 256
  %c512_0 = vm.const.i64 512
  %c1 = vm.const.i32 1
  %c553648160 = vm.const.i32 553648160
  %c524288 = vm.const.i64 524288
  %zero_1 = vm.const.i64.zero
  %c-1 = vm.const.i64 -1
  %c1_2 = vm.const.i64 1
  %c2_3 = vm.const.i64 2
  %c3_4 = vm.const.i64 3
  %c-1_5 = vm.const.i32 -1
  %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
  %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
  vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
  %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
  vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
  vm.cond_br %_device_query_0, ^bb1, ^bb4
^bb1:  // pred: ^bb0
  vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
  %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
  %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %0, ^bb3, ^bb2
^bb2:  // pred: ^bb1
  %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
  vm.return %ref_13 : !vm.ref<!hal.buffer_view>
^bb3:  // pred: ^bb1
  vm.fail %0, "failed to wait on timepoint"
^bb4:  // pred: ^bb0
  vm.fail %c2, "device not supported in the compiled configuration"
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @__init() {
  %null = vm.const.ref.zero : !vm.ref<!hal.executable>
  %null_0 = vm.const.ref.zero : !vm.buffer
  %c3 = vm.const.i32 3
  %c2 = vm.const.i32 2
  %c7 = vm.const.i32 7
  %zero = vm.const.i32.zero
  %c1 = vm.const.i32 1
  %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
  %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
  %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
  %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
  %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
  %2 = vm.and.i32 %1, %c1 : i32
  %3 = vm.select.i32 %0#0, %2, %zero : i32
  %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
  %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
  vm.global.store.i32 %3, @_device_query_0 : i32
  vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.cond_br %3, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
  %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
  vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
^bb2:  // pred: ^bb0
  vm.br ^bb3(%null : !vm.ref<!hal.executable>)
^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
  vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.return
}

// -----// IR Dump After Inliner (inline) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"0x0800000043554441E8F8FFFF10000000480000003C0000005000000001000000040000002700000074776F6164645F7374617469635F64697370617463685F305F67656E657269635F31333130373200010000000000000001000000400000000100000001000000B40600002F2F0A2F2F2047656E657261746564206279204C4C564D204E56505458204261636B2D456E640A2F2F0A0A2E76657273696F6E20362E300A2E74617267657420736D5F37300A2E616464726573735F73697A652036340A0A092F2F202E676C6F626C0974776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037320A0A2E76697369626C65202E656E7472792074776F6164645F7374617469635F64697370617463685F305F67656E657269635F313331303732280A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F302C0A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F312C0A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F322C0A092E706172616D202E7536342074776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F330A290A2E6D61786E7469642036342C20312C20310A7B0A092E726567202E623332200925723C343E3B0A092E726567202E663332200925663C32313E3B0A092E726567202E62363420092572643C31373E3B0A0A096C642E706172616D2E7536342009257264312C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F305D3B0A096C642E706172616D2E7536342009257264322C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F335D3B0A09637674612E746F2E676C6F62616C2E7536342009257264332C20257264323B0A096C642E706172616D2E7536342009257264342C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F315D3B0A096C642E706172616D2E7536342009257264352C205B74776F6164645F7374617469635F64697370617463685F305F67656E657269635F3133313037325F706172616D5F325D3B0A09637674612E746F2E676C6F62616C2E7536342009257264362C20257264353B0A09637674612E746F2E676C6F62616C2E7536342009257264372C20257264343B0A09637674612E746F2E676C6F62616C2E7536342009257264382C20257264313B0A096D6F762E75333220092572312C202563746169642E783B0A096D6F762E75333220092572322C20257469642E783B0A0973686C2E62333220092572332C202572322C20323B0A096376742E7536342E7533322009257264392C202572333B0A096D756C2E776964652E753332200925726431302C202572312C203235363B0A096164642E733634200925726431312C2025726431302C20257264393B0A0973686C2E623634200925726431322C2025726431312C20323B0A096164642E733634200925726431332C20257264382C2025726431323B0A096C642E676C6F62616C2E6E632E76342E66333220097B2566312C202566322C202566332C202566347D2C205B25726431335D3B0A096164642E733634200925726431342C20257264372C2025726431323B0A096C642E676C6F62616C2E6E632E76342E66333220097B2566352C202566362C202566372C202566387D2C205B25726431345D3B0A096164642E733634200925726431352C20257264362C2025726431323B0A096C642E676C6F62616C2E6E632E76342E66333220097B2566392C20256631302C20256631312C20256631327D2C205B25726431355D3B0A096164642E726E2E6633322009256631332C202566312C202566353B0A096164642E726E2E6633322009256631342C202566322C202566363B0A096164642E726E2E6633322009256631352C202566332C202566373B0A096164642E726E2E6633322009256631362C202566342C202566383B0A096164642E726E2E6633322009256631372C20256631362C20256631323B0A096164642E726E2E6633322009256631382C20256631352C20256631313B0A096164642E726E2E6633322009256631392C20256631342C20256631303B0A096164642E726E2E6633322009256632302C20256631332C202566393B0A096164642E733634200925726431362C20257264332C2025726431323B0A0973742E676C6F62616C2E76342E66333220095B25726431365D2C207B256632302C20256631392C20256631382C20256631377D3B0A097265743B0A0A7D0A0C001400040008000C001000"> : vector<1836xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c512 = vm.const.i32 512
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c512_0 = vm.const.i64 512
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c1_2 = vm.const.i64 1
    %c2_3 = vm.const.i64 2
    %c3_4 = vm.const.i64 3
    %c-1_5 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_13 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @twoadd_static attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c3 = vm.const.i32 3
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After CSE (cse) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c512 = vm.const.i32 512
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c512_0 = vm.const.i64 512
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c1_2 = vm.const.i64 1
    %c2_3 = vm.const.i64 2
    %c3_4 = vm.const.i64 3
    %c-1_5 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_13 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @twoadd_static attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c3 = vm.const.i32 3
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c512 = vm.const.i32 512
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c512_0 = vm.const.i64 512
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c1_2 = vm.const.i64 1
    %c2_3 = vm.const.i64 2
    %c3_4 = vm.const.i64 3
    %c-1_5 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_13 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @twoadd_static attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c3 = vm.const.i32 3
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After DropCompilerHints (iree-util-drop-compiler-hints) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
  vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers"} dense<"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vector<1836xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64} "cuda-nvptx-fb"
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c512 = vm.const.i32 512
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c512_0 = vm.const.i64 512
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c1_2 = vm.const.i64 1
    %c2_3 = vm.const.i64 2
    %c3_4 = vm.const.i64 3
    %c-1_5 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_13 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @twoadd_static attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c3 = vm.const.i32 3
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::OrdinalAllocationPass (iree-vm-ordinal-allocation) //----- //
vm.module public @module attributes {ordinal_counts = #vm.ordinal_counts<import_funcs = 19, export_funcs = 2, internal_funcs = 2, global_bytes = 4, global_refs = 2, rodatas = 4, rwdatas = 0>} {
  vm.global.i32 private mutable @_device_query_0 {ordinal = 0 : i32} : i32
  vm.global.ref private mutable @_pipeline_layout_0 {ordinal = 0 : i32} : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_twoadd_static_dispatch_0 {ordinal = 1 : i32} : !vm.ref<!hal.executable>
  vm.rodata private @twoadd_static_dispatch_0_cuda_nvptx_fb {alignment = 16 : i64, mime_type = "application/x-flatbuffers", ordinal = 0 : i32} dense<"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vector<1836xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64, ordinal = 1 : i32} "hal.executable.format"
  vm.rodata private @_utf8_cuda_nvptx_fb_B15B42B96FDBACC {alignment = 1 : i64, ordinal = 2 : i32} "cuda-nvptx-fb"
  vm.import @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, ordinal = 0 : i32, sym_visibility = "private"}
  vm.import @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {ordinal = 1 : i32, sym_visibility = "private"}
  vm.import @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {ordinal = 2 : i32, sym_visibility = "private"}
  vm.import @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, ordinal = 3 : i32, sym_visibility = "private"}
  vm.import @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {ordinal = 4 : i32, sym_visibility = "private"}
  vm.import @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, ordinal = 5 : i32, sym_visibility = "private"}
  vm.import @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {ordinal = 6 : i32, sym_visibility = "private"}
  vm.import @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {ordinal = 7 : i32, sym_visibility = "private"}
  vm.import @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {ordinal = 8 : i32, sym_visibility = "private"}
  vm.import @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {ordinal = 9 : i32, sym_visibility = "private"}
  vm.import @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {ordinal = 10 : i32, sym_visibility = "private"}
  vm.import @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, ordinal = 11 : i32, sym_visibility = "private"}
  vm.import @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, ordinal = 12 : i32, sym_visibility = "private"}
  vm.import @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, ordinal = 13 : i32, sym_visibility = "private"}
  vm.import @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {ordinal = 14 : i32, sym_visibility = "private"}
  vm.import @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, ordinal = 15 : i32, sym_visibility = "private"}
  vm.import @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {ordinal = 16 : i32, sym_visibility = "private"}
  vm.import @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {ordinal = 17 : i32, sym_visibility = "private", vm.yield}
  vm.import @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, ordinal = 18 : i32, sym_visibility = "private"}
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64, ordinal = 3 : i32} "tensor"
  vm.func private @twoadd_static(%arg0: !vm.ref<!hal.buffer_view>, %arg1: !vm.ref<!hal.buffer_view>, %arg2: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {ordinal = 0 : i32} {
    %c2 = vm.const.i32 2
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c512 = vm.const.i32 512
    %zero = vm.const.i32.zero
    %c3 = vm.const.i32 3
    %c17 = vm.const.i32 17
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c256 = vm.const.i64 256
    %c512_0 = vm.const.i64 512
    %c1 = vm.const.i32 1
    %c553648160 = vm.const.i32 553648160
    %c524288 = vm.const.i64 524288
    %zero_1 = vm.const.i64.zero
    %c-1 = vm.const.i64 -1
    %c1_2 = vm.const.i64 1
    %c2_3 = vm.const.i64 2
    %c3_4 = vm.const.i64 3
    %c-1_5 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_twoadd_static_dispatch_0 = vm.global.load.ref @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_6 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_7 = vm.call @hal.device.allocator(%ref_6) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg1, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_8 = vm.call @hal.buffer_view.buffer(%arg1) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_8, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    vm.call.variadic @hal.buffer_view.assert(%arg2, %_utf8_tensor_3C6209B4FD120BDC, %c553648160, %c1, [%c512_0, %c256]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref_9 = vm.call @hal.buffer_view.buffer(%arg2) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    vm.call @hal.buffer.assert(%ref_9, %_utf8_tensor_3C6209B4FD120BDC, %ref_7, %c524288, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_10 = vm.call @hal.allocator.allocate(%ref_7, %c50, %c150998019, %c524288) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_11 = vm.call @hal.command_buffer.create(%ref_6, %c17, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_11, %_pipeline_layout_0, %zero_1, [(%zero_1, %zero, %ref, %zero_1, %c524288), (%c1_2, %zero, %ref_8, %zero_1, %c524288), (%c2_3, %zero, %ref_9, %zero_1, %c524288), (%c3_4, %zero, %ref_10, %zero_1, %c524288)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_11, %_executable_twoadd_static_dispatch_0, %zero, %c512, %c1, %c1) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_11, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_11) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_12 = vm.call @hal.fence.create(%ref_6, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_6, %c-1, %null, %ref_12, [%ref_11]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %0 = vm.call.variadic @hal.fence.await(%c-1_5, [%ref_12]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_13 = vm.call.variadic @hal.buffer_view.create(%ref_10, %zero_1, %c524288, %c553648160, %c1, [%c512_0, %c256]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_13 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @twoadd_static attributes {iree.abi.stub, ordinal = 0 : i32}
  vm.export @__init attributes {ordinal = 1 : i32}
  vm.func private @__init() attributes {ordinal = 1 : i32} {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c3 = vm.const.i32 3
    %c2 = vm.const.i32 2
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_cuda_nvptx_fb_B15B42B96FDBACC = vm.const.ref.rodata @_utf8_cuda_nvptx_fb_B15B42B96FDBACC : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %c1), (%c2, %c7, %c1), (%c3, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %twoadd_static_dispatch_0_cuda_nvptx_fb = vm.const.ref.rodata @twoadd_static_dispatch_0_cuda_nvptx_fb : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_cuda_nvptx_fb_B15B42B96FDBACC, %twoadd_static_dispatch_0_cuda_nvptx_fb, %null_0, [%_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_twoadd_static_dispatch_0 : !vm.ref<!hal.executable>
    vm.return
  }
}

