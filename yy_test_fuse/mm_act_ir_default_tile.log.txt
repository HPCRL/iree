Pass Manager with 1 passes:
builtin.module(hal.executable(hal.executable.variant(iree-llvmcpu-lower-executable-target{test-lowering-configuration=false use-lowering-pipeline=})))

@@@@@@--- Before Tile + Distribute ---
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
  %5 = tensor.empty() : tensor<512x1024xf32>
  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %7 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7 : tensor<512x1024xf32>) outs(%5 : tensor<512x1024xf32>) {
  ^bb0(%in: f32, %out: f32):
    %9 = math.cos %in : f32
    linalg.yield %9 : f32
  } -> tensor<512x1024xf32>
  flow.dispatch.tensor.store %8, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  return
}

Yufan:: tileAndFuseDispatchUsingSCFForOp::   
%8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7 : tensor<512x1024xf32>) outs(%5 : tensor<512x1024xf32>) {
^bb0(%in: f32, %out: f32):
  %9 = math.cos %in : f32
  linalg.yield %9 : f32
} -> tensor<512x1024xf32>
Yufan:: fusableProducer::   
%7 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%6 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
Yufan:: fusableProducer::   
%6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
@@@@@@--- After Tile + Distribute ---
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.min affine_map<(d0) -> (256, -d0 + 512)>(%arg0)
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %7 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %6 to %c1024 step %7 {
      %8 = affine.min affine_map<(d0) -> (128, -d0 + 1024)>(%arg1)
      %9 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%5, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
      %10 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %8], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
      %11 = tensor.empty(%5, %8) : tensor<?x?xf32>
      %12 = linalg.fill ins(%cst : f32) outs(%11 : tensor<?x?xf32>) -> tensor<?x?xf32>
      %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%9, %10 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%12 : tensor<?x?xf32>) -> tensor<?x?xf32>
      %14 = tensor.empty(%5, %8) : tensor<?x?xf32>
      %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%13 : tensor<?x?xf32>) outs(%14 : tensor<?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        %16 = math.cos %in : f32
        linalg.yield %16 : f32
      } -> tensor<?x?xf32>
      flow.dispatch.tensor.store %15, %2, offsets = [%arg0, %arg1], sizes = [%5, %8], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After TileAndDistributeToWorkgroups (iree-codegen-tile-and-distribute-to-workgroups) //----- //
hal.executable.variant public @embedded_elf_x86_64, target = <"llvm-cpu", "embedded-elf-x86_64", {cpu = "generic", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", native_vector_size = 16 : index, target_triple = "x86_64-unknown-unknown-eabi-elf"}> {
  hal.executable.export public @mm_cosine_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingPadExpert>} {
  ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index):
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    hal.return %c8, %c2, %c1 : index, index, index
  }
  builtin.module {
    func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
      %c256 = arith.constant 256 : index
      %c128 = arith.constant 128 : index
      %c1024 = arith.constant 1024 : index
      %c512 = arith.constant 512 : index
      %c0 = arith.constant 0 : index
      %cst = arith.constant 0.000000e+00 : f32
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
      %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
      %workgroup_id_x = hal.interface.workgroup.id[0] : index
      %workgroup_count_x = hal.interface.workgroup.count[0] : index
      %workgroup_id_y = hal.interface.workgroup.id[1] : index
      %workgroup_count_y = hal.interface.workgroup.count[1] : index
      %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
      %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
      scf.for %arg0 = %3 to %c512 step %4 {
        %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
        %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
        scf.for %arg1 = %5 to %c1024 step %6 {
          %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%c256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
          %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %c128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
          %9 = tensor.empty() : tensor<256x128xf32>
          %cast = tensor.cast %9 : tensor<256x128xf32> to tensor<?x?xf32>
          %10 = linalg.fill ins(%cst : f32) outs(%cast : tensor<?x?xf32>) -> tensor<?x?xf32>
          %11 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%7, %8 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%10 : tensor<?x?xf32>) -> tensor<?x?xf32>
          %12 = tensor.empty() : tensor<256x128xf32>
          %cast_0 = tensor.cast %12 : tensor<256x128xf32> to tensor<?x?xf32>
          %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%11 : tensor<?x?xf32>) outs(%cast_0 : tensor<?x?xf32>) {
          ^bb0(%in: f32, %out: f32):
            %14 = math.cos %in : f32
            linalg.yield %14 : f32
          } -> tensor<?x?xf32>
          flow.dispatch.tensor.store %13, %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
        }
      }
      return
    }
  }
}

// -----// IR Dump After TileAndDecomposeAttention (iree-linalg-ext-tile-and-decompose-attention) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %5 to %c1024 step %6 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%c256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %c128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
      %9 = tensor.empty() : tensor<256x128xf32>
      %cast = tensor.cast %9 : tensor<256x128xf32> to tensor<?x?xf32>
      %10 = linalg.fill ins(%cst : f32) outs(%cast : tensor<?x?xf32>) -> tensor<?x?xf32>
      %11 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%7, %8 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%10 : tensor<?x?xf32>) -> tensor<?x?xf32>
      %12 = tensor.empty() : tensor<256x128xf32>
      %cast_0 = tensor.cast %12 : tensor<256x128xf32> to tensor<?x?xf32>
      %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%11 : tensor<?x?xf32>) outs(%cast_0 : tensor<?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        %14 = math.cos %in : f32
        linalg.yield %14 : f32
      } -> tensor<?x?xf32>
      flow.dispatch.tensor.store %13, %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After DecomposeSoftmax (iree-linalg-ext-decompose-softmax) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %5 to %c1024 step %6 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%c256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %c128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
      %9 = tensor.empty() : tensor<256x128xf32>
      %cast = tensor.cast %9 : tensor<256x128xf32> to tensor<?x?xf32>
      %10 = linalg.fill ins(%cst : f32) outs(%cast : tensor<?x?xf32>) -> tensor<?x?xf32>
      %11 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%7, %8 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%10 : tensor<?x?xf32>) -> tensor<?x?xf32>
      %12 = tensor.empty() : tensor<256x128xf32>
      %cast_0 = tensor.cast %12 : tensor<256x128xf32> to tensor<?x?xf32>
      %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%11 : tensor<?x?xf32>) outs(%cast_0 : tensor<?x?xf32>) {
      ^bb0(%in: f32, %out: f32):
        %14 = math.cos %in : f32
        linalg.yield %14 : f32
      } -> tensor<?x?xf32>
      flow.dispatch.tensor.store %13, %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After ConvertToDestinationPassingStyle (iree-codegen-convert-to-destination-passing-style) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %5 to %c1024 step %6 {
      %7 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<?x?xf32>
      %cast = tensor.cast %7 : tensor<?x?xf32> to tensor<256x128xf32>
      %8 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%c256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %c128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
      %cast_0 = tensor.cast %cast : tensor<256x128xf32> to tensor<?x?xf32>
      %10 = linalg.fill ins(%cst : f32) outs(%cast_0 : tensor<?x?xf32>) -> tensor<?x?xf32>
      %11 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%8, %9 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%10 : tensor<?x?xf32>) -> tensor<?x?xf32>
      %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%11 : tensor<?x?xf32>) {
      ^bb0(%out: f32):
        %13 = math.cos %out : f32
        linalg.yield %13 : f32
      } -> tensor<?x?xf32>
      flow.dispatch.tensor.store %12, %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After FoldAffineMinInDistributedLoops (iree-codegen-fold-affinemin-in-distributed-loops) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %5 to %c1024 step %6 {
      %7 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<?x?xf32>
      %cast = tensor.cast %7 : tensor<?x?xf32> to tensor<256x128xf32>
      %8 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%c256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %c128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
      %cast_0 = tensor.cast %cast : tensor<256x128xf32> to tensor<?x?xf32>
      %10 = linalg.fill ins(%cst : f32) outs(%cast_0 : tensor<?x?xf32>) -> tensor<?x?xf32>
      %11 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%8, %9 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%10 : tensor<?x?xf32>) -> tensor<?x?xf32>
      %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%11 : tensor<?x?xf32>) {
      ^bb0(%out: f32):
        %13 = math.cos %out : f32
        linalg.yield %13 : f32
      } -> tensor<?x?xf32>
      flow.dispatch.tensor.store %12, %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    scf.for %arg0 = %3 to %c512 step %4 {
      %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
      %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
      scf.for %arg1 = %5 to %c1024 step %6 {
        %7 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
        %8 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
        %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
        %10 = linalg.fill ins(%cst : f32) outs(%7 : tensor<256x128xf32>) -> tensor<256x128xf32>
        %11 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%8, %9 : tensor<256x256xf32>, tensor<256x128xf32>) outs(%10 : tensor<256x128xf32>) -> tensor<256x128xf32>
        %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%11 : tensor<256x128xf32>) {
        ^bb0(%out: f32):
          %13 = math.cos %out : f32
          linalg.yield %13 : f32
        } -> tensor<256x128xf32>
        flow.dispatch.tensor.store %12, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    scf.for %arg0 = %3 to %c512 step %4 {
      %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
      %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
      scf.for %arg1 = %5 to %c1024 step %6 {
        %7 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
        %8 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
        %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
        %10 = linalg.fill ins(%cst : f32) outs(%7 : tensor<256x128xf32>) -> tensor<256x128xf32>
        %11 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%8, %9 : tensor<256x256xf32>, tensor<256x128xf32>) outs(%10 : tensor<256x128xf32>) -> tensor<256x128xf32>
        %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%11 : tensor<256x128xf32>) {
        ^bb0(%out: f32):
          %13 = math.cos %out : f32
          linalg.yield %13 : f32
        } -> tensor<256x128xf32>
        flow.dispatch.tensor.store %12, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After TileAndDecomposeWinogradTransform (iree-linalg-ext-tile-and-decompose-winograd) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %5 to %c1024 step %6 {
      %7 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %8 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = linalg.fill ins(%cst : f32) outs(%7 : tensor<256x128xf32>) -> tensor<256x128xf32>
      %11 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%8, %9 : tensor<256x256xf32>, tensor<256x128xf32>) outs(%10 : tensor<256x128xf32>) -> tensor<256x128xf32>
      %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%11 : tensor<256x128xf32>) {
      ^bb0(%out: f32):
        %13 = math.cos %out : f32
        linalg.yield %13 : f32
      } -> tensor<256x128xf32>
      flow.dispatch.tensor.store %12, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

Yufan:: SCFTileAndFusePattern::matchAndRewrite 
 tiledOps linalg.generic
%15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%extracted_slice : tensor<8x32xf32>) {
^bb0(%out: f32):
  %16 = math.cos %out : f32
  linalg.yield %16 : f32
} -> tensor<8x32xf32>
Yufan:: ??  
slice operand -->0x2fd1c40
%extracted_slice = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
slice operand -->0x1a89710
%extracted_slice = tensor.extract_slice %8[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
slice operand -->0x2fd6fe0
%extracted_slice_2 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
slice operand -->0x1cbd630
%extracted_slice_3 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
slice operand -->0x1cc0b40
%extracted_slice_3 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
tiledOp linalg.generic
%17 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%16 : tensor<8x32xf32>) {
^bb0(%out: f32):
  %18 = math.cos %out : f32
  linalg.yield %18 : f32
} -> tensor<8x32xf32>
tiledOp linalg.matmul
%16 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_2 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%15 : tensor<8x32xf32>) -> tensor<8x32xf32>
tiledOp linalg.fill
%15 = linalg.fill ins(%cst : f32) outs(%extracted_slice_3 : tensor<8x32xf32>) -> tensor<8x32xf32>
origOp linalg.matmul
%11 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%8, %9 : tensor<256x256xf32>, tensor<256x128xf32>) outs(%10 : tensor<256x128xf32>) -> tensor<256x128xf32>
origOp linalg.generic
%12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%11 : tensor<256x128xf32>) {
^bb0(%out: f32):
  %14 = math.cos %out : f32
  linalg.yield %14 : f32
} -> tensor<256x128xf32>
origOp linalg.fill
%10 = linalg.fill ins(%cst : f32) outs(%7 : tensor<256x128xf32>) -> tensor<256x128xf32>
// -----// IR Dump After LinalgStrategyTileAndFusePass (iree-linalg-strategy-tile-and-fuse-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %5 to %c1024 step %6 {
      %7 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %8 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %7) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %8[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill {__internal_linalg_transform__ = "1"} ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) attrs =  {__internal_linalg_transform__ = "1"} {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill {__internal_linalg_transform__ = "1"} ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) attrs =  {__internal_linalg_transform__ = "1"} {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill {__internal_linalg_transform__ = "1"} ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) attrs =  {__internal_linalg_transform__ = "1"} {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgFuse (linalg-fuse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyPadPass (iree-linalg-strategy-pad-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill {__internal_linalg_transform__ = "1"} ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill {__internal_linalg_transform__ = "1"} ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill {__internal_linalg_transform__ = "1"} ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgFuse (linalg-fuse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyPadPass (iree-linalg-strategy-pad-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgFuse (linalg-fuse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyPadPass (iree-linalg-strategy-pad-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) attrs =  {__internal_linalg_transform__ = "1"} {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) attrs =  {__internal_linalg_transform__ = "1"} {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) attrs =  {__internal_linalg_transform__ = "1"} {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgFuse (linalg-fuse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%12 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyTilePass (iree-linalg-strategy-tile-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill {__internal_linalg_transform__ = "1"} ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %15 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %15 : tensor<8x32xf32>
          }
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) attrs =  {__internal_linalg_transform__ = "1"} {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill {__internal_linalg_transform__ = "1"} ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %15 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %15 : tensor<8x32xf32>
          }
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) attrs =  {__internal_linalg_transform__ = "1"} {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill {__internal_linalg_transform__ = "1"} ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %15 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %15 : tensor<8x32xf32>
          }
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) attrs =  {__internal_linalg_transform__ = "1"} {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %15 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %15 : tensor<8x32xf32>
          }
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %15 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %15 : tensor<8x32xf32>
          }
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %15 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %15 : tensor<8x32xf32>
          }
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgSingleTilingExpert (linalg-single-tiling-expert-driver) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %15 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %15 : tensor<8x32xf32>
          }
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %15 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %15 : tensor<8x32xf32>
          }
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %15 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %15 : tensor<8x32xf32>
          }
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyPadPass (iree-linalg-strategy-pad-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %15 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %15 : tensor<8x32xf32>
          }
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %15 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %15 : tensor<8x32xf32>
          }
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %15 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %15 : tensor<8x32xf32>
          }
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %15 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %15 : tensor<8x32xf32>
          }
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %15 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %15 : tensor<8x32xf32>
          }
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %15 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %15 : tensor<8x32xf32>
          }
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgFuse (linalg-fuse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %15 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %15 : tensor<8x32xf32>
          }
          %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
          ^bb0(%out: f32):
            %15 = math.cos %out : f32
            linalg.yield %15 : f32
          } -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
module {
  func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg0 = %3 to %c512 step %4 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
      scf.for %arg1 = %5 to %c1024 step %6 {
        %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
        %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
        %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
          %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
            %extracted_slice_0 = tensor.extract_slice %9[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
            %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
            %12 = linalg.fill ins(%cst : f32) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
            %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (tensor<8x32xf32>) {
              %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
              %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
              %15 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
              scf.yield %15 : tensor<8x32xf32>
            }
            %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} outs(%13 : tensor<8x32xf32>) {
            ^bb0(%out: f32):
              %15 = math.cos %out : f32
              linalg.yield %15 : f32
            } -> tensor<8x32xf32>
            %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
            scf.yield %inserted_slice : tensor<256x128xf32>
          }
          scf.yield %11 : tensor<256x128xf32>
        }
        flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After LinalgStrategyVectorizePass (iree-linalg-strategy-vectorize-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = vector.transfer_write %cst, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (tensor<8x32xf32>) {
            %17 = vector.transfer_read %7[%arg2, %arg6], %cst_0 {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
            %18 = vector.transfer_read %9[%arg6, %arg4], %cst_0 {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
            %19 = vector.transfer_read %arg7[%c0, %c0], %cst_0 {in_bounds = [true, true]} : tensor<8x32xf32>, vector<8x32xf32>
            %20 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %17, %18, %19 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            %21 = vector.transfer_write %20, %arg7[%c0, %c0] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<8x32xf32>
            scf.yield %21 : tensor<8x32xf32>
          }
          %14 = vector.transfer_read %13[%c0, %c0], %cst_0 {in_bounds = [true, true]} : tensor<8x32xf32>, vector<8x32xf32>
          %15 = math.cos %14 : vector<8x32xf32>
          %16 = vector.transfer_write %15, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<256x128xf32>
          scf.yield %16 : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %cst) -> (vector<8x32xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst_0 {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
            %16 = vector.transfer_read %9[%arg6, %arg4], %cst_0 {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %17 : vector<8x32xf32>
          }
          %13 = math.cos %12 : vector<8x32xf32>
          %14 = vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<256x128xf32>
          scf.yield %14 : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %cst) -> (vector<8x32xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst_0 {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
            %16 = vector.transfer_read %9[%arg6, %arg4], %cst_0 {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %17 : vector<8x32xf32>
          }
          %13 = math.cos %12 : vector<8x32xf32>
          %14 = vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<256x128xf32>
          scf.yield %14 : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %cst) -> (vector<8x32xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst_0 {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
            %16 = vector.transfer_read %9[%arg6, %arg4], %cst_0 {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %17 : vector<8x32xf32>
          }
          %13 = math.cos %12 : vector<8x32xf32>
          %14 = vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<256x128xf32>
          scf.yield %14 : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %cst) -> (vector<8x32xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst_0 {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
            %16 = vector.transfer_read %9[%arg6, %arg4], %cst_0 {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %17 : vector<8x32xf32>
          }
          %13 = math.cos %12 : vector<8x32xf32>
          %14 = vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<256x128xf32>
          scf.yield %14 : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %cst) -> (vector<8x32xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst_0 {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
            %16 = vector.transfer_read %9[%arg6, %arg4], %cst_0 {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %17 : vector<8x32xf32>
          }
          %13 = math.cos %12 : vector<8x32xf32>
          %14 = vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<256x128xf32>
          scf.yield %14 : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgSingleTilingExpert (linalg-single-tiling-expert-driver) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %cst) -> (vector<8x32xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst_0 {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
            %16 = vector.transfer_read %9[%arg6, %arg4], %cst_0 {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %17 : vector<8x32xf32>
          }
          %13 = math.cos %12 : vector<8x32xf32>
          %14 = vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<256x128xf32>
          scf.yield %14 : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %cst) -> (vector<8x32xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst_0 {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
            %16 = vector.transfer_read %9[%arg6, %arg4], %cst_0 {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %17 : vector<8x32xf32>
          }
          %13 = math.cos %12 : vector<8x32xf32>
          %14 = vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<256x128xf32>
          scf.yield %14 : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %cst) -> (vector<8x32xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst_0 {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
            %16 = vector.transfer_read %9[%arg6, %arg4], %cst_0 {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %17 : vector<8x32xf32>
          }
          %13 = math.cos %12 : vector<8x32xf32>
          %14 = vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<256x128xf32>
          scf.yield %14 : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After EliminateEmptyTensors (iree-eliminate-empty-tensors) //----- //
module {
  func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
    %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg0 = %3 to %c512 step %4 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
      scf.for %arg1 = %5 to %c1024 step %6 {
        %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
        %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
        %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
          %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
            %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %cst) -> (vector<8x32xf32>) {
              %15 = vector.transfer_read %7[%arg2, %arg6], %cst_0 {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
              %16 = vector.transfer_read %9[%arg6, %arg4], %cst_0 {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
              %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
              scf.yield %17 : vector<8x32xf32>
            }
            %13 = math.cos %12 : vector<8x32xf32>
            %14 = vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<256x128xf32>
            scf.yield %14 : tensor<256x128xf32>
          }
          scf.yield %11 : tensor<256x128xf32>
        }
        flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After EmptyTensorToAllocTensor (empty-tensor-to-alloc-tensor) //----- //
module {
  func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
    %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg0 = %3 to %c512 step %4 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
      scf.for %arg1 = %5 to %c1024 step %6 {
        %8 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>> -> tensor<256x128xf32>
        %9 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
        %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %8) -> (tensor<256x128xf32>) {
          %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
            %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %cst) -> (vector<8x32xf32>) {
              %15 = vector.transfer_read %7[%arg2, %arg6], %cst_0 {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
              %16 = vector.transfer_read %9[%arg6, %arg4], %cst_0 {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
              %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
              scf.yield %17 : vector<8x32xf32>
            }
            %13 = math.cos %12 : vector<8x32xf32>
            %14 = vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<256x128xf32>
            scf.yield %14 : tensor<256x128xf32>
          }
          scf.yield %11 : tensor<256x128xf32>
        }
        flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After IREEComprehensiveBufferize (iree-codegen-iree-comprehensive-bufferize) //----- //
module {
  func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
    %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %6 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %7 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %9 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg0 = %6 to %c512 step %7 {
      %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.for %arg1 = %8 to %c1024 step %9 {
        %subview_1 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_2 = memref.subview %2[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %subview_1) -> (memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
          %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
            %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %cst) -> (vector<8x32xf32>) {
              %14 = vector.transfer_read %subview[%arg2, %arg6], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<8x16xf32>
              %15 = vector.transfer_read %subview_2[%arg6, %arg4], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<16x32xf32>
              %16 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %14, %15, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
              scf.yield %16 : vector<8x32xf32>
            }
            %13 = math.cos %12 : vector<8x32xf32>
            vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
            scf.yield %arg5 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
          }
          scf.yield %11 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        }
        %subview_3 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%10 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_3 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        }
      }
    }
    return
  }
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
module {
  func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
    %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %6 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %7 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %9 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg0 = %6 to %c512 step %7 {
      %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.for %arg1 = %8 to %c1024 step %9 {
        %subview_1 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_2 = memref.subview %2[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %subview_1) -> (memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
          %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
            %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %cst) -> (vector<8x32xf32>) {
              %14 = vector.transfer_read %subview[%arg2, %arg6], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<8x16xf32>
              %15 = vector.transfer_read %subview_2[%arg6, %arg4], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<16x32xf32>
              %16 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %14, %15, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
              scf.yield %16 : vector<8x32xf32>
            }
            %13 = math.cos %12 : vector<8x32xf32>
            vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
            scf.yield %arg5 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
          }
          scf.yield %11 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        }
        %subview_3 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%10 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_3 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        }
      }
    }
    return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %6 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %7 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %9 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %6 to %c512 step %7 {
    %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.for %arg1 = %8 to %c1024 step %9 {
      %subview_1 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_2 = memref.subview %2[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.for %arg2 = %c0 to %c256 step %c8 {
        scf.for %arg3 = %c0 to %c128 step %c32 {
          %10 = scf.for %arg4 = %c0 to %c256 step %c16 iter_args(%arg5 = %cst) -> (vector<8x32xf32>) {
            %12 = vector.transfer_read %subview[%arg2, %arg4], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<8x16xf32>
            %13 = vector.transfer_read %subview_2[%arg4, %arg3], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<16x32xf32>
            %14 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %12, %13, %arg5 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %14 : vector<8x32xf32>
          }
          %11 = math.cos %10 : vector<8x32xf32>
          vector.transfer_write %11, %subview_1[%arg2, %arg3] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        }
      }
      %subview_3 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_1 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_3 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %6 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %7 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %9 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %6 to %c512 step %7 {
    %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.for %arg1 = %8 to %c1024 step %9 {
      %subview_1 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_2 = memref.subview %2[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.for %arg2 = %c0 to %c256 step %c8 {
        scf.for %arg3 = %c0 to %c128 step %c32 {
          %10 = scf.for %arg4 = %c0 to %c256 step %c16 iter_args(%arg5 = %cst) -> (vector<8x32xf32>) {
            %12 = vector.transfer_read %subview[%arg2, %arg4], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<8x16xf32>
            %13 = vector.transfer_read %subview_2[%arg4, %arg3], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<16x32xf32>
            %14 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %12, %13, %arg5 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %14 : vector<8x32xf32>
          }
          %11 = math.cos %10 : vector<8x32xf32>
          vector.transfer_write %11, %subview_1[%arg2, %arg3] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        }
      }
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_1 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_1 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %6 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %7 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %9 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %6 to %c512 step %7 {
    %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.for %arg1 = %8 to %c1024 step %9 {
      %subview_1 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_2 = memref.subview %2[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.for %arg2 = %c0 to %c256 step %c8 {
        scf.for %arg3 = %c0 to %c128 step %c32 {
          %10 = scf.for %arg4 = %c0 to %c256 step %c16 iter_args(%arg5 = %cst) -> (vector<8x32xf32>) {
            %12 = vector.transfer_read %subview[%arg2, %arg4], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<8x16xf32>
            %13 = vector.transfer_read %subview_2[%arg4, %arg3], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<16x32xf32>
            %14 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %12, %13, %arg5 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %14 : vector<8x32xf32>
          }
          %11 = math.cos %10 : vector<8x32xf32>
          vector.transfer_write %11, %subview_1[%arg2, %arg3] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        }
      }
    }
  }
  return
}

// -----// IR Dump After CleanupBufferAllocView (iree-codegen-cleanup-buffer-alloc-view) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %subview_1 = memref.subview %2[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_2 = memref.subview %1[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.for %arg2 = %c0 to %c256 step %c8 {
        scf.for %arg3 = %c0 to %c128 step %c32 {
          %7 = scf.for %arg4 = %c0 to %c256 step %c16 iter_args(%arg5 = %cst) -> (vector<8x32xf32>) {
            %9 = vector.transfer_read %subview[%arg2, %arg4], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<8x16xf32>
            %10 = vector.transfer_read %subview_2[%arg4, %arg3], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<16x32xf32>
            %11 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %9, %10, %arg5 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %11 : vector<8x32xf32>
          }
          %8 = math.cos %7 : vector<8x32xf32>
          vector.transfer_write %8, %subview_1[%arg2, %arg3] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        }
      }
    }
  }
  return
}

// -----// IR Dump After EraseHALDescriptorTypeFromMemRef (iree-codegen-erase-hal-descriptor-type-from-memref) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %subview_1 = memref.subview %2[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
      %subview_2 = memref.subview %1[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
      scf.for %arg2 = %c0 to %c256 step %c8 {
        scf.for %arg3 = %c0 to %c128 step %c32 {
          %7 = scf.for %arg4 = %c0 to %c256 step %c16 iter_args(%arg5 = %cst) -> (vector<8x32xf32>) {
            %9 = vector.transfer_read %subview[%arg2, %arg4], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
            %10 = vector.transfer_read %subview_2[%arg4, %arg3], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
            %11 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %9, %10, %arg5 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %11 : vector<8x32xf32>
          }
          %8 = math.cos %7 : vector<8x32xf32>
          vector.transfer_write %8, %subview_1[%arg2, %arg3] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
        }
      }
    }
  }
  return
}

// -----// IR Dump After RemoveSingleIterationLoop (iree-codegen-remove-single-iteration-loop) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %7, %8, %arg3 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
        scf.yield %9 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %cst_0 = arith.constant 0.000000e+00 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst_0 {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_2[%arg2, %arg1], %cst_0 {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.insert %22, %cst [0] : vector<16xf32> into vector<8x16xf32>
        %24 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
        %25 = vector.load %subview[%24, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %26 = vector.insert %25, %23 [1] : vector<16xf32> into vector<8x16xf32>
        %27 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
        %28 = vector.load %subview[%27, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.insert %28, %26 [2] : vector<16xf32> into vector<8x16xf32>
        %30 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
        %31 = vector.load %subview[%30, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %32 = vector.insert %31, %29 [3] : vector<16xf32> into vector<8x16xf32>
        %33 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
        %34 = vector.load %subview[%33, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %35 = vector.insert %34, %32 [4] : vector<16xf32> into vector<8x16xf32>
        %36 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
        %37 = vector.load %subview[%36, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.insert %37, %35 [5] : vector<16xf32> into vector<8x16xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
        %40 = vector.load %subview[%39, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %41 = vector.insert %40, %38 [6] : vector<16xf32> into vector<8x16xf32>
        %42 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
        %43 = vector.load %subview[%42, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.insert %43, %41 [7] : vector<16xf32> into vector<8x16xf32>
        %45 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %46 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %47 = vector.load %subview_2[%46, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %48 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %49 = vector.load %subview_2[%48, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %50 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %51 = vector.load %subview_2[%50, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %52 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %53 = vector.load %subview_2[%52, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %55 = vector.load %subview_2[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %57 = vector.load %subview_2[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %59 = vector.load %subview_2[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %61 = vector.load %subview_2[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %63 = vector.load %subview_2[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %65 = vector.load %subview_2[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %67 = vector.load %subview_2[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %69 = vector.load %subview_2[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %71 = vector.load %subview_2[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %73 = vector.load %subview_2[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %75 = vector.load %subview_2[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = vector.transpose %44, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %77 = vector.extract %76[0] : vector<16x8xf32>
        %78 = vector.outerproduct %77, %45, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %79 = vector.extract %76[1] : vector<16x8xf32>
        %80 = vector.outerproduct %79, %47, %78 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %81 = vector.extract %76[2] : vector<16x8xf32>
        %82 = vector.outerproduct %81, %49, %80 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %83 = vector.extract %76[3] : vector<16x8xf32>
        %84 = vector.outerproduct %83, %51, %82 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %85 = vector.extract %76[4] : vector<16x8xf32>
        %86 = vector.outerproduct %85, %53, %84 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %87 = vector.extract %76[5] : vector<16x8xf32>
        %88 = vector.outerproduct %87, %55, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.extract %76[6] : vector<16x8xf32>
        %90 = vector.outerproduct %89, %57, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.extract %76[7] : vector<16x8xf32>
        %92 = vector.outerproduct %91, %59, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.extract %76[8] : vector<16x8xf32>
        %94 = vector.outerproduct %93, %61, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.extract %76[9] : vector<16x8xf32>
        %96 = vector.outerproduct %95, %63, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.extract %76[10] : vector<16x8xf32>
        %98 = vector.outerproduct %97, %65, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.extract %76[11] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %67, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %76[12] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %69, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %76[13] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %71, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %76[14] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %73, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %76[15] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %75, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %108 : vector<8x32xf32>
      }
      %6 = math.cos %5 : vector<8x32xf32>
      %7 = vector.extract %6[0] : vector<8x32xf32>
      vector.store %7, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %8 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
      %9 = vector.extract %6[1] : vector<8x32xf32>
      vector.store %9, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %10 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
      %11 = vector.extract %6[2] : vector<8x32xf32>
      vector.store %11, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %12 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
      %13 = vector.extract %6[3] : vector<8x32xf32>
      vector.store %13, %subview_1[%12, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %14 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
      %15 = vector.extract %6[4] : vector<8x32xf32>
      vector.store %15, %subview_1[%14, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
      %17 = vector.extract %6[5] : vector<8x32xf32>
      vector.store %17, %subview_1[%16, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
      %19 = vector.extract %6[6] : vector<8x32xf32>
      vector.store %19, %subview_1[%18, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
      %21 = vector.extract %6[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%20, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.insert %22, %cst [0] : vector<16xf32> into vector<8x16xf32>
        %24 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.insert %24, %23 [1] : vector<16xf32> into vector<8x16xf32>
        %26 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.insert %26, %25 [2] : vector<16xf32> into vector<8x16xf32>
        %28 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.insert %28, %27 [3] : vector<16xf32> into vector<8x16xf32>
        %30 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %31 = vector.insert %30, %29 [4] : vector<16xf32> into vector<8x16xf32>
        %32 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %33 = vector.insert %32, %31 [5] : vector<16xf32> into vector<8x16xf32>
        %34 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %35 = vector.insert %34, %33 [6] : vector<16xf32> into vector<8x16xf32>
        %36 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %37 = vector.insert %36, %35 [7] : vector<16xf32> into vector<8x16xf32>
        %38 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %62 = vector.load %subview_2[%61, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %63 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %64 = vector.load %subview_2[%63, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %65 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %66 = vector.load %subview_2[%65, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %67 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %68 = vector.load %subview_2[%67, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %69 = vector.transpose %37, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %70 = vector.extract %69[0] : vector<16x8xf32>
        %71 = vector.outerproduct %70, %38, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %72 = vector.extract %69[1] : vector<16x8xf32>
        %73 = vector.outerproduct %72, %40, %71 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %74 = vector.extract %69[2] : vector<16x8xf32>
        %75 = vector.outerproduct %74, %42, %73 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %76 = vector.extract %69[3] : vector<16x8xf32>
        %77 = vector.outerproduct %76, %44, %75 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %78 = vector.extract %69[4] : vector<16x8xf32>
        %79 = vector.outerproduct %78, %46, %77 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %80 = vector.extract %69[5] : vector<16x8xf32>
        %81 = vector.outerproduct %80, %48, %79 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %82 = vector.extract %69[6] : vector<16x8xf32>
        %83 = vector.outerproduct %82, %50, %81 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %84 = vector.extract %69[7] : vector<16x8xf32>
        %85 = vector.outerproduct %84, %52, %83 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %86 = vector.extract %69[8] : vector<16x8xf32>
        %87 = vector.outerproduct %86, %54, %85 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.extract %69[9] : vector<16x8xf32>
        %89 = vector.outerproduct %88, %56, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.extract %69[10] : vector<16x8xf32>
        %91 = vector.outerproduct %90, %58, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.extract %69[11] : vector<16x8xf32>
        %93 = vector.outerproduct %92, %60, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.extract %69[12] : vector<16x8xf32>
        %95 = vector.outerproduct %94, %62, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.extract %69[13] : vector<16x8xf32>
        %97 = vector.outerproduct %96, %64, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.extract %69[14] : vector<16x8xf32>
        %99 = vector.outerproduct %98, %66, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.extract %69[15] : vector<16x8xf32>
        %101 = vector.outerproduct %100, %68, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %101 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.insert %22, %cst [0] : vector<16xf32> into vector<8x16xf32>
        %24 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.insert %24, %23 [1] : vector<16xf32> into vector<8x16xf32>
        %26 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.insert %26, %25 [2] : vector<16xf32> into vector<8x16xf32>
        %28 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.insert %28, %27 [3] : vector<16xf32> into vector<8x16xf32>
        %30 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %31 = vector.insert %30, %29 [4] : vector<16xf32> into vector<8x16xf32>
        %32 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %33 = vector.insert %32, %31 [5] : vector<16xf32> into vector<8x16xf32>
        %34 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %35 = vector.insert %34, %33 [6] : vector<16xf32> into vector<8x16xf32>
        %36 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %37 = vector.insert %36, %35 [7] : vector<16xf32> into vector<8x16xf32>
        %38 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %62 = vector.load %subview_2[%61, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %63 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %64 = vector.load %subview_2[%63, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %65 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %66 = vector.load %subview_2[%65, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %67 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %68 = vector.load %subview_2[%67, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %69 = vector.transpose %37, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %70 = vector.extract %69[0] : vector<16x8xf32>
        %71 = vector.outerproduct %70, %38, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %72 = vector.extract %69[1] : vector<16x8xf32>
        %73 = vector.outerproduct %72, %40, %71 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %74 = vector.extract %69[2] : vector<16x8xf32>
        %75 = vector.outerproduct %74, %42, %73 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %76 = vector.extract %69[3] : vector<16x8xf32>
        %77 = vector.outerproduct %76, %44, %75 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %78 = vector.extract %69[4] : vector<16x8xf32>
        %79 = vector.outerproduct %78, %46, %77 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %80 = vector.extract %69[5] : vector<16x8xf32>
        %81 = vector.outerproduct %80, %48, %79 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %82 = vector.extract %69[6] : vector<16x8xf32>
        %83 = vector.outerproduct %82, %50, %81 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %84 = vector.extract %69[7] : vector<16x8xf32>
        %85 = vector.outerproduct %84, %52, %83 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %86 = vector.extract %69[8] : vector<16x8xf32>
        %87 = vector.outerproduct %86, %54, %85 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.extract %69[9] : vector<16x8xf32>
        %89 = vector.outerproduct %88, %56, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.extract %69[10] : vector<16x8xf32>
        %91 = vector.outerproduct %90, %58, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.extract %69[11] : vector<16x8xf32>
        %93 = vector.outerproduct %92, %60, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.extract %69[12] : vector<16x8xf32>
        %95 = vector.outerproduct %94, %62, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.extract %69[13] : vector<16x8xf32>
        %97 = vector.outerproduct %96, %64, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.extract %69[14] : vector<16x8xf32>
        %99 = vector.outerproduct %98, %66, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.extract %69[15] : vector<16x8xf32>
        %101 = vector.outerproduct %100, %68, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %101 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.insert %22, %cst [0] : vector<16xf32> into vector<8x16xf32>
        %24 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.insert %24, %23 [1] : vector<16xf32> into vector<8x16xf32>
        %26 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.insert %26, %25 [2] : vector<16xf32> into vector<8x16xf32>
        %28 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.insert %28, %27 [3] : vector<16xf32> into vector<8x16xf32>
        %30 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %31 = vector.insert %30, %29 [4] : vector<16xf32> into vector<8x16xf32>
        %32 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %33 = vector.insert %32, %31 [5] : vector<16xf32> into vector<8x16xf32>
        %34 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %35 = vector.insert %34, %33 [6] : vector<16xf32> into vector<8x16xf32>
        %36 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %37 = vector.insert %36, %35 [7] : vector<16xf32> into vector<8x16xf32>
        %38 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %62 = vector.load %subview_2[%61, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %63 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %64 = vector.load %subview_2[%63, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %65 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %66 = vector.load %subview_2[%65, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %67 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %68 = vector.load %subview_2[%67, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %69 = vector.transpose %37, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %70 = vector.extract %69[0] : vector<16x8xf32>
        %71 = vector.outerproduct %70, %38, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %72 = vector.extract %69[1] : vector<16x8xf32>
        %73 = vector.outerproduct %72, %40, %71 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %74 = vector.extract %69[2] : vector<16x8xf32>
        %75 = vector.outerproduct %74, %42, %73 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %76 = vector.extract %69[3] : vector<16x8xf32>
        %77 = vector.outerproduct %76, %44, %75 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %78 = vector.extract %69[4] : vector<16x8xf32>
        %79 = vector.outerproduct %78, %46, %77 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %80 = vector.extract %69[5] : vector<16x8xf32>
        %81 = vector.outerproduct %80, %48, %79 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %82 = vector.extract %69[6] : vector<16x8xf32>
        %83 = vector.outerproduct %82, %50, %81 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %84 = vector.extract %69[7] : vector<16x8xf32>
        %85 = vector.outerproduct %84, %52, %83 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %86 = vector.extract %69[8] : vector<16x8xf32>
        %87 = vector.outerproduct %86, %54, %85 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.extract %69[9] : vector<16x8xf32>
        %89 = vector.outerproduct %88, %56, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.extract %69[10] : vector<16x8xf32>
        %91 = vector.outerproduct %90, %58, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.extract %69[11] : vector<16x8xf32>
        %93 = vector.outerproduct %92, %60, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.extract %69[12] : vector<16x8xf32>
        %95 = vector.outerproduct %94, %62, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.extract %69[13] : vector<16x8xf32>
        %97 = vector.outerproduct %96, %64, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.extract %69[14] : vector<16x8xf32>
        %99 = vector.outerproduct %98, %66, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.extract %69[15] : vector<16x8xf32>
        %101 = vector.outerproduct %100, %68, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %101 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.insert %22, %cst [0] : vector<16xf32> into vector<8x16xf32>
        %24 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.insert %24, %23 [1] : vector<16xf32> into vector<8x16xf32>
        %26 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.insert %26, %25 [2] : vector<16xf32> into vector<8x16xf32>
        %28 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.insert %28, %27 [3] : vector<16xf32> into vector<8x16xf32>
        %30 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %31 = vector.insert %30, %29 [4] : vector<16xf32> into vector<8x16xf32>
        %32 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %33 = vector.insert %32, %31 [5] : vector<16xf32> into vector<8x16xf32>
        %34 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %35 = vector.insert %34, %33 [6] : vector<16xf32> into vector<8x16xf32>
        %36 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %37 = vector.insert %36, %35 [7] : vector<16xf32> into vector<8x16xf32>
        %38 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %62 = vector.load %subview_2[%61, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %63 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %64 = vector.load %subview_2[%63, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %65 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %66 = vector.load %subview_2[%65, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %67 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %68 = vector.load %subview_2[%67, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %69 = vector.transpose %37, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %70 = vector.extract %69[0] : vector<16x8xf32>
        %71 = vector.outerproduct %70, %38, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %72 = vector.extract %69[1] : vector<16x8xf32>
        %73 = vector.outerproduct %72, %40, %71 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %74 = vector.extract %69[2] : vector<16x8xf32>
        %75 = vector.outerproduct %74, %42, %73 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %76 = vector.extract %69[3] : vector<16x8xf32>
        %77 = vector.outerproduct %76, %44, %75 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %78 = vector.extract %69[4] : vector<16x8xf32>
        %79 = vector.outerproduct %78, %46, %77 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %80 = vector.extract %69[5] : vector<16x8xf32>
        %81 = vector.outerproduct %80, %48, %79 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %82 = vector.extract %69[6] : vector<16x8xf32>
        %83 = vector.outerproduct %82, %50, %81 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %84 = vector.extract %69[7] : vector<16x8xf32>
        %85 = vector.outerproduct %84, %52, %83 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %86 = vector.extract %69[8] : vector<16x8xf32>
        %87 = vector.outerproduct %86, %54, %85 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.extract %69[9] : vector<16x8xf32>
        %89 = vector.outerproduct %88, %56, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.extract %69[10] : vector<16x8xf32>
        %91 = vector.outerproduct %90, %58, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.extract %69[11] : vector<16x8xf32>
        %93 = vector.outerproduct %92, %60, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.extract %69[12] : vector<16x8xf32>
        %95 = vector.outerproduct %94, %62, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.extract %69[13] : vector<16x8xf32>
        %97 = vector.outerproduct %96, %64, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.extract %69[14] : vector<16x8xf32>
        %99 = vector.outerproduct %98, %66, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.extract %69[15] : vector<16x8xf32>
        %101 = vector.outerproduct %100, %68, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %101 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.insert %22, %cst [0] : vector<16xf32> into vector<8x16xf32>
        %24 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.insert %24, %23 [1] : vector<16xf32> into vector<8x16xf32>
        %26 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.insert %26, %25 [2] : vector<16xf32> into vector<8x16xf32>
        %28 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.insert %28, %27 [3] : vector<16xf32> into vector<8x16xf32>
        %30 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %31 = vector.insert %30, %29 [4] : vector<16xf32> into vector<8x16xf32>
        %32 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %33 = vector.insert %32, %31 [5] : vector<16xf32> into vector<8x16xf32>
        %34 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %35 = vector.insert %34, %33 [6] : vector<16xf32> into vector<8x16xf32>
        %36 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %37 = vector.insert %36, %35 [7] : vector<16xf32> into vector<8x16xf32>
        %38 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %62 = vector.load %subview_2[%61, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %63 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %64 = vector.load %subview_2[%63, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %65 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %66 = vector.load %subview_2[%65, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %67 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %68 = vector.load %subview_2[%67, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %69 = vector.transpose %37, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %70 = vector.extract %69[0] : vector<16x8xf32>
        %71 = vector.outerproduct %70, %38, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %72 = vector.extract %69[1] : vector<16x8xf32>
        %73 = vector.outerproduct %72, %40, %71 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %74 = vector.extract %69[2] : vector<16x8xf32>
        %75 = vector.outerproduct %74, %42, %73 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %76 = vector.extract %69[3] : vector<16x8xf32>
        %77 = vector.outerproduct %76, %44, %75 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %78 = vector.extract %69[4] : vector<16x8xf32>
        %79 = vector.outerproduct %78, %46, %77 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %80 = vector.extract %69[5] : vector<16x8xf32>
        %81 = vector.outerproduct %80, %48, %79 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %82 = vector.extract %69[6] : vector<16x8xf32>
        %83 = vector.outerproduct %82, %50, %81 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %84 = vector.extract %69[7] : vector<16x8xf32>
        %85 = vector.outerproduct %84, %52, %83 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %86 = vector.extract %69[8] : vector<16x8xf32>
        %87 = vector.outerproduct %86, %54, %85 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.extract %69[9] : vector<16x8xf32>
        %89 = vector.outerproduct %88, %56, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.extract %69[10] : vector<16x8xf32>
        %91 = vector.outerproduct %90, %58, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.extract %69[11] : vector<16x8xf32>
        %93 = vector.outerproduct %92, %60, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.extract %69[12] : vector<16x8xf32>
        %95 = vector.outerproduct %94, %62, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.extract %69[13] : vector<16x8xf32>
        %97 = vector.outerproduct %96, %64, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.extract %69[14] : vector<16x8xf32>
        %99 = vector.outerproduct %98, %66, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.extract %69[15] : vector<16x8xf32>
        %101 = vector.outerproduct %100, %68, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %101 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.insert %22, %cst [0] : vector<16xf32> into vector<8x16xf32>
        %24 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.insert %24, %23 [1] : vector<16xf32> into vector<8x16xf32>
        %26 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.insert %26, %25 [2] : vector<16xf32> into vector<8x16xf32>
        %28 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.insert %28, %27 [3] : vector<16xf32> into vector<8x16xf32>
        %30 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %31 = vector.insert %30, %29 [4] : vector<16xf32> into vector<8x16xf32>
        %32 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %33 = vector.insert %32, %31 [5] : vector<16xf32> into vector<8x16xf32>
        %34 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %35 = vector.insert %34, %33 [6] : vector<16xf32> into vector<8x16xf32>
        %36 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %37 = vector.insert %36, %35 [7] : vector<16xf32> into vector<8x16xf32>
        %38 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %62 = vector.load %subview_2[%61, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %63 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %64 = vector.load %subview_2[%63, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %65 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %66 = vector.load %subview_2[%65, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %67 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %68 = vector.load %subview_2[%67, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %69 = vector.transpose %37, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %70 = vector.extract %69[0] : vector<16x8xf32>
        %71 = vector.outerproduct %70, %38, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %72 = vector.extract %69[1] : vector<16x8xf32>
        %73 = vector.outerproduct %72, %40, %71 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %74 = vector.extract %69[2] : vector<16x8xf32>
        %75 = vector.outerproduct %74, %42, %73 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %76 = vector.extract %69[3] : vector<16x8xf32>
        %77 = vector.outerproduct %76, %44, %75 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %78 = vector.extract %69[4] : vector<16x8xf32>
        %79 = vector.outerproduct %78, %46, %77 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %80 = vector.extract %69[5] : vector<16x8xf32>
        %81 = vector.outerproduct %80, %48, %79 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %82 = vector.extract %69[6] : vector<16x8xf32>
        %83 = vector.outerproduct %82, %50, %81 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %84 = vector.extract %69[7] : vector<16x8xf32>
        %85 = vector.outerproduct %84, %52, %83 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %86 = vector.extract %69[8] : vector<16x8xf32>
        %87 = vector.outerproduct %86, %54, %85 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.extract %69[9] : vector<16x8xf32>
        %89 = vector.outerproduct %88, %56, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.extract %69[10] : vector<16x8xf32>
        %91 = vector.outerproduct %90, %58, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.extract %69[11] : vector<16x8xf32>
        %93 = vector.outerproduct %92, %60, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.extract %69[12] : vector<16x8xf32>
        %95 = vector.outerproduct %94, %62, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.extract %69[13] : vector<16x8xf32>
        %97 = vector.outerproduct %96, %64, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.extract %69[14] : vector<16x8xf32>
        %99 = vector.outerproduct %98, %66, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.extract %69[15] : vector<16x8xf32>
        %101 = vector.outerproduct %100, %68, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %101 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.insert %22, %cst [0] : vector<16xf32> into vector<8x16xf32>
        %24 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.insert %24, %23 [1] : vector<16xf32> into vector<8x16xf32>
        %26 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.insert %26, %25 [2] : vector<16xf32> into vector<8x16xf32>
        %28 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.insert %28, %27 [3] : vector<16xf32> into vector<8x16xf32>
        %30 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %31 = vector.insert %30, %29 [4] : vector<16xf32> into vector<8x16xf32>
        %32 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %33 = vector.insert %32, %31 [5] : vector<16xf32> into vector<8x16xf32>
        %34 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %35 = vector.insert %34, %33 [6] : vector<16xf32> into vector<8x16xf32>
        %36 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %37 = vector.insert %36, %35 [7] : vector<16xf32> into vector<8x16xf32>
        %38 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %62 = vector.load %subview_2[%61, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %63 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %64 = vector.load %subview_2[%63, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %65 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %66 = vector.load %subview_2[%65, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %67 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %68 = vector.load %subview_2[%67, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %69 = vector.transpose %37, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %70 = vector.extract %69[0] : vector<16x8xf32>
        %71 = vector.outerproduct %70, %38, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %72 = vector.extract %69[1] : vector<16x8xf32>
        %73 = vector.outerproduct %72, %40, %71 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %74 = vector.extract %69[2] : vector<16x8xf32>
        %75 = vector.outerproduct %74, %42, %73 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %76 = vector.extract %69[3] : vector<16x8xf32>
        %77 = vector.outerproduct %76, %44, %75 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %78 = vector.extract %69[4] : vector<16x8xf32>
        %79 = vector.outerproduct %78, %46, %77 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %80 = vector.extract %69[5] : vector<16x8xf32>
        %81 = vector.outerproduct %80, %48, %79 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %82 = vector.extract %69[6] : vector<16x8xf32>
        %83 = vector.outerproduct %82, %50, %81 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %84 = vector.extract %69[7] : vector<16x8xf32>
        %85 = vector.outerproduct %84, %52, %83 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %86 = vector.extract %69[8] : vector<16x8xf32>
        %87 = vector.outerproduct %86, %54, %85 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.extract %69[9] : vector<16x8xf32>
        %89 = vector.outerproduct %88, %56, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.extract %69[10] : vector<16x8xf32>
        %91 = vector.outerproduct %90, %58, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.extract %69[11] : vector<16x8xf32>
        %93 = vector.outerproduct %92, %60, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.extract %69[12] : vector<16x8xf32>
        %95 = vector.outerproduct %94, %62, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.extract %69[13] : vector<16x8xf32>
        %97 = vector.outerproduct %96, %64, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.extract %69[14] : vector<16x8xf32>
        %99 = vector.outerproduct %98, %66, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.extract %69[15] : vector<16x8xf32>
        %101 = vector.outerproduct %100, %68, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %101 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.insert %22, %cst [0] : vector<16xf32> into vector<8x16xf32>
        %24 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.insert %24, %23 [1] : vector<16xf32> into vector<8x16xf32>
        %26 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.insert %26, %25 [2] : vector<16xf32> into vector<8x16xf32>
        %28 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.insert %28, %27 [3] : vector<16xf32> into vector<8x16xf32>
        %30 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %31 = vector.insert %30, %29 [4] : vector<16xf32> into vector<8x16xf32>
        %32 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %33 = vector.insert %32, %31 [5] : vector<16xf32> into vector<8x16xf32>
        %34 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %35 = vector.insert %34, %33 [6] : vector<16xf32> into vector<8x16xf32>
        %36 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %37 = vector.insert %36, %35 [7] : vector<16xf32> into vector<8x16xf32>
        %38 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %62 = vector.load %subview_2[%61, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %63 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %64 = vector.load %subview_2[%63, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %65 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %66 = vector.load %subview_2[%65, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %67 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %68 = vector.load %subview_2[%67, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %69 = vector.transpose %37, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %70 = vector.extract %69[0] : vector<16x8xf32>
        %71 = vector.outerproduct %70, %38, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %72 = vector.extract %69[1] : vector<16x8xf32>
        %73 = vector.outerproduct %72, %40, %71 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %74 = vector.extract %69[2] : vector<16x8xf32>
        %75 = vector.outerproduct %74, %42, %73 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %76 = vector.extract %69[3] : vector<16x8xf32>
        %77 = vector.outerproduct %76, %44, %75 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %78 = vector.extract %69[4] : vector<16x8xf32>
        %79 = vector.outerproduct %78, %46, %77 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %80 = vector.extract %69[5] : vector<16x8xf32>
        %81 = vector.outerproduct %80, %48, %79 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %82 = vector.extract %69[6] : vector<16x8xf32>
        %83 = vector.outerproduct %82, %50, %81 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %84 = vector.extract %69[7] : vector<16x8xf32>
        %85 = vector.outerproduct %84, %52, %83 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %86 = vector.extract %69[8] : vector<16x8xf32>
        %87 = vector.outerproduct %86, %54, %85 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.extract %69[9] : vector<16x8xf32>
        %89 = vector.outerproduct %88, %56, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.extract %69[10] : vector<16x8xf32>
        %91 = vector.outerproduct %90, %58, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.extract %69[11] : vector<16x8xf32>
        %93 = vector.outerproduct %92, %60, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.extract %69[12] : vector<16x8xf32>
        %95 = vector.outerproduct %94, %62, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.extract %69[13] : vector<16x8xf32>
        %97 = vector.outerproduct %96, %64, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.extract %69[14] : vector<16x8xf32>
        %99 = vector.outerproduct %98, %66, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.extract %69[15] : vector<16x8xf32>
        %101 = vector.outerproduct %100, %68, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %101 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.insert %22, %cst [0] : vector<16xf32> into vector<8x16xf32>
        %24 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.insert %24, %23 [1] : vector<16xf32> into vector<8x16xf32>
        %26 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.insert %26, %25 [2] : vector<16xf32> into vector<8x16xf32>
        %28 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.insert %28, %27 [3] : vector<16xf32> into vector<8x16xf32>
        %30 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %31 = vector.insert %30, %29 [4] : vector<16xf32> into vector<8x16xf32>
        %32 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %33 = vector.insert %32, %31 [5] : vector<16xf32> into vector<8x16xf32>
        %34 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %35 = vector.insert %34, %33 [6] : vector<16xf32> into vector<8x16xf32>
        %36 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %37 = vector.insert %36, %35 [7] : vector<16xf32> into vector<8x16xf32>
        %38 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %62 = vector.load %subview_2[%61, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %63 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %64 = vector.load %subview_2[%63, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %65 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %66 = vector.load %subview_2[%65, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %67 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %68 = vector.load %subview_2[%67, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %69 = vector.shape_cast %37 : vector<8x16xf32> to vector<128xf32>
        %70 = vector.shuffle %69, %69 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %71 = vector.shape_cast %70 : vector<128xf32> to vector<16x8xf32>
        %72 = vector.extract %71[0] : vector<16x8xf32>
        %73 = vector.outerproduct %72, %38, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %74 = vector.extract %71[1] : vector<16x8xf32>
        %75 = vector.outerproduct %74, %40, %73 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %76 = vector.extract %71[2] : vector<16x8xf32>
        %77 = vector.outerproduct %76, %42, %75 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %78 = vector.extract %71[3] : vector<16x8xf32>
        %79 = vector.outerproduct %78, %44, %77 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %80 = vector.extract %71[4] : vector<16x8xf32>
        %81 = vector.outerproduct %80, %46, %79 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %82 = vector.extract %71[5] : vector<16x8xf32>
        %83 = vector.outerproduct %82, %48, %81 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %84 = vector.extract %71[6] : vector<16x8xf32>
        %85 = vector.outerproduct %84, %50, %83 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %86 = vector.extract %71[7] : vector<16x8xf32>
        %87 = vector.outerproduct %86, %52, %85 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.extract %71[8] : vector<16x8xf32>
        %89 = vector.outerproduct %88, %54, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.extract %71[9] : vector<16x8xf32>
        %91 = vector.outerproduct %90, %56, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.extract %71[10] : vector<16x8xf32>
        %93 = vector.outerproduct %92, %58, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.extract %71[11] : vector<16x8xf32>
        %95 = vector.outerproduct %94, %60, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.extract %71[12] : vector<16x8xf32>
        %97 = vector.outerproduct %96, %62, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.extract %71[13] : vector<16x8xf32>
        %99 = vector.outerproduct %98, %64, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.extract %71[14] : vector<16x8xf32>
        %101 = vector.outerproduct %100, %66, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %102 = vector.extract %71[15] : vector<16x8xf32>
        %103 = vector.outerproduct %102, %68, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %103 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.insert %22, %cst [0] : vector<16xf32> into vector<8x16xf32>
        %24 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.insert %24, %23 [1] : vector<16xf32> into vector<8x16xf32>
        %26 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.insert %26, %25 [2] : vector<16xf32> into vector<8x16xf32>
        %28 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.insert %28, %27 [3] : vector<16xf32> into vector<8x16xf32>
        %30 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %31 = vector.insert %30, %29 [4] : vector<16xf32> into vector<8x16xf32>
        %32 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %33 = vector.insert %32, %31 [5] : vector<16xf32> into vector<8x16xf32>
        %34 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %35 = vector.insert %34, %33 [6] : vector<16xf32> into vector<8x16xf32>
        %36 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %37 = vector.insert %36, %35 [7] : vector<16xf32> into vector<8x16xf32>
        %38 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %62 = vector.load %subview_2[%61, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %63 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %64 = vector.load %subview_2[%63, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %65 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %66 = vector.load %subview_2[%65, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %67 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %68 = vector.load %subview_2[%67, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %69 = vector.shape_cast %37 : vector<8x16xf32> to vector<128xf32>
        %70 = vector.shuffle %69, %69 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %71 = vector.shape_cast %70 : vector<128xf32> to vector<16x8xf32>
        %72 = vector.extract %71[0] : vector<16x8xf32>
        %73 = vector.outerproduct %72, %38, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %74 = vector.extract %71[1] : vector<16x8xf32>
        %75 = vector.outerproduct %74, %40, %73 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %76 = vector.extract %71[2] : vector<16x8xf32>
        %77 = vector.outerproduct %76, %42, %75 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %78 = vector.extract %71[3] : vector<16x8xf32>
        %79 = vector.outerproduct %78, %44, %77 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %80 = vector.extract %71[4] : vector<16x8xf32>
        %81 = vector.outerproduct %80, %46, %79 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %82 = vector.extract %71[5] : vector<16x8xf32>
        %83 = vector.outerproduct %82, %48, %81 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %84 = vector.extract %71[6] : vector<16x8xf32>
        %85 = vector.outerproduct %84, %50, %83 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %86 = vector.extract %71[7] : vector<16x8xf32>
        %87 = vector.outerproduct %86, %52, %85 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.extract %71[8] : vector<16x8xf32>
        %89 = vector.outerproduct %88, %54, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.extract %71[9] : vector<16x8xf32>
        %91 = vector.outerproduct %90, %56, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.extract %71[10] : vector<16x8xf32>
        %93 = vector.outerproduct %92, %58, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.extract %71[11] : vector<16x8xf32>
        %95 = vector.outerproduct %94, %60, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.extract %71[12] : vector<16x8xf32>
        %97 = vector.outerproduct %96, %62, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.extract %71[13] : vector<16x8xf32>
        %99 = vector.outerproduct %98, %64, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.extract %71[14] : vector<16x8xf32>
        %101 = vector.outerproduct %100, %66, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %102 = vector.extract %71[15] : vector<16x8xf32>
        %103 = vector.outerproduct %102, %68, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %103 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.insert %22, %cst [0] : vector<16xf32> into vector<8x16xf32>
        %24 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.insert %24, %23 [1] : vector<16xf32> into vector<8x16xf32>
        %26 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.insert %26, %25 [2] : vector<16xf32> into vector<8x16xf32>
        %28 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.insert %28, %27 [3] : vector<16xf32> into vector<8x16xf32>
        %30 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %31 = vector.insert %30, %29 [4] : vector<16xf32> into vector<8x16xf32>
        %32 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %33 = vector.insert %32, %31 [5] : vector<16xf32> into vector<8x16xf32>
        %34 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %35 = vector.insert %34, %33 [6] : vector<16xf32> into vector<8x16xf32>
        %36 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %37 = vector.insert %36, %35 [7] : vector<16xf32> into vector<8x16xf32>
        %38 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %62 = vector.load %subview_2[%61, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %63 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %64 = vector.load %subview_2[%63, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %65 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %66 = vector.load %subview_2[%65, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %67 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %68 = vector.load %subview_2[%67, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %69 = vector.shape_cast %37 : vector<8x16xf32> to vector<128xf32>
        %70 = vector.shuffle %69, %69 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %71 = vector.shape_cast %70 : vector<128xf32> to vector<16x8xf32>
        %72 = vector.extract %71[0] : vector<16x8xf32>
        %73 = vector.outerproduct %72, %38, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %74 = vector.extract %71[1] : vector<16x8xf32>
        %75 = vector.outerproduct %74, %40, %73 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %76 = vector.extract %71[2] : vector<16x8xf32>
        %77 = vector.outerproduct %76, %42, %75 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %78 = vector.extract %71[3] : vector<16x8xf32>
        %79 = vector.outerproduct %78, %44, %77 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %80 = vector.extract %71[4] : vector<16x8xf32>
        %81 = vector.outerproduct %80, %46, %79 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %82 = vector.extract %71[5] : vector<16x8xf32>
        %83 = vector.outerproduct %82, %48, %81 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %84 = vector.extract %71[6] : vector<16x8xf32>
        %85 = vector.outerproduct %84, %50, %83 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %86 = vector.extract %71[7] : vector<16x8xf32>
        %87 = vector.outerproduct %86, %52, %85 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.extract %71[8] : vector<16x8xf32>
        %89 = vector.outerproduct %88, %54, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.extract %71[9] : vector<16x8xf32>
        %91 = vector.outerproduct %90, %56, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.extract %71[10] : vector<16x8xf32>
        %93 = vector.outerproduct %92, %58, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.extract %71[11] : vector<16x8xf32>
        %95 = vector.outerproduct %94, %60, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.extract %71[12] : vector<16x8xf32>
        %97 = vector.outerproduct %96, %62, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.extract %71[13] : vector<16x8xf32>
        %99 = vector.outerproduct %98, %64, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.extract %71[14] : vector<16x8xf32>
        %101 = vector.outerproduct %100, %66, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %102 = vector.extract %71[15] : vector<16x8xf32>
        %103 = vector.outerproduct %102, %68, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %103 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.insert %22, %cst [0] : vector<16xf32> into vector<8x16xf32>
        %24 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.insert %24, %23 [1] : vector<16xf32> into vector<8x16xf32>
        %26 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.insert %26, %25 [2] : vector<16xf32> into vector<8x16xf32>
        %28 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.insert %28, %27 [3] : vector<16xf32> into vector<8x16xf32>
        %30 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %31 = vector.insert %30, %29 [4] : vector<16xf32> into vector<8x16xf32>
        %32 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %33 = vector.insert %32, %31 [5] : vector<16xf32> into vector<8x16xf32>
        %34 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %35 = vector.insert %34, %33 [6] : vector<16xf32> into vector<8x16xf32>
        %36 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %37 = vector.insert %36, %35 [7] : vector<16xf32> into vector<8x16xf32>
        %38 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %62 = vector.load %subview_2[%61, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %63 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %64 = vector.load %subview_2[%63, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %65 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %66 = vector.load %subview_2[%65, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %67 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %68 = vector.load %subview_2[%67, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %69 = vector.shape_cast %37 : vector<8x16xf32> to vector<128xf32>
        %70 = vector.shuffle %69, %69 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %71 = vector.shape_cast %70 : vector<128xf32> to vector<16x8xf32>
        %72 = vector.extract %71[0] : vector<16x8xf32>
        %73 = vector.outerproduct %72, %38, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %74 = vector.extract %71[1] : vector<16x8xf32>
        %75 = vector.outerproduct %74, %40, %73 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %76 = vector.extract %71[2] : vector<16x8xf32>
        %77 = vector.outerproduct %76, %42, %75 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %78 = vector.extract %71[3] : vector<16x8xf32>
        %79 = vector.outerproduct %78, %44, %77 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %80 = vector.extract %71[4] : vector<16x8xf32>
        %81 = vector.outerproduct %80, %46, %79 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %82 = vector.extract %71[5] : vector<16x8xf32>
        %83 = vector.outerproduct %82, %48, %81 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %84 = vector.extract %71[6] : vector<16x8xf32>
        %85 = vector.outerproduct %84, %50, %83 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %86 = vector.extract %71[7] : vector<16x8xf32>
        %87 = vector.outerproduct %86, %52, %85 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.extract %71[8] : vector<16x8xf32>
        %89 = vector.outerproduct %88, %54, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.extract %71[9] : vector<16x8xf32>
        %91 = vector.outerproduct %90, %56, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.extract %71[10] : vector<16x8xf32>
        %93 = vector.outerproduct %92, %58, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.extract %71[11] : vector<16x8xf32>
        %95 = vector.outerproduct %94, %60, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.extract %71[12] : vector<16x8xf32>
        %97 = vector.outerproduct %96, %62, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.extract %71[13] : vector<16x8xf32>
        %99 = vector.outerproduct %98, %64, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.extract %71[14] : vector<16x8xf32>
        %101 = vector.outerproduct %100, %66, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %102 = vector.extract %71[15] : vector<16x8xf32>
        %103 = vector.outerproduct %102, %68, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %103 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.insert %22, %cst [0] : vector<16xf32> into vector<8x16xf32>
        %24 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.insert %24, %23 [1] : vector<16xf32> into vector<8x16xf32>
        %26 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.insert %26, %25 [2] : vector<16xf32> into vector<8x16xf32>
        %28 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.insert %28, %27 [3] : vector<16xf32> into vector<8x16xf32>
        %30 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %31 = vector.insert %30, %29 [4] : vector<16xf32> into vector<8x16xf32>
        %32 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %33 = vector.insert %32, %31 [5] : vector<16xf32> into vector<8x16xf32>
        %34 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %35 = vector.insert %34, %33 [6] : vector<16xf32> into vector<8x16xf32>
        %36 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %37 = vector.insert %36, %35 [7] : vector<16xf32> into vector<8x16xf32>
        %38 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %62 = vector.load %subview_2[%61, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %63 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %64 = vector.load %subview_2[%63, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %65 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %66 = vector.load %subview_2[%65, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %67 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %68 = vector.load %subview_2[%67, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %69 = vector.shape_cast %37 : vector<8x16xf32> to vector<128xf32>
        %70 = vector.shuffle %69, %69 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %71 = vector.shape_cast %70 : vector<128xf32> to vector<16x8xf32>
        %72 = vector.extract %71[0] : vector<16x8xf32>
        %73 = vector.outerproduct %72, %38, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %74 = vector.extract %71[1] : vector<16x8xf32>
        %75 = vector.outerproduct %74, %40, %73 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %76 = vector.extract %71[2] : vector<16x8xf32>
        %77 = vector.outerproduct %76, %42, %75 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %78 = vector.extract %71[3] : vector<16x8xf32>
        %79 = vector.outerproduct %78, %44, %77 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %80 = vector.extract %71[4] : vector<16x8xf32>
        %81 = vector.outerproduct %80, %46, %79 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %82 = vector.extract %71[5] : vector<16x8xf32>
        %83 = vector.outerproduct %82, %48, %81 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %84 = vector.extract %71[6] : vector<16x8xf32>
        %85 = vector.outerproduct %84, %50, %83 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %86 = vector.extract %71[7] : vector<16x8xf32>
        %87 = vector.outerproduct %86, %52, %85 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.extract %71[8] : vector<16x8xf32>
        %89 = vector.outerproduct %88, %54, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.extract %71[9] : vector<16x8xf32>
        %91 = vector.outerproduct %90, %56, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.extract %71[10] : vector<16x8xf32>
        %93 = vector.outerproduct %92, %58, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.extract %71[11] : vector<16x8xf32>
        %95 = vector.outerproduct %94, %60, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.extract %71[12] : vector<16x8xf32>
        %97 = vector.outerproduct %96, %62, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.extract %71[13] : vector<16x8xf32>
        %99 = vector.outerproduct %98, %64, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.extract %71[14] : vector<16x8xf32>
        %101 = vector.outerproduct %100, %66, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %102 = vector.extract %71[15] : vector<16x8xf32>
        %103 = vector.outerproduct %102, %68, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %103 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.insert %22, %cst [0] : vector<16xf32> into vector<8x16xf32>
        %24 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.insert %24, %23 [1] : vector<16xf32> into vector<8x16xf32>
        %26 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.insert %26, %25 [2] : vector<16xf32> into vector<8x16xf32>
        %28 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.insert %28, %27 [3] : vector<16xf32> into vector<8x16xf32>
        %30 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %31 = vector.insert %30, %29 [4] : vector<16xf32> into vector<8x16xf32>
        %32 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %33 = vector.insert %32, %31 [5] : vector<16xf32> into vector<8x16xf32>
        %34 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %35 = vector.insert %34, %33 [6] : vector<16xf32> into vector<8x16xf32>
        %36 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %37 = vector.insert %36, %35 [7] : vector<16xf32> into vector<8x16xf32>
        %38 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %62 = vector.load %subview_2[%61, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %63 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %64 = vector.load %subview_2[%63, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %65 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %66 = vector.load %subview_2[%65, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %67 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %68 = vector.load %subview_2[%67, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %69 = vector.shape_cast %37 : vector<8x16xf32> to vector<128xf32>
        %70 = vector.shuffle %69, %69 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %71 = vector.shape_cast %70 : vector<128xf32> to vector<16x8xf32>
        %72 = vector.extract %71[0] : vector<16x8xf32>
        %73 = vector.outerproduct %72, %38, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %74 = vector.extract %71[1] : vector<16x8xf32>
        %75 = vector.outerproduct %74, %40, %73 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %76 = vector.extract %71[2] : vector<16x8xf32>
        %77 = vector.outerproduct %76, %42, %75 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %78 = vector.extract %71[3] : vector<16x8xf32>
        %79 = vector.outerproduct %78, %44, %77 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %80 = vector.extract %71[4] : vector<16x8xf32>
        %81 = vector.outerproduct %80, %46, %79 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %82 = vector.extract %71[5] : vector<16x8xf32>
        %83 = vector.outerproduct %82, %48, %81 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %84 = vector.extract %71[6] : vector<16x8xf32>
        %85 = vector.outerproduct %84, %50, %83 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %86 = vector.extract %71[7] : vector<16x8xf32>
        %87 = vector.outerproduct %86, %52, %85 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.extract %71[8] : vector<16x8xf32>
        %89 = vector.outerproduct %88, %54, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.extract %71[9] : vector<16x8xf32>
        %91 = vector.outerproduct %90, %56, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.extract %71[10] : vector<16x8xf32>
        %93 = vector.outerproduct %92, %58, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.extract %71[11] : vector<16x8xf32>
        %95 = vector.outerproduct %94, %60, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.extract %71[12] : vector<16x8xf32>
        %97 = vector.outerproduct %96, %62, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.extract %71[13] : vector<16x8xf32>
        %99 = vector.outerproduct %98, %64, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.extract %71[14] : vector<16x8xf32>
        %101 = vector.outerproduct %100, %66, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %102 = vector.extract %71[15] : vector<16x8xf32>
        %103 = vector.outerproduct %102, %68, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %103 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.insert %22, %cst [0] : vector<16xf32> into vector<8x16xf32>
        %24 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.insert %24, %23 [1] : vector<16xf32> into vector<8x16xf32>
        %26 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.insert %26, %25 [2] : vector<16xf32> into vector<8x16xf32>
        %28 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.insert %28, %27 [3] : vector<16xf32> into vector<8x16xf32>
        %30 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %31 = vector.insert %30, %29 [4] : vector<16xf32> into vector<8x16xf32>
        %32 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %33 = vector.insert %32, %31 [5] : vector<16xf32> into vector<8x16xf32>
        %34 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %35 = vector.insert %34, %33 [6] : vector<16xf32> into vector<8x16xf32>
        %36 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %37 = vector.insert %36, %35 [7] : vector<16xf32> into vector<8x16xf32>
        %38 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %62 = vector.load %subview_2[%61, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %63 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %64 = vector.load %subview_2[%63, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %65 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %66 = vector.load %subview_2[%65, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %67 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %68 = vector.load %subview_2[%67, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %69 = vector.shape_cast %37 : vector<8x16xf32> to vector<128xf32>
        %70 = vector.shuffle %69, %69 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %71 = vector.shape_cast %70 : vector<128xf32> to vector<16x8xf32>
        %72 = vector.extract %71[0] : vector<16x8xf32>
        %73 = vector.outerproduct %72, %38, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %74 = vector.extract %71[1] : vector<16x8xf32>
        %75 = vector.outerproduct %74, %40, %73 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %76 = vector.extract %71[2] : vector<16x8xf32>
        %77 = vector.outerproduct %76, %42, %75 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %78 = vector.extract %71[3] : vector<16x8xf32>
        %79 = vector.outerproduct %78, %44, %77 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %80 = vector.extract %71[4] : vector<16x8xf32>
        %81 = vector.outerproduct %80, %46, %79 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %82 = vector.extract %71[5] : vector<16x8xf32>
        %83 = vector.outerproduct %82, %48, %81 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %84 = vector.extract %71[6] : vector<16x8xf32>
        %85 = vector.outerproduct %84, %50, %83 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %86 = vector.extract %71[7] : vector<16x8xf32>
        %87 = vector.outerproduct %86, %52, %85 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.extract %71[8] : vector<16x8xf32>
        %89 = vector.outerproduct %88, %54, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.extract %71[9] : vector<16x8xf32>
        %91 = vector.outerproduct %90, %56, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.extract %71[10] : vector<16x8xf32>
        %93 = vector.outerproduct %92, %58, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.extract %71[11] : vector<16x8xf32>
        %95 = vector.outerproduct %94, %60, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.extract %71[12] : vector<16x8xf32>
        %97 = vector.outerproduct %96, %62, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.extract %71[13] : vector<16x8xf32>
        %99 = vector.outerproduct %98, %64, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.extract %71[14] : vector<16x8xf32>
        %101 = vector.outerproduct %100, %66, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %102 = vector.extract %71[15] : vector<16x8xf32>
        %103 = vector.outerproduct %102, %68, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %103 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.insert %22, %cst [0] : vector<16xf32> into vector<8x16xf32>
        %24 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.insert %24, %23 [1] : vector<16xf32> into vector<8x16xf32>
        %26 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.insert %26, %25 [2] : vector<16xf32> into vector<8x16xf32>
        %28 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.insert %28, %27 [3] : vector<16xf32> into vector<8x16xf32>
        %30 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %31 = vector.insert %30, %29 [4] : vector<16xf32> into vector<8x16xf32>
        %32 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %33 = vector.insert %32, %31 [5] : vector<16xf32> into vector<8x16xf32>
        %34 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %35 = vector.insert %34, %33 [6] : vector<16xf32> into vector<8x16xf32>
        %36 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %37 = vector.insert %36, %35 [7] : vector<16xf32> into vector<8x16xf32>
        %38 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %62 = vector.load %subview_2[%61, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %63 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %64 = vector.load %subview_2[%63, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %65 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %66 = vector.load %subview_2[%65, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %67 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %68 = vector.load %subview_2[%67, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %69 = vector.shape_cast %37 : vector<8x16xf32> to vector<128xf32>
        %70 = vector.shuffle %69, %69 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %71 = vector.shape_cast %70 : vector<128xf32> to vector<16x8xf32>
        %72 = vector.extract %71[0] : vector<16x8xf32>
        %73 = vector.outerproduct %72, %38, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %74 = vector.extract %71[1] : vector<16x8xf32>
        %75 = vector.outerproduct %74, %40, %73 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %76 = vector.extract %71[2] : vector<16x8xf32>
        %77 = vector.outerproduct %76, %42, %75 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %78 = vector.extract %71[3] : vector<16x8xf32>
        %79 = vector.outerproduct %78, %44, %77 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %80 = vector.extract %71[4] : vector<16x8xf32>
        %81 = vector.outerproduct %80, %46, %79 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %82 = vector.extract %71[5] : vector<16x8xf32>
        %83 = vector.outerproduct %82, %48, %81 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %84 = vector.extract %71[6] : vector<16x8xf32>
        %85 = vector.outerproduct %84, %50, %83 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %86 = vector.extract %71[7] : vector<16x8xf32>
        %87 = vector.outerproduct %86, %52, %85 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.extract %71[8] : vector<16x8xf32>
        %89 = vector.outerproduct %88, %54, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.extract %71[9] : vector<16x8xf32>
        %91 = vector.outerproduct %90, %56, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.extract %71[10] : vector<16x8xf32>
        %93 = vector.outerproduct %92, %58, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.extract %71[11] : vector<16x8xf32>
        %95 = vector.outerproduct %94, %60, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.extract %71[12] : vector<16x8xf32>
        %97 = vector.outerproduct %96, %62, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.extract %71[13] : vector<16x8xf32>
        %99 = vector.outerproduct %98, %64, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.extract %71[14] : vector<16x8xf32>
        %101 = vector.outerproduct %100, %66, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %102 = vector.extract %71[15] : vector<16x8xf32>
        %103 = vector.outerproduct %102, %68, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %103 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.insert %22, %cst [0] : vector<16xf32> into vector<8x16xf32>
        %24 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.insert %24, %23 [1] : vector<16xf32> into vector<8x16xf32>
        %26 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.insert %26, %25 [2] : vector<16xf32> into vector<8x16xf32>
        %28 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.insert %28, %27 [3] : vector<16xf32> into vector<8x16xf32>
        %30 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %31 = vector.insert %30, %29 [4] : vector<16xf32> into vector<8x16xf32>
        %32 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %33 = vector.insert %32, %31 [5] : vector<16xf32> into vector<8x16xf32>
        %34 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %35 = vector.insert %34, %33 [6] : vector<16xf32> into vector<8x16xf32>
        %36 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %37 = vector.insert %36, %35 [7] : vector<16xf32> into vector<8x16xf32>
        %38 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %62 = vector.load %subview_2[%61, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %63 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %64 = vector.load %subview_2[%63, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %65 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %66 = vector.load %subview_2[%65, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %67 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %68 = vector.load %subview_2[%67, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %69 = vector.shape_cast %37 : vector<8x16xf32> to vector<128xf32>
        %70 = vector.shuffle %69, %69 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %71 = vector.shape_cast %70 : vector<128xf32> to vector<16x8xf32>
        %72 = vector.extract %71[0] : vector<16x8xf32>
        %73 = vector.outerproduct %72, %38, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %74 = vector.extract %71[1] : vector<16x8xf32>
        %75 = vector.outerproduct %74, %40, %73 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %76 = vector.extract %71[2] : vector<16x8xf32>
        %77 = vector.outerproduct %76, %42, %75 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %78 = vector.extract %71[3] : vector<16x8xf32>
        %79 = vector.outerproduct %78, %44, %77 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %80 = vector.extract %71[4] : vector<16x8xf32>
        %81 = vector.outerproduct %80, %46, %79 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %82 = vector.extract %71[5] : vector<16x8xf32>
        %83 = vector.outerproduct %82, %48, %81 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %84 = vector.extract %71[6] : vector<16x8xf32>
        %85 = vector.outerproduct %84, %50, %83 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %86 = vector.extract %71[7] : vector<16x8xf32>
        %87 = vector.outerproduct %86, %52, %85 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.extract %71[8] : vector<16x8xf32>
        %89 = vector.outerproduct %88, %54, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.extract %71[9] : vector<16x8xf32>
        %91 = vector.outerproduct %90, %56, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.extract %71[10] : vector<16x8xf32>
        %93 = vector.outerproduct %92, %58, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.extract %71[11] : vector<16x8xf32>
        %95 = vector.outerproduct %94, %60, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.extract %71[12] : vector<16x8xf32>
        %97 = vector.outerproduct %96, %62, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.extract %71[13] : vector<16x8xf32>
        %99 = vector.outerproduct %98, %64, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.extract %71[14] : vector<16x8xf32>
        %101 = vector.outerproduct %100, %66, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %102 = vector.extract %71[15] : vector<16x8xf32>
        %103 = vector.outerproduct %102, %68, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %103 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %24 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %26 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %28 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %30 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %31 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %32 = vector.load %subview_2[%31, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %33 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %34 = vector.load %subview_2[%33, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %35 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %36 = vector.load %subview_2[%35, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %37 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %38 = vector.load %subview_2[%37, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = vector.insert_strided_slice %22, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
        %62 = vector.insert_strided_slice %23, %61 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
        %63 = vector.insert_strided_slice %24, %62 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
        %64 = vector.insert_strided_slice %25, %63 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
        %65 = vector.insert_strided_slice %26, %64 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
        %66 = vector.insert_strided_slice %27, %65 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
        %67 = vector.insert_strided_slice %28, %66 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
        %68 = vector.insert_strided_slice %29, %67 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
        %69 = vector.shuffle %68, %68 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %70 = vector.extract_strided_slice %69 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %71 = vector.extract_strided_slice %69 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %72 = vector.extract_strided_slice %69 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %73 = vector.extract_strided_slice %69 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %74 = vector.extract_strided_slice %69 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %75 = vector.extract_strided_slice %69 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %76 = vector.extract_strided_slice %69 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %77 = vector.extract_strided_slice %69 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %78 = vector.extract_strided_slice %69 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %79 = vector.extract_strided_slice %69 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %80 = vector.extract_strided_slice %69 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %81 = vector.extract_strided_slice %69 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %82 = vector.extract_strided_slice %69 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %83 = vector.extract_strided_slice %69 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %84 = vector.extract_strided_slice %69 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %85 = vector.extract_strided_slice %69 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %86 = vector.outerproduct %70, %30, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %87 = vector.outerproduct %71, %32, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.outerproduct %72, %34, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.outerproduct %73, %36, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.outerproduct %74, %38, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.outerproduct %75, %40, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.outerproduct %76, %42, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.outerproduct %77, %44, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.outerproduct %78, %46, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.outerproduct %79, %48, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.outerproduct %80, %50, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.outerproduct %81, %52, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.outerproduct %82, %54, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.outerproduct %83, %56, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.outerproduct %84, %58, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.outerproduct %85, %60, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %101 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %24 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %26 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %28 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %30 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %31 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %32 = vector.load %subview_2[%31, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %33 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %34 = vector.load %subview_2[%33, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %35 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %36 = vector.load %subview_2[%35, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %37 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %38 = vector.load %subview_2[%37, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = vector.insert_strided_slice %22, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
        %62 = vector.insert_strided_slice %23, %61 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
        %63 = vector.insert_strided_slice %24, %62 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
        %64 = vector.insert_strided_slice %25, %63 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
        %65 = vector.insert_strided_slice %26, %64 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
        %66 = vector.insert_strided_slice %27, %65 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
        %67 = vector.insert_strided_slice %28, %66 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
        %68 = vector.insert_strided_slice %29, %67 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
        %69 = vector.shuffle %68, %68 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %70 = vector.extract_strided_slice %69 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %71 = vector.extract_strided_slice %69 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %72 = vector.extract_strided_slice %69 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %73 = vector.extract_strided_slice %69 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %74 = vector.extract_strided_slice %69 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %75 = vector.extract_strided_slice %69 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %76 = vector.extract_strided_slice %69 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %77 = vector.extract_strided_slice %69 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %78 = vector.extract_strided_slice %69 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %79 = vector.extract_strided_slice %69 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %80 = vector.extract_strided_slice %69 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %81 = vector.extract_strided_slice %69 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %82 = vector.extract_strided_slice %69 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %83 = vector.extract_strided_slice %69 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %84 = vector.extract_strided_slice %69 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %85 = vector.extract_strided_slice %69 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %86 = vector.outerproduct %70, %30, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %87 = vector.outerproduct %71, %32, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.outerproduct %72, %34, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.outerproduct %73, %36, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.outerproduct %74, %38, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.outerproduct %75, %40, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.outerproduct %76, %42, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.outerproduct %77, %44, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.outerproduct %78, %46, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.outerproduct %79, %48, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.outerproduct %80, %50, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.outerproduct %81, %52, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.outerproduct %82, %54, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.outerproduct %83, %56, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.outerproduct %84, %58, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.outerproduct %85, %60, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %101 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %24 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %26 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %28 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %30 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %31 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %32 = vector.load %subview_2[%31, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %33 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %34 = vector.load %subview_2[%33, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %35 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %36 = vector.load %subview_2[%35, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %37 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %38 = vector.load %subview_2[%37, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = vector.insert_strided_slice %22, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
        %62 = vector.insert_strided_slice %23, %61 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
        %63 = vector.insert_strided_slice %24, %62 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
        %64 = vector.insert_strided_slice %25, %63 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
        %65 = vector.insert_strided_slice %26, %64 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
        %66 = vector.insert_strided_slice %27, %65 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
        %67 = vector.insert_strided_slice %28, %66 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
        %68 = vector.insert_strided_slice %29, %67 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
        %69 = vector.shuffle %68, %68 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %70 = vector.extract_strided_slice %69 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %71 = vector.extract_strided_slice %69 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %72 = vector.extract_strided_slice %69 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %73 = vector.extract_strided_slice %69 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %74 = vector.extract_strided_slice %69 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %75 = vector.extract_strided_slice %69 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %76 = vector.extract_strided_slice %69 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %77 = vector.extract_strided_slice %69 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %78 = vector.extract_strided_slice %69 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %79 = vector.extract_strided_slice %69 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %80 = vector.extract_strided_slice %69 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %81 = vector.extract_strided_slice %69 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %82 = vector.extract_strided_slice %69 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %83 = vector.extract_strided_slice %69 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %84 = vector.extract_strided_slice %69 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %85 = vector.extract_strided_slice %69 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %86 = vector.outerproduct %70, %30, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %87 = vector.outerproduct %71, %32, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.outerproduct %72, %34, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.outerproduct %73, %36, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.outerproduct %74, %38, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.outerproduct %75, %40, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.outerproduct %76, %42, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.outerproduct %77, %44, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.outerproduct %78, %46, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.outerproduct %79, %48, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.outerproduct %80, %50, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.outerproduct %81, %52, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.outerproduct %82, %54, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.outerproduct %83, %56, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.outerproduct %84, %58, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.outerproduct %85, %60, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %101 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %24 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %26 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %28 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %30 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %31 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %32 = vector.load %subview_2[%31, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %33 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %34 = vector.load %subview_2[%33, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %35 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %36 = vector.load %subview_2[%35, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %37 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %38 = vector.load %subview_2[%37, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = vector.insert_strided_slice %22, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
        %62 = vector.insert_strided_slice %23, %61 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
        %63 = vector.insert_strided_slice %24, %62 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
        %64 = vector.insert_strided_slice %25, %63 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
        %65 = vector.insert_strided_slice %26, %64 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
        %66 = vector.insert_strided_slice %27, %65 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
        %67 = vector.insert_strided_slice %28, %66 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
        %68 = vector.insert_strided_slice %29, %67 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
        %69 = vector.shuffle %68, %68 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %70 = vector.extract_strided_slice %69 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %71 = vector.extract_strided_slice %69 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %72 = vector.extract_strided_slice %69 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %73 = vector.extract_strided_slice %69 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %74 = vector.extract_strided_slice %69 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %75 = vector.extract_strided_slice %69 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %76 = vector.extract_strided_slice %69 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %77 = vector.extract_strided_slice %69 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %78 = vector.extract_strided_slice %69 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %79 = vector.extract_strided_slice %69 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %80 = vector.extract_strided_slice %69 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %81 = vector.extract_strided_slice %69 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %82 = vector.extract_strided_slice %69 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %83 = vector.extract_strided_slice %69 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %84 = vector.extract_strided_slice %69 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %85 = vector.extract_strided_slice %69 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %86 = vector.outerproduct %70, %30, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %87 = vector.outerproduct %71, %32, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.outerproduct %72, %34, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.outerproduct %73, %36, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.outerproduct %74, %38, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.outerproduct %75, %40, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.outerproduct %76, %42, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.outerproduct %77, %44, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.outerproduct %78, %46, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.outerproduct %79, %48, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.outerproduct %80, %50, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.outerproduct %81, %52, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.outerproduct %82, %54, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.outerproduct %83, %56, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.outerproduct %84, %58, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.outerproduct %85, %60, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %101 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %24 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %26 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %28 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %30 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %31 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %32 = vector.load %subview_2[%31, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %33 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %34 = vector.load %subview_2[%33, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %35 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %36 = vector.load %subview_2[%35, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %37 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %38 = vector.load %subview_2[%37, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = vector.insert_strided_slice %22, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
        %62 = vector.insert_strided_slice %23, %61 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
        %63 = vector.insert_strided_slice %24, %62 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
        %64 = vector.insert_strided_slice %25, %63 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
        %65 = vector.insert_strided_slice %26, %64 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
        %66 = vector.insert_strided_slice %27, %65 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
        %67 = vector.insert_strided_slice %28, %66 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
        %68 = vector.insert_strided_slice %29, %67 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
        %69 = vector.shuffle %68, %68 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %70 = vector.extract_strided_slice %69 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %71 = vector.extract_strided_slice %69 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %72 = vector.extract_strided_slice %69 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %73 = vector.extract_strided_slice %69 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %74 = vector.extract_strided_slice %69 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %75 = vector.extract_strided_slice %69 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %76 = vector.extract_strided_slice %69 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %77 = vector.extract_strided_slice %69 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %78 = vector.extract_strided_slice %69 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %79 = vector.extract_strided_slice %69 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %80 = vector.extract_strided_slice %69 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %81 = vector.extract_strided_slice %69 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %82 = vector.extract_strided_slice %69 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %83 = vector.extract_strided_slice %69 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %84 = vector.extract_strided_slice %69 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %85 = vector.extract_strided_slice %69 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %86 = vector.outerproduct %70, %30, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %87 = vector.outerproduct %71, %32, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.outerproduct %72, %34, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.outerproduct %73, %36, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.outerproduct %74, %38, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.outerproduct %75, %40, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.outerproduct %76, %42, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.outerproduct %77, %44, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.outerproduct %78, %46, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.outerproduct %79, %48, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.outerproduct %80, %50, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.outerproduct %81, %52, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.outerproduct %82, %54, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.outerproduct %83, %56, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.outerproduct %84, %58, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.outerproduct %85, %60, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %101 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %24 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %26 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %28 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %30 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %31 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %32 = vector.load %subview_2[%31, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %33 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %34 = vector.load %subview_2[%33, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %35 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %36 = vector.load %subview_2[%35, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %37 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %38 = vector.load %subview_2[%37, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = vector.insert_strided_slice %22, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
        %62 = vector.insert_strided_slice %23, %61 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
        %63 = vector.insert_strided_slice %24, %62 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
        %64 = vector.insert_strided_slice %25, %63 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
        %65 = vector.insert_strided_slice %26, %64 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
        %66 = vector.insert_strided_slice %27, %65 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
        %67 = vector.insert_strided_slice %28, %66 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
        %68 = vector.insert_strided_slice %29, %67 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
        %69 = vector.shuffle %68, %68 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %70 = vector.extract_strided_slice %69 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %71 = vector.extract_strided_slice %69 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %72 = vector.extract_strided_slice %69 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %73 = vector.extract_strided_slice %69 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %74 = vector.extract_strided_slice %69 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %75 = vector.extract_strided_slice %69 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %76 = vector.extract_strided_slice %69 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %77 = vector.extract_strided_slice %69 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %78 = vector.extract_strided_slice %69 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %79 = vector.extract_strided_slice %69 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %80 = vector.extract_strided_slice %69 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %81 = vector.extract_strided_slice %69 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %82 = vector.extract_strided_slice %69 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %83 = vector.extract_strided_slice %69 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %84 = vector.extract_strided_slice %69 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %85 = vector.extract_strided_slice %69 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %86 = vector.outerproduct %70, %30, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %87 = vector.outerproduct %71, %32, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.outerproduct %72, %34, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.outerproduct %73, %36, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.outerproduct %74, %38, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.outerproduct %75, %40, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.outerproduct %76, %42, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.outerproduct %77, %44, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.outerproduct %78, %46, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.outerproduct %79, %48, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.outerproduct %80, %50, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.outerproduct %81, %52, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.outerproduct %82, %54, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.outerproduct %83, %56, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.outerproduct %84, %58, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.outerproduct %85, %60, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %101 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %24 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %26 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %28 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %30 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %31 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %32 = vector.load %subview_2[%31, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %33 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %34 = vector.load %subview_2[%33, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %35 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %36 = vector.load %subview_2[%35, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %37 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %38 = vector.load %subview_2[%37, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = vector.insert_strided_slice %22, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
        %62 = vector.insert_strided_slice %23, %61 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
        %63 = vector.insert_strided_slice %24, %62 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
        %64 = vector.insert_strided_slice %25, %63 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
        %65 = vector.insert_strided_slice %26, %64 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
        %66 = vector.insert_strided_slice %27, %65 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
        %67 = vector.insert_strided_slice %28, %66 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
        %68 = vector.insert_strided_slice %29, %67 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
        %69 = vector.shuffle %68, %68 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %70 = vector.extract_strided_slice %69 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %71 = vector.extract_strided_slice %69 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %72 = vector.extract_strided_slice %69 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %73 = vector.extract_strided_slice %69 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %74 = vector.extract_strided_slice %69 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %75 = vector.extract_strided_slice %69 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %76 = vector.extract_strided_slice %69 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %77 = vector.extract_strided_slice %69 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %78 = vector.extract_strided_slice %69 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %79 = vector.extract_strided_slice %69 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %80 = vector.extract_strided_slice %69 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %81 = vector.extract_strided_slice %69 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %82 = vector.extract_strided_slice %69 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %83 = vector.extract_strided_slice %69 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %84 = vector.extract_strided_slice %69 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %85 = vector.extract_strided_slice %69 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %86 = vector.outerproduct %70, %30, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %87 = vector.outerproduct %71, %32, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.outerproduct %72, %34, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.outerproduct %73, %36, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.outerproduct %74, %38, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.outerproduct %75, %40, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.outerproduct %76, %42, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.outerproduct %77, %44, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.outerproduct %78, %46, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.outerproduct %79, %48, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.outerproduct %80, %50, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.outerproduct %81, %52, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.outerproduct %82, %54, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.outerproduct %83, %56, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.outerproduct %84, %58, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.outerproduct %85, %60, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %101 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %24 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %26 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %28 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %30 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %31 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %32 = vector.load %subview_2[%31, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %33 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %34 = vector.load %subview_2[%33, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %35 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %36 = vector.load %subview_2[%35, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %37 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %38 = vector.load %subview_2[%37, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = vector.insert_strided_slice %22, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
        %62 = vector.insert_strided_slice %23, %61 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
        %63 = vector.insert_strided_slice %24, %62 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
        %64 = vector.insert_strided_slice %25, %63 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
        %65 = vector.insert_strided_slice %26, %64 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
        %66 = vector.insert_strided_slice %27, %65 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
        %67 = vector.insert_strided_slice %28, %66 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
        %68 = vector.insert_strided_slice %29, %67 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
        %69 = vector.shuffle %68, %68 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %70 = vector.extract_strided_slice %69 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %71 = vector.extract_strided_slice %69 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %72 = vector.extract_strided_slice %69 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %73 = vector.extract_strided_slice %69 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %74 = vector.extract_strided_slice %69 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %75 = vector.extract_strided_slice %69 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %76 = vector.extract_strided_slice %69 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %77 = vector.extract_strided_slice %69 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %78 = vector.extract_strided_slice %69 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %79 = vector.extract_strided_slice %69 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %80 = vector.extract_strided_slice %69 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %81 = vector.extract_strided_slice %69 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %82 = vector.extract_strided_slice %69 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %83 = vector.extract_strided_slice %69 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %84 = vector.extract_strided_slice %69 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %85 = vector.extract_strided_slice %69 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %86 = vector.outerproduct %70, %30, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %87 = vector.outerproduct %71, %32, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.outerproduct %72, %34, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.outerproduct %73, %36, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.outerproduct %74, %38, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.outerproduct %75, %40, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.outerproduct %76, %42, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.outerproduct %77, %44, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.outerproduct %78, %46, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.outerproduct %79, %48, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.outerproduct %80, %50, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.outerproduct %81, %52, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.outerproduct %82, %54, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.outerproduct %83, %56, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.outerproduct %84, %58, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.outerproduct %85, %60, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %101 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
        %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %24 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %25 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %26 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %27 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %28 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %29 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %30 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %31 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %32 = vector.load %subview_2[%31, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %33 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %34 = vector.load %subview_2[%33, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %35 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %36 = vector.load %subview_2[%35, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %37 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %38 = vector.load %subview_2[%37, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %39 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %41 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %43 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %45 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %47 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %51 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %53 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %57 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %59 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %61 = vector.insert_strided_slice %22, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
        %62 = vector.insert_strided_slice %23, %61 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
        %63 = vector.insert_strided_slice %24, %62 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
        %64 = vector.insert_strided_slice %25, %63 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
        %65 = vector.insert_strided_slice %26, %64 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
        %66 = vector.insert_strided_slice %27, %65 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
        %67 = vector.insert_strided_slice %28, %66 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
        %68 = vector.insert_strided_slice %29, %67 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
        %69 = vector.shuffle %68, %68 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %70 = vector.extract_strided_slice %69 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %71 = vector.extract_strided_slice %69 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %72 = vector.extract_strided_slice %69 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %73 = vector.extract_strided_slice %69 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %74 = vector.extract_strided_slice %69 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %75 = vector.extract_strided_slice %69 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %76 = vector.extract_strided_slice %69 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %77 = vector.extract_strided_slice %69 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %78 = vector.extract_strided_slice %69 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %79 = vector.extract_strided_slice %69 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %80 = vector.extract_strided_slice %69 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %81 = vector.extract_strided_slice %69 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %82 = vector.extract_strided_slice %69 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %83 = vector.extract_strided_slice %69 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %84 = vector.extract_strided_slice %69 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %85 = vector.extract_strided_slice %69 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %86 = vector.outerproduct %70, %30, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %87 = vector.outerproduct %71, %32, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %88 = vector.outerproduct %72, %34, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.outerproduct %73, %36, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %90 = vector.outerproduct %74, %38, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.outerproduct %75, %40, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %92 = vector.outerproduct %76, %42, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.outerproduct %77, %44, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %94 = vector.outerproduct %78, %46, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.outerproduct %79, %48, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %96 = vector.outerproduct %80, %50, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.outerproduct %81, %52, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %98 = vector.outerproduct %82, %54, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.outerproduct %83, %56, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %100 = vector.outerproduct %84, %58, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.outerproduct %85, %60, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %101 : vector<8x32xf32>
      }
      %13 = math.cos %12 : vector<8x32xf32>
      %14 = vector.extract %13[0] : vector<8x32xf32>
      vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.extract %13[1] : vector<8x32xf32>
      vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %16 = vector.extract %13[2] : vector<8x32xf32>
      vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.extract %13[3] : vector<8x32xf32>
      vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.extract %13[4] : vector<8x32xf32>
      vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.extract %13[5] : vector<8x32xf32>
      vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %20 = vector.extract %13[6] : vector<8x32xf32>
      vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.extract %13[7] : vector<8x32xf32>
      vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LLVMCPULowerExecutableTarget (iree-llvmcpu-lower-executable-target) //----- //
hal.executable.variant public @embedded_elf_x86_64, target = <"llvm-cpu", "embedded-elf-x86_64", {cpu = "generic", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", native_vector_size = 16 : index, target_triple = "x86_64-unknown-unknown-eabi-elf"}> {
  hal.executable.export public @mm_cosine_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingPadExpert>} {
  ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index):
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    hal.return %c8, %c2, %c1 : index, index, index
  }
  builtin.module {
    func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
      %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
      %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
      %c16 = arith.constant 16 : index
      %c8 = arith.constant 8 : index
      %c32 = arith.constant 32 : index
      %c256 = arith.constant 256 : index
      %c128 = arith.constant 128 : index
      %c0 = arith.constant 0 : index
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
      memref.assume_alignment %0, 64 : memref<512x256xf32>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
      memref.assume_alignment %1, 64 : memref<256x1024xf32>
      %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
      memref.assume_alignment %2, 64 : memref<512x1024xf32>
      %workgroup_id_x = hal.interface.workgroup.id[0] : index
      %workgroup_id_y = hal.interface.workgroup.id[1] : index
      %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
      %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
      %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
      %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
      %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
      scf.for %arg0 = %c0 to %c256 step %c8 {
        %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
        %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
        %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
        %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
        %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
        %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
        %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
        scf.for %arg1 = %c0 to %c128 step %c32 {
          %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
            %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
            %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
            %24 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
            %25 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
            %26 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
            %27 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
            %28 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
            %29 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
            %30 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %31 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
            %32 = vector.load %subview_2[%31, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %33 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
            %34 = vector.load %subview_2[%33, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %35 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
            %36 = vector.load %subview_2[%35, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %37 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
            %38 = vector.load %subview_2[%37, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %39 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
            %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %41 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
            %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %43 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
            %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %45 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
            %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %47 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
            %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %49 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
            %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %51 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
            %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %53 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
            %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %55 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
            %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %57 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
            %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %59 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
            %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %61 = vector.insert_strided_slice %22, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
            %62 = vector.insert_strided_slice %23, %61 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
            %63 = vector.insert_strided_slice %24, %62 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
            %64 = vector.insert_strided_slice %25, %63 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
            %65 = vector.insert_strided_slice %26, %64 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
            %66 = vector.insert_strided_slice %27, %65 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
            %67 = vector.insert_strided_slice %28, %66 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
            %68 = vector.insert_strided_slice %29, %67 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
            %69 = vector.shuffle %68, %68 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
            %70 = vector.extract_strided_slice %69 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %71 = vector.extract_strided_slice %69 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %72 = vector.extract_strided_slice %69 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %73 = vector.extract_strided_slice %69 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %74 = vector.extract_strided_slice %69 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %75 = vector.extract_strided_slice %69 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %76 = vector.extract_strided_slice %69 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %77 = vector.extract_strided_slice %69 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %78 = vector.extract_strided_slice %69 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %79 = vector.extract_strided_slice %69 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %80 = vector.extract_strided_slice %69 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %81 = vector.extract_strided_slice %69 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %82 = vector.extract_strided_slice %69 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %83 = vector.extract_strided_slice %69 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %84 = vector.extract_strided_slice %69 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %85 = vector.extract_strided_slice %69 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %86 = vector.outerproduct %70, %30, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %87 = vector.outerproduct %71, %32, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %88 = vector.outerproduct %72, %34, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %89 = vector.outerproduct %73, %36, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %90 = vector.outerproduct %74, %38, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %91 = vector.outerproduct %75, %40, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %92 = vector.outerproduct %76, %42, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %93 = vector.outerproduct %77, %44, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %94 = vector.outerproduct %78, %46, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %95 = vector.outerproduct %79, %48, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %96 = vector.outerproduct %80, %50, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %97 = vector.outerproduct %81, %52, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %98 = vector.outerproduct %82, %54, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %99 = vector.outerproduct %83, %56, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %100 = vector.outerproduct %84, %58, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %101 = vector.outerproduct %85, %60, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            scf.yield %101 : vector<8x32xf32>
          }
          %13 = math.cos %12 : vector<8x32xf32>
          %14 = vector.extract %13[0] : vector<8x32xf32>
          vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %15 = vector.extract %13[1] : vector<8x32xf32>
          vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %16 = vector.extract %13[2] : vector<8x32xf32>
          vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %17 = vector.extract %13[3] : vector<8x32xf32>
          vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %18 = vector.extract %13[4] : vector<8x32xf32>
          vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %19 = vector.extract %13[5] : vector<8x32xf32>
          vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %20 = vector.extract %13[6] : vector<8x32xf32>
          vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %21 = vector.extract %13[7] : vector<8x32xf32>
          vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        }
      }
      return
    }
  }
}

#executable_target_embedded_elf_x86_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "generic", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", native_vector_size = 16 : index, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<()[s0] -> (s0 * 256)>
#map1 = affine_map<()[s0] -> (s0 * 128)>
#map2 = affine_map<(d0) -> (d0 + 1)>
#map3 = affine_map<(d0) -> (d0 + 2)>
#map4 = affine_map<(d0) -> (d0 + 3)>
#map5 = affine_map<(d0) -> (d0 + 4)>
#map6 = affine_map<(d0) -> (d0 + 5)>
#map7 = affine_map<(d0) -> (d0 + 6)>
#map8 = affine_map<(d0) -> (d0 + 7)>
#map9 = affine_map<(d0) -> (d0 + 8)>
#map10 = affine_map<(d0) -> (d0 + 9)>
#map11 = affine_map<(d0) -> (d0 + 10)>
#map12 = affine_map<(d0) -> (d0 + 11)>
#map13 = affine_map<(d0) -> (d0 + 12)>
#map14 = affine_map<(d0) -> (d0 + 13)>
#map15 = affine_map<(d0) -> (d0 + 14)>
#map16 = affine_map<(d0) -> (d0 + 15)>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingPadExpert>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_embedded_elf_x86_64_]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  hal.executable private @mm_cosine_static_dispatch_0 {
    hal.executable.variant public @embedded_elf_x86_64, target = #executable_target_embedded_elf_x86_64_ {
      hal.executable.export public @mm_cosine_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index):
        %c8 = arith.constant 8 : index
        %c2 = arith.constant 2 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c2, %c1 : index, index, index
      }
      builtin.module {
        func.func @mm_cosine_static_dispatch_0_matmul_512x1024x256() {
          %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
          %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
          %c16 = arith.constant 16 : index
          %c8 = arith.constant 8 : index
          %c32 = arith.constant 32 : index
          %c256 = arith.constant 256 : index
          %c128 = arith.constant 128 : index
          %c0 = arith.constant 0 : index
          %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
          memref.assume_alignment %0, 64 : memref<512x256xf32>
          %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
          memref.assume_alignment %1, 64 : memref<256x1024xf32>
          %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
          memref.assume_alignment %2, 64 : memref<512x1024xf32>
          %workgroup_id_x = hal.interface.workgroup.id[0] : index
          %workgroup_id_y = hal.interface.workgroup.id[1] : index
          %3 = affine.apply #map()[%workgroup_id_y]
          %4 = affine.apply #map1()[%workgroup_id_x]
          %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
          %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
          %subview_2 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
          scf.for %arg0 = %c0 to %c256 step %c8 {
            %5 = affine.apply #map2(%arg0)
            %6 = affine.apply #map3(%arg0)
            %7 = affine.apply #map4(%arg0)
            %8 = affine.apply #map5(%arg0)
            %9 = affine.apply #map6(%arg0)
            %10 = affine.apply #map7(%arg0)
            %11 = affine.apply #map8(%arg0)
            scf.for %arg1 = %c0 to %c128 step %c32 {
              %12 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %cst_0) -> (vector<8x32xf32>) {
                %22 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
                %23 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
                %24 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
                %25 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
                %26 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
                %27 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
                %28 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
                %29 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
                %30 = vector.load %subview_2[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %31 = affine.apply #map2(%arg2)
                %32 = vector.load %subview_2[%31, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %33 = affine.apply #map3(%arg2)
                %34 = vector.load %subview_2[%33, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %35 = affine.apply #map4(%arg2)
                %36 = vector.load %subview_2[%35, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %37 = affine.apply #map5(%arg2)
                %38 = vector.load %subview_2[%37, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %39 = affine.apply #map6(%arg2)
                %40 = vector.load %subview_2[%39, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %41 = affine.apply #map7(%arg2)
                %42 = vector.load %subview_2[%41, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %43 = affine.apply #map8(%arg2)
                %44 = vector.load %subview_2[%43, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %45 = affine.apply #map9(%arg2)
                %46 = vector.load %subview_2[%45, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %47 = affine.apply #map10(%arg2)
                %48 = vector.load %subview_2[%47, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %49 = affine.apply #map11(%arg2)
                %50 = vector.load %subview_2[%49, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %51 = affine.apply #map12(%arg2)
                %52 = vector.load %subview_2[%51, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %53 = affine.apply #map13(%arg2)
                %54 = vector.load %subview_2[%53, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %55 = affine.apply #map14(%arg2)
                %56 = vector.load %subview_2[%55, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %57 = affine.apply #map15(%arg2)
                %58 = vector.load %subview_2[%57, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %59 = affine.apply #map16(%arg2)
                %60 = vector.load %subview_2[%59, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %61 = vector.insert_strided_slice %22, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
                %62 = vector.insert_strided_slice %23, %61 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
                %63 = vector.insert_strided_slice %24, %62 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
                %64 = vector.insert_strided_slice %25, %63 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
                %65 = vector.insert_strided_slice %26, %64 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
                %66 = vector.insert_strided_slice %27, %65 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
                %67 = vector.insert_strided_slice %28, %66 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
                %68 = vector.insert_strided_slice %29, %67 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
                %69 = vector.shuffle %68, %68 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
                %70 = vector.extract_strided_slice %69 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %71 = vector.extract_strided_slice %69 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %72 = vector.extract_strided_slice %69 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %73 = vector.extract_strided_slice %69 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %74 = vector.extract_strided_slice %69 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %75 = vector.extract_strided_slice %69 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %76 = vector.extract_strided_slice %69 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %77 = vector.extract_strided_slice %69 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %78 = vector.extract_strided_slice %69 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %79 = vector.extract_strided_slice %69 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %80 = vector.extract_strided_slice %69 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %81 = vector.extract_strided_slice %69 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %82 = vector.extract_strided_slice %69 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %83 = vector.extract_strided_slice %69 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %84 = vector.extract_strided_slice %69 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %85 = vector.extract_strided_slice %69 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %86 = vector.outerproduct %70, %30, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %87 = vector.outerproduct %71, %32, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %88 = vector.outerproduct %72, %34, %87 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %89 = vector.outerproduct %73, %36, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %90 = vector.outerproduct %74, %38, %89 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %91 = vector.outerproduct %75, %40, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %92 = vector.outerproduct %76, %42, %91 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %93 = vector.outerproduct %77, %44, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %94 = vector.outerproduct %78, %46, %93 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %95 = vector.outerproduct %79, %48, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %96 = vector.outerproduct %80, %50, %95 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %97 = vector.outerproduct %81, %52, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %98 = vector.outerproduct %82, %54, %97 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %99 = vector.outerproduct %83, %56, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %100 = vector.outerproduct %84, %58, %99 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %101 = vector.outerproduct %85, %60, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                scf.yield %101 : vector<8x32xf32>
              }
              %13 = math.cos %12 : vector<8x32xf32>
              %14 = vector.extract %13[0] : vector<8x32xf32>
              vector.store %14, %subview_1[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %15 = vector.extract %13[1] : vector<8x32xf32>
              vector.store %15, %subview_1[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %16 = vector.extract %13[2] : vector<8x32xf32>
              vector.store %16, %subview_1[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %17 = vector.extract %13[3] : vector<8x32xf32>
              vector.store %17, %subview_1[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %18 = vector.extract %13[4] : vector<8x32xf32>
              vector.store %18, %subview_1[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %19 = vector.extract %13[5] : vector<8x32xf32>
              vector.store %19, %subview_1[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %20 = vector.extract %13[6] : vector<8x32xf32>
              vector.store %20, %subview_1[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %21 = vector.extract %13[7] : vector<8x32xf32>
              vector.store %21, %subview_1[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            }
          }
          return
        }
      }
    }
  }
}

