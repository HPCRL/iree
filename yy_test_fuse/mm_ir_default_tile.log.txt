Pass Manager with 1 passes:
builtin.module(hal.executable(hal.executable.variant(iree-llvmcpu-lower-executable-target{test-lowering-configuration=false use-lowering-pipeline=})))
g.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<512x1024xf32>) outs(%2 : tensor<512x1024xf32>) {
    ^bb0(%in: f32, %out: f32):
      %10 = math.cos %in : f32
      linalg.yield %10 : f32
    } -> tensor<512x102
@@@@@@--- Before Tile + Distribute ---
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %3 = flow.dispatch.tensor.load %0, offsets = [0, 0], sizes = [512, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<512x256xf32>
  %4 = flow.dispatch.tensor.load %1, offsets = [0, 0], sizes = [256, 1024], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x1024xf32>
  %5 = flow.dispatch.tensor.load %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<512x1024xf32>
  %6 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
  flow.dispatch.tensor.store %6, %2, offsets = [0, 0], sizes = [512, 1024], strides = [1, 1] : tensor<512x1024xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  return
}

Yufan:: tileAndFuseDispatchUsingSCFForOp::   
%6 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%3, %4 : tensor<512x256xf32>, tensor<256x1024xf32>) outs(%5 : tensor<512x1024xf32>) -> tensor<512x1024xf32>
@@@@@@--- After Tile + Distribute ---
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.min affine_map<(d0) -> (256, -d0 + 512)>(%arg0)
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %7 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %6 to %c1024 step %7 {
      %8 = affine.min affine_map<(d0) -> (128, -d0 + 1024)>(%arg1)
      %9 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%5, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
      %10 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %8], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
      %11 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [%5, %8], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<?x?xf32>
      %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%9, %10 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%11 : tensor<?x?xf32>) -> tensor<?x?xf32>
      flow.dispatch.tensor.store %12, %2, offsets = [%arg0, %arg1], sizes = [%5, %8], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After TileAndDistributeToWorkgroups (iree-codegen-tile-and-distribute-to-workgroups) //----- //
hal.executable.variant public @embedded_elf_x86_64, target = <"llvm-cpu", "embedded-elf-x86_64", {cpu = "generic", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", native_vector_size = 16 : index, target_triple = "x86_64-unknown-unknown-eabi-elf"}> {
  hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingPadExpert>} {
  ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    hal.return %c8, %c2, %c1 : index, index, index
  }
  builtin.module {
    func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
      %c256 = arith.constant 256 : index
      %c128 = arith.constant 128 : index
      %c1024 = arith.constant 1024 : index
      %c512 = arith.constant 512 : index
      %c0 = arith.constant 0 : index
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
      %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
      %workgroup_id_x = hal.interface.workgroup.id[0] : index
      %workgroup_count_x = hal.interface.workgroup.count[0] : index
      %workgroup_id_y = hal.interface.workgroup.id[1] : index
      %workgroup_count_y = hal.interface.workgroup.count[1] : index
      %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
      %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
      scf.for %arg0 = %3 to %c512 step %4 {
        %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
        %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
        scf.for %arg1 = %5 to %c1024 step %6 {
          %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%c256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
          %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %c128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
          %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<?x?xf32>
          %10 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%7, %8 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%9 : tensor<?x?xf32>) -> tensor<?x?xf32>
          flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
        }
      }
      return
    }
  }
}

// -----// IR Dump After TileAndDecomposeAttention (iree-linalg-ext-tile-and-decompose-attention) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %5 to %c1024 step %6 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%c256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %c128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<?x?xf32>
      %10 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%7, %8 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%9 : tensor<?x?xf32>) -> tensor<?x?xf32>
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After DecomposeSoftmax (iree-linalg-ext-decompose-softmax) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %5 to %c1024 step %6 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%c256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %c128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<?x?xf32>
      %10 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%7, %8 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%9 : tensor<?x?xf32>) -> tensor<?x?xf32>
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After ConvertToDestinationPassingStyle (iree-codegen-convert-to-destination-passing-style) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %5 to %c1024 step %6 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%c256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %c128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<?x?xf32>
      %10 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%7, %8 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%9 : tensor<?x?xf32>) -> tensor<?x?xf32>
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After FoldAffineMinInDistributedLoops (iree-codegen-fold-affinemin-in-distributed-loops) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %5 to %c1024 step %6 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [%c256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<?x256xf32>
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, %c128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x?xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<?x?xf32>
      %10 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%7, %8 : tensor<?x256xf32>, tensor<256x?xf32>) outs(%9 : tensor<?x?xf32>) -> tensor<?x?xf32>
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [%c256, %c128], strides = [1, 1] : tensor<?x?xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    scf.for %arg0 = %3 to %c512 step %4 {
      %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
      %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
      scf.for %arg1 = %5 to %c1024 step %6 {
        %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
        %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
        %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
        %10 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%7, %8 : tensor<256x256xf32>, tensor<256x128xf32>) outs(%9 : tensor<256x128xf32>) -> tensor<256x128xf32>
        flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    scf.for %arg0 = %3 to %c512 step %4 {
      %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
      %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
      scf.for %arg1 = %5 to %c1024 step %6 {
        %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
        %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
        %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
        %10 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%7, %8 : tensor<256x256xf32>, tensor<256x128xf32>) outs(%9 : tensor<256x128xf32>) -> tensor<256x128xf32>
        flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After TileAndDecomposeWinogradTransform (iree-linalg-ext-tile-and-decompose-winograd) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %5 to %c1024 step %6 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%7, %8 : tensor<256x256xf32>, tensor<256x128xf32>) outs(%9 : tensor<256x128xf32>) -> tensor<256x128xf32>
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

Yufan:: SCFTileAndFusePattern::matchAndRewrite 
 tiledOps linalg.matmul
%13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_5 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_6 : tensor<8x32xf32>) -> tensor<8x32xf32>
Yufan:: ??  
slice operand -->0x1fc2ec0
%extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
slice operand -->0x1fc2f70
%extracted_slice_5 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
slice operand -->0x1fc3020
%extracted_slice_6 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
tiledOp linalg.matmul
%13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_5 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_6 : tensor<8x32xf32>) -> tensor<8x32xf32>
origOp linalg.matmul
%10 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%7, %8 : tensor<256x256xf32>, tensor<256x128xf32>) outs(%9 : tensor<256x128xf32>) -> tensor<256x128xf32>
// -----// IR Dump After LinalgStrategyTileAndFusePass (iree-linalg-strategy-tile-and-fuse-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  scf.for %arg0 = %3 to %c512 step %4 {
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg1 = %5 to %c1024 step %6 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgFuse (linalg-fuse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyPadPass (iree-linalg-strategy-pad-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgFuse (linalg-fuse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyPadPass (iree-linalg-strategy-pad-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgFuse (linalg-fuse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyPadPass (iree-linalg-strategy-pad-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgFuse (linalg-fuse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice, %extracted_slice_0 : tensor<8x256xf32>, tensor<256x32xf32>) outs(%extracted_slice_1 : tensor<8x32xf32>) -> tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyTilePass (iree-linalg-strategy-tile-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %extracted_slice_1) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %13 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %13 : tensor<8x32xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %extracted_slice_1) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %13 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %13 : tensor<8x32xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %extracted_slice_1) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %13 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %13 : tensor<8x32xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %extracted_slice_1) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %13 : tensor<8x32xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %extracted_slice_1) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %13 : tensor<8x32xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %extracted_slice_1) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %13 : tensor<8x32xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgSingleTilingExpert (linalg-single-tiling-expert-driver) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %extracted_slice_1) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %13 : tensor<8x32xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %extracted_slice_1) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %13 : tensor<8x32xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %extracted_slice_1) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %13 : tensor<8x32xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyPadPass (iree-linalg-strategy-pad-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %extracted_slice_1) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %13 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %13 : tensor<8x32xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %extracted_slice_1) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %13 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %13 : tensor<8x32xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %extracted_slice_1) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %13 = linalg.matmul {__internal_linalg_transform__ = "1", lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %13 : tensor<8x32xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %extracted_slice_1) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %13 : tensor<8x32xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %extracted_slice_1) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %13 : tensor<8x32xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %extracted_slice_1) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %13 : tensor<8x32xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgFuse (linalg-fuse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
          %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %extracted_slice_1) -> (tensor<8x32xf32>) {
            %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
            %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
            %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
            scf.yield %13 : tensor<8x32xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg0 = %3 to %c512 step %4 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
      scf.for %arg1 = %5 to %c1024 step %6 {
        %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
        %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
        %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %7[%arg2, 0] [8, 256] [1, 1] : tensor<256x256xf32> to tensor<8x256xf32>
          %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
            %extracted_slice_0 = tensor.extract_slice %8[0, %arg4] [256, 32] [1, 1] : tensor<256x128xf32> to tensor<256x32xf32>
            %extracted_slice_1 = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
            %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %extracted_slice_1) -> (tensor<8x32xf32>) {
              %extracted_slice_2 = tensor.extract_slice %extracted_slice[0, %arg6] [8, 16] [1, 1] : tensor<8x256xf32> to tensor<8x16xf32>
              %extracted_slice_3 = tensor.extract_slice %extracted_slice_0[%arg6, 0] [16, 32] [1, 1] : tensor<256x32xf32> to tensor<16x32xf32>
              %13 = linalg.matmul {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[256, 128, 0], [8, 32, 0], [0, 0, 16]]>} ins(%extracted_slice_2, %extracted_slice_3 : tensor<8x16xf32>, tensor<16x32xf32>) outs(%arg7 : tensor<8x32xf32>) -> tensor<8x32xf32>
              scf.yield %13 : tensor<8x32xf32>
            }
            %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
            scf.yield %inserted_slice : tensor<256x128xf32>
          }
          scf.yield %11 : tensor<256x128xf32>
        }
        flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After LinalgStrategyVectorizePass (iree-linalg-strategy-vectorize-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %extracted_slice) -> (tensor<8x32xf32>) {
            %13 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
            %14 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
            %15 = vector.transfer_read %arg7[%c0, %c0], %cst {in_bounds = [true, true]} : tensor<8x32xf32>, vector<8x32xf32>
            %16 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %13, %14, %15 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            %17 = vector.transfer_write %16, %arg7[%c0, %c0] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<8x32xf32>
            scf.yield %17 : tensor<8x32xf32>
          }
          %inserted_slice = tensor.insert_slice %12 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = vector.transfer_read %extracted_slice[%c0, %c0], %cst {in_bounds = [true, true]} : tensor<8x32xf32>, vector<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (vector<8x32xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
            %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %17 : vector<8x32xf32>
          }
          %14 = vector.transfer_write %13, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = vector.transfer_read %extracted_slice[%c0, %c0], %cst {in_bounds = [true, true]} : tensor<8x32xf32>, vector<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (vector<8x32xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
            %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %17 : vector<8x32xf32>
          }
          %14 = vector.transfer_write %13, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = vector.transfer_read %extracted_slice[%c0, %c0], %cst {in_bounds = [true, true]} : tensor<8x32xf32>, vector<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (vector<8x32xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
            %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %17 : vector<8x32xf32>
          }
          %14 = vector.transfer_write %13, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = vector.transfer_read %extracted_slice[%c0, %c0], %cst {in_bounds = [true, true]} : tensor<8x32xf32>, vector<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (vector<8x32xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
            %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %17 : vector<8x32xf32>
          }
          %14 = vector.transfer_write %13, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = vector.transfer_read %extracted_slice[%c0, %c0], %cst {in_bounds = [true, true]} : tensor<8x32xf32>, vector<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (vector<8x32xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
            %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %17 : vector<8x32xf32>
          }
          %14 = vector.transfer_write %13, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After LinalgSingleTilingExpert (linalg-single-tiling-expert-driver) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %extracted_slice = tensor.extract_slice %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<256x128xf32> to tensor<8x32xf32>
          %12 = vector.transfer_read %extracted_slice[%c0, %c0], %cst {in_bounds = [true, true]} : tensor<8x32xf32>, vector<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (vector<8x32xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
            %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %17 : vector<8x32xf32>
          }
          %14 = vector.transfer_write %13, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<8x32xf32>
          %inserted_slice = tensor.insert_slice %14 into %arg5[%arg2, %arg4] [8, 32] [1, 1] : tensor<8x32xf32> into tensor<256x128xf32>
          scf.yield %inserted_slice : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %12 = vector.transfer_read %arg5[%arg2, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (vector<8x32xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
            %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %17 : vector<8x32xf32>
          }
          %14 = vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<256x128xf32>
          scf.yield %14 : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
      %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
      %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
        %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
          %12 = vector.transfer_read %arg5[%arg2, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<8x32xf32>
          %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (vector<8x32xf32>) {
            %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
            %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
            %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %17 : vector<8x32xf32>
          }
          %14 = vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<256x128xf32>
          scf.yield %14 : tensor<256x128xf32>
        }
        scf.yield %11 : tensor<256x128xf32>
      }
      flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    }
  }
  return
}

// -----// IR Dump After EliminateEmptyTensors (iree-eliminate-empty-tensors) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %cst = arith.constant 0.000000e+00 : f32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg0 = %3 to %c512 step %4 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
      scf.for %arg1 = %5 to %c1024 step %6 {
        %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
        %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
        %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
          %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
            %12 = vector.transfer_read %arg5[%arg2, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<8x32xf32>
            %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (vector<8x32xf32>) {
              %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
              %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
              %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
              scf.yield %17 : vector<8x32xf32>
            }
            %14 = vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<256x128xf32>
            scf.yield %14 : tensor<256x128xf32>
          }
          scf.yield %11 : tensor<256x128xf32>
        }
        flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After EmptyTensorToAllocTensor (empty-tensor-to-alloc-tensor) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %cst = arith.constant 0.000000e+00 : f32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg0 = %3 to %c512 step %4 {
      %7 = flow.dispatch.tensor.load %0, offsets = [%arg0, 0], sizes = [256, 256], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<512x256xf32>> -> tensor<256x256xf32>
      scf.for %arg1 = %5 to %c1024 step %6 {
        %8 = flow.dispatch.tensor.load %1, offsets = [0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>> -> tensor<256x128xf32>
        %9 = flow.dispatch.tensor.load %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>> -> tensor<256x128xf32>
        %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %9) -> (tensor<256x128xf32>) {
          %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (tensor<256x128xf32>) {
            %12 = vector.transfer_read %arg5[%arg2, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<8x32xf32>
            %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (vector<8x32xf32>) {
              %15 = vector.transfer_read %7[%arg2, %arg6], %cst {in_bounds = [true, true]} : tensor<256x256xf32>, vector<8x16xf32>
              %16 = vector.transfer_read %8[%arg6, %arg4], %cst {in_bounds = [true, true]} : tensor<256x128xf32>, vector<16x32xf32>
              %17 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %15, %16, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
              scf.yield %17 : vector<8x32xf32>
            }
            %14 = vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, tensor<256x128xf32>
            scf.yield %14 : tensor<256x128xf32>
          }
          scf.yield %11 : tensor<256x128xf32>
        }
        flow.dispatch.tensor.store %10, %2, offsets = [%arg0, %arg1], sizes = [256, 128], strides = [1, 1] : tensor<256x128xf32> -> !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
      }
    }
    return
  }
}

// -----// IR Dump After IREEComprehensiveBufferize (iree-codegen-iree-comprehensive-bufferize) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %cst = arith.constant 0.000000e+00 : f32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %6 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %7 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %9 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg0 = %6 to %c512 step %7 {
      %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.for %arg1 = %8 to %c1024 step %9 {
        %subview_0 = memref.subview %2[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_1 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %subview_1) -> (memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
          %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
            %12 = vector.transfer_read %arg5[%arg2, %arg4], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<8x32xf32>
            %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (vector<8x32xf32>) {
              %14 = vector.transfer_read %subview[%arg2, %arg6], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<8x16xf32>
              %15 = vector.transfer_read %subview_0[%arg6, %arg4], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<16x32xf32>
              %16 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %14, %15, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
              scf.yield %16 : vector<8x32xf32>
            }
            vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
            scf.yield %arg5 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
          }
          scf.yield %11 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        }
        %subview_2 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%10 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        }
      }
    }
    return
  }
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
module {
  func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
    %cst = arith.constant 0.000000e+00 : f32
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c1024 = arith.constant 1024 : index
    %c512 = arith.constant 512 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
    %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
    %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
    %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
    %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %workgroup_id_y = hal.interface.workgroup.id[1] : index
    %workgroup_count_y = hal.interface.workgroup.count[1] : index
    %6 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
    %7 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
    %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
    %9 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
    scf.for %arg0 = %6 to %c512 step %7 {
      %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.for %arg1 = %8 to %c1024 step %9 {
        %subview_0 = memref.subview %2[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %subview_1 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %10 = scf.for %arg2 = %c0 to %c256 step %c8 iter_args(%arg3 = %subview_1) -> (memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
          %11 = scf.for %arg4 = %c0 to %c128 step %c32 iter_args(%arg5 = %arg3) -> (memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
            %12 = vector.transfer_read %arg5[%arg2, %arg4], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<8x32xf32>
            %13 = scf.for %arg6 = %c0 to %c256 step %c16 iter_args(%arg7 = %12) -> (vector<8x32xf32>) {
              %14 = vector.transfer_read %subview[%arg2, %arg6], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<8x16xf32>
              %15 = vector.transfer_read %subview_0[%arg6, %arg4], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<16x32xf32>
              %16 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %14, %15, %arg7 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
              scf.yield %16 : vector<8x32xf32>
            }
            vector.transfer_write %13, %arg5[%arg2, %arg4] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
            scf.yield %arg5 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
          }
          scf.yield %11 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        }
        %subview_2 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%10 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        }
      }
    }
    return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %6 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %7 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %9 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %6 to %c512 step %7 {
    %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.for %arg1 = %8 to %c1024 step %9 {
      %subview_0 = memref.subview %2[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_1 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.for %arg2 = %c0 to %c256 step %c8 {
        scf.for %arg3 = %c0 to %c128 step %c32 {
          %10 = vector.transfer_read %subview_1[%arg2, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<8x32xf32>
          %11 = scf.for %arg4 = %c0 to %c256 step %c16 iter_args(%arg5 = %10) -> (vector<8x32xf32>) {
            %12 = vector.transfer_read %subview[%arg2, %arg4], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<8x16xf32>
            %13 = vector.transfer_read %subview_0[%arg4, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<16x32xf32>
            %14 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %12, %13, %arg5 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %14 : vector<8x32xf32>
          }
          vector.transfer_write %11, %subview_1[%arg2, %arg3] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        }
      }
      %subview_2 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_1 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_2 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %6 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %7 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %9 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %6 to %c512 step %7 {
    %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.for %arg1 = %8 to %c1024 step %9 {
      %subview_0 = memref.subview %2[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_1 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.for %arg2 = %c0 to %c256 step %c8 {
        scf.for %arg3 = %c0 to %c128 step %c32 {
          %10 = vector.transfer_read %subview_1[%arg2, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<8x32xf32>
          %11 = scf.for %arg4 = %c0 to %c256 step %c16 iter_args(%arg5 = %10) -> (vector<8x32xf32>) {
            %12 = vector.transfer_read %subview[%arg2, %arg4], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<8x16xf32>
            %13 = vector.transfer_read %subview_0[%arg4, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<16x32xf32>
            %14 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %12, %13, %arg5 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %14 : vector<8x32xf32>
          }
          vector.transfer_write %11, %subview_1[%arg2, %arg3] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        }
      }
      linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%subview_1 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) outs(%subview_1 : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<512x256xf32>>
  %2 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  %3 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<256x1024xf32>>
  %4 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %4, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  %5 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<readwrite:tensor<512x1024xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %6 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %7 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %8 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %9 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %6 to %c512 step %7 {
    %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.for %arg1 = %8 to %c1024 step %9 {
      %subview_0 = memref.subview %2[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_1 = memref.subview %4[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.for %arg2 = %c0 to %c256 step %c8 {
        scf.for %arg3 = %c0 to %c128 step %c32 {
          %10 = vector.transfer_read %subview_1[%arg2, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<8x32xf32>
          %11 = scf.for %arg4 = %c0 to %c256 step %c16 iter_args(%arg5 = %10) -> (vector<8x32xf32>) {
            %12 = vector.transfer_read %subview[%arg2, %arg4], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<8x16xf32>
            %13 = vector.transfer_read %subview_0[%arg4, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<16x32xf32>
            %14 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %12, %13, %arg5 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %14 : vector<8x32xf32>
          }
          vector.transfer_write %11, %subview_1[%arg2, %arg3] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        }
      }
    }
  }
  return
}

// -----// IR Dump After CleanupBufferAllocView (iree-codegen-cleanup-buffer-alloc-view) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<512x256xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %2, 64 : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32, #hal.descriptor_type<storage_buffer>> to memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %subview_0 = memref.subview %1[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %subview_1 = memref.subview %2[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32, #hal.descriptor_type<storage_buffer>> to memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      scf.for %arg2 = %c0 to %c256 step %c8 {
        scf.for %arg3 = %c0 to %c128 step %c32 {
          %7 = vector.transfer_read %subview_1[%arg2, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<8x32xf32>
          %8 = scf.for %arg4 = %c0 to %c256 step %c16 iter_args(%arg5 = %7) -> (vector<8x32xf32>) {
            %9 = vector.transfer_read %subview[%arg2, %arg4], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<8x16xf32>
            %10 = vector.transfer_read %subview_0[%arg4, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>, vector<16x32xf32>
            %11 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %9, %10, %arg5 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %11 : vector<8x32xf32>
          }
          vector.transfer_write %8, %subview_1[%arg2, %arg3] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        }
      }
    }
  }
  return
}

// -----// IR Dump After EraseHALDescriptorTypeFromMemRef (iree-codegen-erase-hal-descriptor-type-from-memref) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c1024 = arith.constant 1024 : index
  %c512 = arith.constant 512 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %workgroup_count_y = hal.interface.workgroup.count[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_count_y]
  %5 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %6 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_count_x]
  scf.for %arg0 = %3 to %c512 step %4 {
    %subview = memref.subview %0[%arg0, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
    scf.for %arg1 = %5 to %c1024 step %6 {
      %subview_0 = memref.subview %1[0, %arg1] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
      %subview_1 = memref.subview %2[%arg0, %arg1] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
      scf.for %arg2 = %c0 to %c256 step %c8 {
        scf.for %arg3 = %c0 to %c128 step %c32 {
          %7 = vector.transfer_read %subview_1[%arg2, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
          %8 = scf.for %arg4 = %c0 to %c256 step %c16 iter_args(%arg5 = %7) -> (vector<8x32xf32>) {
            %9 = vector.transfer_read %subview[%arg2, %arg4], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
            %10 = vector.transfer_read %subview_0[%arg4, %arg3], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
            %11 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %9, %10, %arg5 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
            scf.yield %11 : vector<8x32xf32>
          }
          vector.transfer_write %8, %subview_1[%arg2, %arg3] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
        }
      }
    }
  }
  return
}

// -----// IR Dump After RemoveSingleIterationLoop (iree-codegen-remove-single-iteration-loop) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.contract {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d2, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"], kind = #vector.kind<add>} %7, %8, %arg3 : vector<8x16xf32>, vector<16x32xf32> into vector<8x32xf32>
        scf.yield %9 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant 0.000000e+00 : f32
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_0 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_1 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.transfer_read %subview_1[%arg0, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<8x32xf32>
      %6 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %5) -> (vector<8x32xf32>) {
        %7 = vector.transfer_read %subview[%arg0, %arg2], %cst {in_bounds = [true, true]} : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<8x16xf32>
        %8 = vector.transfer_read %subview_0[%arg2, %arg1], %cst {in_bounds = [true, true]} : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<16x32xf32>
        %9 = vector.transpose %7, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %10 = vector.extract %9[0] : vector<16x8xf32>
        %11 = vector.extract %8[0] : vector<16x32xf32>
        %12 = vector.outerproduct %10, %11, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %13 = vector.extract %9[1] : vector<16x8xf32>
        %14 = vector.extract %8[1] : vector<16x32xf32>
        %15 = vector.outerproduct %13, %14, %12 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %16 = vector.extract %9[2] : vector<16x8xf32>
        %17 = vector.extract %8[2] : vector<16x32xf32>
        %18 = vector.outerproduct %16, %17, %15 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %19 = vector.extract %9[3] : vector<16x8xf32>
        %20 = vector.extract %8[3] : vector<16x32xf32>
        %21 = vector.outerproduct %19, %20, %18 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %22 = vector.extract %9[4] : vector<16x8xf32>
        %23 = vector.extract %8[4] : vector<16x32xf32>
        %24 = vector.outerproduct %22, %23, %21 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %25 = vector.extract %9[5] : vector<16x8xf32>
        %26 = vector.extract %8[5] : vector<16x32xf32>
        %27 = vector.outerproduct %25, %26, %24 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %28 = vector.extract %9[6] : vector<16x8xf32>
        %29 = vector.extract %8[6] : vector<16x32xf32>
        %30 = vector.outerproduct %28, %29, %27 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %31 = vector.extract %9[7] : vector<16x8xf32>
        %32 = vector.extract %8[7] : vector<16x32xf32>
        %33 = vector.outerproduct %31, %32, %30 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %34 = vector.extract %9[8] : vector<16x8xf32>
        %35 = vector.extract %8[8] : vector<16x32xf32>
        %36 = vector.outerproduct %34, %35, %33 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %37 = vector.extract %9[9] : vector<16x8xf32>
        %38 = vector.extract %8[9] : vector<16x32xf32>
        %39 = vector.outerproduct %37, %38, %36 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %40 = vector.extract %9[10] : vector<16x8xf32>
        %41 = vector.extract %8[10] : vector<16x32xf32>
        %42 = vector.outerproduct %40, %41, %39 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %43 = vector.extract %9[11] : vector<16x8xf32>
        %44 = vector.extract %8[11] : vector<16x32xf32>
        %45 = vector.outerproduct %43, %44, %42 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %46 = vector.extract %9[12] : vector<16x8xf32>
        %47 = vector.extract %8[12] : vector<16x32xf32>
        %48 = vector.outerproduct %46, %47, %45 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %49 = vector.extract %9[13] : vector<16x8xf32>
        %50 = vector.extract %8[13] : vector<16x32xf32>
        %51 = vector.outerproduct %49, %50, %48 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %52 = vector.extract %9[14] : vector<16x8xf32>
        %53 = vector.extract %8[14] : vector<16x32xf32>
        %54 = vector.outerproduct %52, %53, %51 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %55 = vector.extract %9[15] : vector<16x8xf32>
        %56 = vector.extract %8[15] : vector<16x32xf32>
        %57 = vector.outerproduct %55, %56, %54 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %57 : vector<8x32xf32>
      }
      vector.transfer_write %6, %subview_1[%arg0, %arg1] {in_bounds = [true, true]} : vector<8x32xf32>, memref<256x128xf32, strided<[1024, 1], offset: ?>>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %5 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %6 = vector.insert %5, %cst [0] : vector<32xf32> into vector<8x32xf32>
      %7 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
      %8 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %9 = vector.insert %8, %6 [1] : vector<32xf32> into vector<8x32xf32>
      %10 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
      %11 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %12 = vector.insert %11, %9 [2] : vector<32xf32> into vector<8x32xf32>
      %13 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
      %14 = vector.load %subview_2[%13, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %12 [3] : vector<32xf32> into vector<8x32xf32>
      %16 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
      %17 = vector.load %subview_2[%16, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %18 = vector.insert %17, %15 [4] : vector<32xf32> into vector<8x32xf32>
      %19 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
      %20 = vector.load %subview_2[%19, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %18 [5] : vector<32xf32> into vector<8x32xf32>
      %22 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
      %23 = vector.load %subview_2[%22, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %24 = vector.insert %23, %21 [6] : vector<32xf32> into vector<8x32xf32>
      %25 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
      %26 = vector.load %subview_2[%25, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %24 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %44 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %45 = vector.insert %44, %cst_0 [0] : vector<16xf32> into vector<8x16xf32>
        %46 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
        %47 = vector.load %subview[%46, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %48 = vector.insert %47, %45 [1] : vector<16xf32> into vector<8x16xf32>
        %49 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
        %50 = vector.load %subview[%49, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %51 = vector.insert %50, %48 [2] : vector<16xf32> into vector<8x16xf32>
        %52 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
        %53 = vector.load %subview[%52, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %54 = vector.insert %53, %51 [3] : vector<16xf32> into vector<8x16xf32>
        %55 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
        %56 = vector.load %subview[%55, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %57 = vector.insert %56, %54 [4] : vector<16xf32> into vector<8x16xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
        %59 = vector.load %subview[%58, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %60 = vector.insert %59, %57 [5] : vector<16xf32> into vector<8x16xf32>
        %61 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
        %62 = vector.load %subview[%61, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %63 = vector.insert %62, %60 [6] : vector<16xf32> into vector<8x16xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
        %65 = vector.load %subview[%64, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %66 = vector.insert %65, %63 [7] : vector<16xf32> into vector<8x16xf32>
        %67 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %77 = vector.load %subview_1[%76, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %78 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %79 = vector.load %subview_1[%78, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %80 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %81 = vector.load %subview_1[%80, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %82 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %83 = vector.load %subview_1[%82, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %84 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %85 = vector.load %subview_1[%84, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %86 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %87 = vector.load %subview_1[%86, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %88 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %89 = vector.load %subview_1[%88, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %90 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %91 = vector.load %subview_1[%90, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %92 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %93 = vector.load %subview_1[%92, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %94 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %95 = vector.load %subview_1[%94, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %96 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %97 = vector.load %subview_1[%96, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %98 = vector.transpose %66, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %99 = vector.extract %98[0] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %67, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %98[1] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %69, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %98[2] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %71, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %98[3] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %73, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %98[4] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %75, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.extract %98[5] : vector<16x8xf32>
        %110 = vector.outerproduct %109, %77, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.extract %98[6] : vector<16x8xf32>
        %112 = vector.outerproduct %111, %79, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.extract %98[7] : vector<16x8xf32>
        %114 = vector.outerproduct %113, %81, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.extract %98[8] : vector<16x8xf32>
        %116 = vector.outerproduct %115, %83, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %117 = vector.extract %98[9] : vector<16x8xf32>
        %118 = vector.outerproduct %117, %85, %116 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %119 = vector.extract %98[10] : vector<16x8xf32>
        %120 = vector.outerproduct %119, %87, %118 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %121 = vector.extract %98[11] : vector<16x8xf32>
        %122 = vector.outerproduct %121, %89, %120 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %123 = vector.extract %98[12] : vector<16x8xf32>
        %124 = vector.outerproduct %123, %91, %122 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %125 = vector.extract %98[13] : vector<16x8xf32>
        %126 = vector.outerproduct %125, %93, %124 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %127 = vector.extract %98[14] : vector<16x8xf32>
        %128 = vector.outerproduct %127, %95, %126 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %129 = vector.extract %98[15] : vector<16x8xf32>
        %130 = vector.outerproduct %129, %97, %128 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %130 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
      %31 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %31, %subview_2[%30, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
      %33 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %33, %subview_2[%32, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
      %35 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %35, %subview_2[%34, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
      %37 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %37, %subview_2[%36, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %38 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
      %39 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %39, %subview_2[%38, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %40 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
      %41 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %41, %subview_2[%40, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %42 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
      %43 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %43, %subview_2[%42, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.insert %37, %cst_0 [0] : vector<16xf32> into vector<8x16xf32>
        %39 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.insert %39, %38 [1] : vector<16xf32> into vector<8x16xf32>
        %41 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.insert %41, %40 [2] : vector<16xf32> into vector<8x16xf32>
        %43 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.insert %43, %42 [3] : vector<16xf32> into vector<8x16xf32>
        %45 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %46 = vector.insert %45, %44 [4] : vector<16xf32> into vector<8x16xf32>
        %47 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %48 = vector.insert %47, %46 [5] : vector<16xf32> into vector<8x16xf32>
        %49 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %50 = vector.insert %49, %48 [6] : vector<16xf32> into vector<8x16xf32>
        %51 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %52 = vector.insert %51, %50 [7] : vector<16xf32> into vector<8x16xf32>
        %53 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %77 = vector.load %subview_1[%76, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %78 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %79 = vector.load %subview_1[%78, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %80 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %81 = vector.load %subview_1[%80, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %82 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %83 = vector.load %subview_1[%82, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %84 = vector.transpose %52, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %85 = vector.extract %84[0] : vector<16x8xf32>
        %86 = vector.outerproduct %85, %53, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %87 = vector.extract %84[1] : vector<16x8xf32>
        %88 = vector.outerproduct %87, %55, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.extract %84[2] : vector<16x8xf32>
        %90 = vector.outerproduct %89, %57, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.extract %84[3] : vector<16x8xf32>
        %92 = vector.outerproduct %91, %59, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.extract %84[4] : vector<16x8xf32>
        %94 = vector.outerproduct %93, %61, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.extract %84[5] : vector<16x8xf32>
        %96 = vector.outerproduct %95, %63, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.extract %84[6] : vector<16x8xf32>
        %98 = vector.outerproduct %97, %65, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.extract %84[7] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %67, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %84[8] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %69, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %84[9] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %71, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %84[10] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %73, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %84[11] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %75, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.extract %84[12] : vector<16x8xf32>
        %110 = vector.outerproduct %109, %77, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.extract %84[13] : vector<16x8xf32>
        %112 = vector.outerproduct %111, %79, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.extract %84[14] : vector<16x8xf32>
        %114 = vector.outerproduct %113, %81, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.extract %84[15] : vector<16x8xf32>
        %116 = vector.outerproduct %115, %83, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %116 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.insert %37, %cst_0 [0] : vector<16xf32> into vector<8x16xf32>
        %39 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.insert %39, %38 [1] : vector<16xf32> into vector<8x16xf32>
        %41 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.insert %41, %40 [2] : vector<16xf32> into vector<8x16xf32>
        %43 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.insert %43, %42 [3] : vector<16xf32> into vector<8x16xf32>
        %45 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %46 = vector.insert %45, %44 [4] : vector<16xf32> into vector<8x16xf32>
        %47 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %48 = vector.insert %47, %46 [5] : vector<16xf32> into vector<8x16xf32>
        %49 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %50 = vector.insert %49, %48 [6] : vector<16xf32> into vector<8x16xf32>
        %51 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %52 = vector.insert %51, %50 [7] : vector<16xf32> into vector<8x16xf32>
        %53 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %77 = vector.load %subview_1[%76, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %78 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %79 = vector.load %subview_1[%78, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %80 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %81 = vector.load %subview_1[%80, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %82 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %83 = vector.load %subview_1[%82, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %84 = vector.transpose %52, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %85 = vector.extract %84[0] : vector<16x8xf32>
        %86 = vector.outerproduct %85, %53, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %87 = vector.extract %84[1] : vector<16x8xf32>
        %88 = vector.outerproduct %87, %55, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.extract %84[2] : vector<16x8xf32>
        %90 = vector.outerproduct %89, %57, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.extract %84[3] : vector<16x8xf32>
        %92 = vector.outerproduct %91, %59, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.extract %84[4] : vector<16x8xf32>
        %94 = vector.outerproduct %93, %61, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.extract %84[5] : vector<16x8xf32>
        %96 = vector.outerproduct %95, %63, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.extract %84[6] : vector<16x8xf32>
        %98 = vector.outerproduct %97, %65, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.extract %84[7] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %67, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %84[8] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %69, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %84[9] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %71, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %84[10] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %73, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %84[11] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %75, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.extract %84[12] : vector<16x8xf32>
        %110 = vector.outerproduct %109, %77, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.extract %84[13] : vector<16x8xf32>
        %112 = vector.outerproduct %111, %79, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.extract %84[14] : vector<16x8xf32>
        %114 = vector.outerproduct %113, %81, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.extract %84[15] : vector<16x8xf32>
        %116 = vector.outerproduct %115, %83, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %116 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.insert %37, %cst_0 [0] : vector<16xf32> into vector<8x16xf32>
        %39 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.insert %39, %38 [1] : vector<16xf32> into vector<8x16xf32>
        %41 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.insert %41, %40 [2] : vector<16xf32> into vector<8x16xf32>
        %43 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.insert %43, %42 [3] : vector<16xf32> into vector<8x16xf32>
        %45 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %46 = vector.insert %45, %44 [4] : vector<16xf32> into vector<8x16xf32>
        %47 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %48 = vector.insert %47, %46 [5] : vector<16xf32> into vector<8x16xf32>
        %49 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %50 = vector.insert %49, %48 [6] : vector<16xf32> into vector<8x16xf32>
        %51 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %52 = vector.insert %51, %50 [7] : vector<16xf32> into vector<8x16xf32>
        %53 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %77 = vector.load %subview_1[%76, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %78 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %79 = vector.load %subview_1[%78, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %80 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %81 = vector.load %subview_1[%80, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %82 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %83 = vector.load %subview_1[%82, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %84 = vector.transpose %52, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %85 = vector.extract %84[0] : vector<16x8xf32>
        %86 = vector.outerproduct %85, %53, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %87 = vector.extract %84[1] : vector<16x8xf32>
        %88 = vector.outerproduct %87, %55, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.extract %84[2] : vector<16x8xf32>
        %90 = vector.outerproduct %89, %57, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.extract %84[3] : vector<16x8xf32>
        %92 = vector.outerproduct %91, %59, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.extract %84[4] : vector<16x8xf32>
        %94 = vector.outerproduct %93, %61, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.extract %84[5] : vector<16x8xf32>
        %96 = vector.outerproduct %95, %63, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.extract %84[6] : vector<16x8xf32>
        %98 = vector.outerproduct %97, %65, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.extract %84[7] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %67, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %84[8] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %69, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %84[9] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %71, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %84[10] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %73, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %84[11] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %75, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.extract %84[12] : vector<16x8xf32>
        %110 = vector.outerproduct %109, %77, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.extract %84[13] : vector<16x8xf32>
        %112 = vector.outerproduct %111, %79, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.extract %84[14] : vector<16x8xf32>
        %114 = vector.outerproduct %113, %81, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.extract %84[15] : vector<16x8xf32>
        %116 = vector.outerproduct %115, %83, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %116 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.insert %37, %cst_0 [0] : vector<16xf32> into vector<8x16xf32>
        %39 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.insert %39, %38 [1] : vector<16xf32> into vector<8x16xf32>
        %41 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.insert %41, %40 [2] : vector<16xf32> into vector<8x16xf32>
        %43 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.insert %43, %42 [3] : vector<16xf32> into vector<8x16xf32>
        %45 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %46 = vector.insert %45, %44 [4] : vector<16xf32> into vector<8x16xf32>
        %47 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %48 = vector.insert %47, %46 [5] : vector<16xf32> into vector<8x16xf32>
        %49 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %50 = vector.insert %49, %48 [6] : vector<16xf32> into vector<8x16xf32>
        %51 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %52 = vector.insert %51, %50 [7] : vector<16xf32> into vector<8x16xf32>
        %53 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %77 = vector.load %subview_1[%76, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %78 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %79 = vector.load %subview_1[%78, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %80 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %81 = vector.load %subview_1[%80, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %82 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %83 = vector.load %subview_1[%82, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %84 = vector.transpose %52, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %85 = vector.extract %84[0] : vector<16x8xf32>
        %86 = vector.outerproduct %85, %53, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %87 = vector.extract %84[1] : vector<16x8xf32>
        %88 = vector.outerproduct %87, %55, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.extract %84[2] : vector<16x8xf32>
        %90 = vector.outerproduct %89, %57, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.extract %84[3] : vector<16x8xf32>
        %92 = vector.outerproduct %91, %59, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.extract %84[4] : vector<16x8xf32>
        %94 = vector.outerproduct %93, %61, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.extract %84[5] : vector<16x8xf32>
        %96 = vector.outerproduct %95, %63, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.extract %84[6] : vector<16x8xf32>
        %98 = vector.outerproduct %97, %65, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.extract %84[7] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %67, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %84[8] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %69, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %84[9] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %71, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %84[10] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %73, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %84[11] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %75, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.extract %84[12] : vector<16x8xf32>
        %110 = vector.outerproduct %109, %77, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.extract %84[13] : vector<16x8xf32>
        %112 = vector.outerproduct %111, %79, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.extract %84[14] : vector<16x8xf32>
        %114 = vector.outerproduct %113, %81, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.extract %84[15] : vector<16x8xf32>
        %116 = vector.outerproduct %115, %83, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %116 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.insert %37, %cst_0 [0] : vector<16xf32> into vector<8x16xf32>
        %39 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.insert %39, %38 [1] : vector<16xf32> into vector<8x16xf32>
        %41 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.insert %41, %40 [2] : vector<16xf32> into vector<8x16xf32>
        %43 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.insert %43, %42 [3] : vector<16xf32> into vector<8x16xf32>
        %45 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %46 = vector.insert %45, %44 [4] : vector<16xf32> into vector<8x16xf32>
        %47 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %48 = vector.insert %47, %46 [5] : vector<16xf32> into vector<8x16xf32>
        %49 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %50 = vector.insert %49, %48 [6] : vector<16xf32> into vector<8x16xf32>
        %51 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %52 = vector.insert %51, %50 [7] : vector<16xf32> into vector<8x16xf32>
        %53 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %77 = vector.load %subview_1[%76, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %78 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %79 = vector.load %subview_1[%78, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %80 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %81 = vector.load %subview_1[%80, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %82 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %83 = vector.load %subview_1[%82, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %84 = vector.transpose %52, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %85 = vector.extract %84[0] : vector<16x8xf32>
        %86 = vector.outerproduct %85, %53, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %87 = vector.extract %84[1] : vector<16x8xf32>
        %88 = vector.outerproduct %87, %55, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.extract %84[2] : vector<16x8xf32>
        %90 = vector.outerproduct %89, %57, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.extract %84[3] : vector<16x8xf32>
        %92 = vector.outerproduct %91, %59, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.extract %84[4] : vector<16x8xf32>
        %94 = vector.outerproduct %93, %61, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.extract %84[5] : vector<16x8xf32>
        %96 = vector.outerproduct %95, %63, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.extract %84[6] : vector<16x8xf32>
        %98 = vector.outerproduct %97, %65, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.extract %84[7] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %67, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %84[8] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %69, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %84[9] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %71, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %84[10] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %73, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %84[11] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %75, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.extract %84[12] : vector<16x8xf32>
        %110 = vector.outerproduct %109, %77, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.extract %84[13] : vector<16x8xf32>
        %112 = vector.outerproduct %111, %79, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.extract %84[14] : vector<16x8xf32>
        %114 = vector.outerproduct %113, %81, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.extract %84[15] : vector<16x8xf32>
        %116 = vector.outerproduct %115, %83, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %116 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.insert %37, %cst_0 [0] : vector<16xf32> into vector<8x16xf32>
        %39 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.insert %39, %38 [1] : vector<16xf32> into vector<8x16xf32>
        %41 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.insert %41, %40 [2] : vector<16xf32> into vector<8x16xf32>
        %43 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.insert %43, %42 [3] : vector<16xf32> into vector<8x16xf32>
        %45 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %46 = vector.insert %45, %44 [4] : vector<16xf32> into vector<8x16xf32>
        %47 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %48 = vector.insert %47, %46 [5] : vector<16xf32> into vector<8x16xf32>
        %49 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %50 = vector.insert %49, %48 [6] : vector<16xf32> into vector<8x16xf32>
        %51 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %52 = vector.insert %51, %50 [7] : vector<16xf32> into vector<8x16xf32>
        %53 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %77 = vector.load %subview_1[%76, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %78 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %79 = vector.load %subview_1[%78, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %80 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %81 = vector.load %subview_1[%80, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %82 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %83 = vector.load %subview_1[%82, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %84 = vector.transpose %52, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %85 = vector.extract %84[0] : vector<16x8xf32>
        %86 = vector.outerproduct %85, %53, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %87 = vector.extract %84[1] : vector<16x8xf32>
        %88 = vector.outerproduct %87, %55, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.extract %84[2] : vector<16x8xf32>
        %90 = vector.outerproduct %89, %57, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.extract %84[3] : vector<16x8xf32>
        %92 = vector.outerproduct %91, %59, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.extract %84[4] : vector<16x8xf32>
        %94 = vector.outerproduct %93, %61, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.extract %84[5] : vector<16x8xf32>
        %96 = vector.outerproduct %95, %63, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.extract %84[6] : vector<16x8xf32>
        %98 = vector.outerproduct %97, %65, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.extract %84[7] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %67, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %84[8] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %69, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %84[9] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %71, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %84[10] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %73, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %84[11] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %75, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.extract %84[12] : vector<16x8xf32>
        %110 = vector.outerproduct %109, %77, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.extract %84[13] : vector<16x8xf32>
        %112 = vector.outerproduct %111, %79, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.extract %84[14] : vector<16x8xf32>
        %114 = vector.outerproduct %113, %81, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.extract %84[15] : vector<16x8xf32>
        %116 = vector.outerproduct %115, %83, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %116 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.insert %37, %cst_0 [0] : vector<16xf32> into vector<8x16xf32>
        %39 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.insert %39, %38 [1] : vector<16xf32> into vector<8x16xf32>
        %41 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.insert %41, %40 [2] : vector<16xf32> into vector<8x16xf32>
        %43 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.insert %43, %42 [3] : vector<16xf32> into vector<8x16xf32>
        %45 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %46 = vector.insert %45, %44 [4] : vector<16xf32> into vector<8x16xf32>
        %47 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %48 = vector.insert %47, %46 [5] : vector<16xf32> into vector<8x16xf32>
        %49 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %50 = vector.insert %49, %48 [6] : vector<16xf32> into vector<8x16xf32>
        %51 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %52 = vector.insert %51, %50 [7] : vector<16xf32> into vector<8x16xf32>
        %53 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %77 = vector.load %subview_1[%76, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %78 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %79 = vector.load %subview_1[%78, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %80 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %81 = vector.load %subview_1[%80, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %82 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %83 = vector.load %subview_1[%82, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %84 = vector.transpose %52, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %85 = vector.extract %84[0] : vector<16x8xf32>
        %86 = vector.outerproduct %85, %53, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %87 = vector.extract %84[1] : vector<16x8xf32>
        %88 = vector.outerproduct %87, %55, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.extract %84[2] : vector<16x8xf32>
        %90 = vector.outerproduct %89, %57, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.extract %84[3] : vector<16x8xf32>
        %92 = vector.outerproduct %91, %59, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.extract %84[4] : vector<16x8xf32>
        %94 = vector.outerproduct %93, %61, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.extract %84[5] : vector<16x8xf32>
        %96 = vector.outerproduct %95, %63, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.extract %84[6] : vector<16x8xf32>
        %98 = vector.outerproduct %97, %65, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.extract %84[7] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %67, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %84[8] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %69, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %84[9] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %71, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %84[10] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %73, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %84[11] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %75, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.extract %84[12] : vector<16x8xf32>
        %110 = vector.outerproduct %109, %77, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.extract %84[13] : vector<16x8xf32>
        %112 = vector.outerproduct %111, %79, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.extract %84[14] : vector<16x8xf32>
        %114 = vector.outerproduct %113, %81, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.extract %84[15] : vector<16x8xf32>
        %116 = vector.outerproduct %115, %83, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %116 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.insert %37, %cst_0 [0] : vector<16xf32> into vector<8x16xf32>
        %39 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.insert %39, %38 [1] : vector<16xf32> into vector<8x16xf32>
        %41 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.insert %41, %40 [2] : vector<16xf32> into vector<8x16xf32>
        %43 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.insert %43, %42 [3] : vector<16xf32> into vector<8x16xf32>
        %45 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %46 = vector.insert %45, %44 [4] : vector<16xf32> into vector<8x16xf32>
        %47 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %48 = vector.insert %47, %46 [5] : vector<16xf32> into vector<8x16xf32>
        %49 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %50 = vector.insert %49, %48 [6] : vector<16xf32> into vector<8x16xf32>
        %51 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %52 = vector.insert %51, %50 [7] : vector<16xf32> into vector<8x16xf32>
        %53 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %77 = vector.load %subview_1[%76, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %78 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %79 = vector.load %subview_1[%78, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %80 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %81 = vector.load %subview_1[%80, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %82 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %83 = vector.load %subview_1[%82, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %84 = vector.transpose %52, [1, 0] : vector<8x16xf32> to vector<16x8xf32>
        %85 = vector.extract %84[0] : vector<16x8xf32>
        %86 = vector.outerproduct %85, %53, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %87 = vector.extract %84[1] : vector<16x8xf32>
        %88 = vector.outerproduct %87, %55, %86 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.extract %84[2] : vector<16x8xf32>
        %90 = vector.outerproduct %89, %57, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.extract %84[3] : vector<16x8xf32>
        %92 = vector.outerproduct %91, %59, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.extract %84[4] : vector<16x8xf32>
        %94 = vector.outerproduct %93, %61, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.extract %84[5] : vector<16x8xf32>
        %96 = vector.outerproduct %95, %63, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.extract %84[6] : vector<16x8xf32>
        %98 = vector.outerproduct %97, %65, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.extract %84[7] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %67, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %84[8] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %69, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %84[9] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %71, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %84[10] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %73, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %84[11] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %75, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.extract %84[12] : vector<16x8xf32>
        %110 = vector.outerproduct %109, %77, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.extract %84[13] : vector<16x8xf32>
        %112 = vector.outerproduct %111, %79, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.extract %84[14] : vector<16x8xf32>
        %114 = vector.outerproduct %113, %81, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.extract %84[15] : vector<16x8xf32>
        %116 = vector.outerproduct %115, %83, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %116 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.insert %37, %cst_0 [0] : vector<16xf32> into vector<8x16xf32>
        %39 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.insert %39, %38 [1] : vector<16xf32> into vector<8x16xf32>
        %41 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.insert %41, %40 [2] : vector<16xf32> into vector<8x16xf32>
        %43 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.insert %43, %42 [3] : vector<16xf32> into vector<8x16xf32>
        %45 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %46 = vector.insert %45, %44 [4] : vector<16xf32> into vector<8x16xf32>
        %47 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %48 = vector.insert %47, %46 [5] : vector<16xf32> into vector<8x16xf32>
        %49 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %50 = vector.insert %49, %48 [6] : vector<16xf32> into vector<8x16xf32>
        %51 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %52 = vector.insert %51, %50 [7] : vector<16xf32> into vector<8x16xf32>
        %53 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %77 = vector.load %subview_1[%76, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %78 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %79 = vector.load %subview_1[%78, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %80 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %81 = vector.load %subview_1[%80, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %82 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %83 = vector.load %subview_1[%82, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %84 = vector.shape_cast %52 : vector<8x16xf32> to vector<128xf32>
        %85 = vector.shuffle %84, %84 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %86 = vector.shape_cast %85 : vector<128xf32> to vector<16x8xf32>
        %87 = vector.extract %86[0] : vector<16x8xf32>
        %88 = vector.outerproduct %87, %53, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.extract %86[1] : vector<16x8xf32>
        %90 = vector.outerproduct %89, %55, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.extract %86[2] : vector<16x8xf32>
        %92 = vector.outerproduct %91, %57, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.extract %86[3] : vector<16x8xf32>
        %94 = vector.outerproduct %93, %59, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.extract %86[4] : vector<16x8xf32>
        %96 = vector.outerproduct %95, %61, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.extract %86[5] : vector<16x8xf32>
        %98 = vector.outerproduct %97, %63, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.extract %86[6] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %65, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %86[7] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %67, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %86[8] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %69, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %86[9] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %71, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %86[10] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %73, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.extract %86[11] : vector<16x8xf32>
        %110 = vector.outerproduct %109, %75, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.extract %86[12] : vector<16x8xf32>
        %112 = vector.outerproduct %111, %77, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.extract %86[13] : vector<16x8xf32>
        %114 = vector.outerproduct %113, %79, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.extract %86[14] : vector<16x8xf32>
        %116 = vector.outerproduct %115, %81, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %117 = vector.extract %86[15] : vector<16x8xf32>
        %118 = vector.outerproduct %117, %83, %116 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %118 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.insert %37, %cst_0 [0] : vector<16xf32> into vector<8x16xf32>
        %39 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.insert %39, %38 [1] : vector<16xf32> into vector<8x16xf32>
        %41 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.insert %41, %40 [2] : vector<16xf32> into vector<8x16xf32>
        %43 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.insert %43, %42 [3] : vector<16xf32> into vector<8x16xf32>
        %45 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %46 = vector.insert %45, %44 [4] : vector<16xf32> into vector<8x16xf32>
        %47 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %48 = vector.insert %47, %46 [5] : vector<16xf32> into vector<8x16xf32>
        %49 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %50 = vector.insert %49, %48 [6] : vector<16xf32> into vector<8x16xf32>
        %51 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %52 = vector.insert %51, %50 [7] : vector<16xf32> into vector<8x16xf32>
        %53 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %77 = vector.load %subview_1[%76, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %78 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %79 = vector.load %subview_1[%78, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %80 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %81 = vector.load %subview_1[%80, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %82 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %83 = vector.load %subview_1[%82, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %84 = vector.shape_cast %52 : vector<8x16xf32> to vector<128xf32>
        %85 = vector.shuffle %84, %84 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %86 = vector.shape_cast %85 : vector<128xf32> to vector<16x8xf32>
        %87 = vector.extract %86[0] : vector<16x8xf32>
        %88 = vector.outerproduct %87, %53, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.extract %86[1] : vector<16x8xf32>
        %90 = vector.outerproduct %89, %55, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.extract %86[2] : vector<16x8xf32>
        %92 = vector.outerproduct %91, %57, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.extract %86[3] : vector<16x8xf32>
        %94 = vector.outerproduct %93, %59, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.extract %86[4] : vector<16x8xf32>
        %96 = vector.outerproduct %95, %61, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.extract %86[5] : vector<16x8xf32>
        %98 = vector.outerproduct %97, %63, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.extract %86[6] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %65, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %86[7] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %67, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %86[8] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %69, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %86[9] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %71, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %86[10] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %73, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.extract %86[11] : vector<16x8xf32>
        %110 = vector.outerproduct %109, %75, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.extract %86[12] : vector<16x8xf32>
        %112 = vector.outerproduct %111, %77, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.extract %86[13] : vector<16x8xf32>
        %114 = vector.outerproduct %113, %79, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.extract %86[14] : vector<16x8xf32>
        %116 = vector.outerproduct %115, %81, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %117 = vector.extract %86[15] : vector<16x8xf32>
        %118 = vector.outerproduct %117, %83, %116 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %118 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.insert %37, %cst_0 [0] : vector<16xf32> into vector<8x16xf32>
        %39 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.insert %39, %38 [1] : vector<16xf32> into vector<8x16xf32>
        %41 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.insert %41, %40 [2] : vector<16xf32> into vector<8x16xf32>
        %43 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.insert %43, %42 [3] : vector<16xf32> into vector<8x16xf32>
        %45 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %46 = vector.insert %45, %44 [4] : vector<16xf32> into vector<8x16xf32>
        %47 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %48 = vector.insert %47, %46 [5] : vector<16xf32> into vector<8x16xf32>
        %49 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %50 = vector.insert %49, %48 [6] : vector<16xf32> into vector<8x16xf32>
        %51 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %52 = vector.insert %51, %50 [7] : vector<16xf32> into vector<8x16xf32>
        %53 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %77 = vector.load %subview_1[%76, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %78 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %79 = vector.load %subview_1[%78, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %80 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %81 = vector.load %subview_1[%80, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %82 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %83 = vector.load %subview_1[%82, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %84 = vector.shape_cast %52 : vector<8x16xf32> to vector<128xf32>
        %85 = vector.shuffle %84, %84 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %86 = vector.shape_cast %85 : vector<128xf32> to vector<16x8xf32>
        %87 = vector.extract %86[0] : vector<16x8xf32>
        %88 = vector.outerproduct %87, %53, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.extract %86[1] : vector<16x8xf32>
        %90 = vector.outerproduct %89, %55, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.extract %86[2] : vector<16x8xf32>
        %92 = vector.outerproduct %91, %57, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.extract %86[3] : vector<16x8xf32>
        %94 = vector.outerproduct %93, %59, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.extract %86[4] : vector<16x8xf32>
        %96 = vector.outerproduct %95, %61, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.extract %86[5] : vector<16x8xf32>
        %98 = vector.outerproduct %97, %63, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.extract %86[6] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %65, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %86[7] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %67, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %86[8] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %69, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %86[9] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %71, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %86[10] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %73, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.extract %86[11] : vector<16x8xf32>
        %110 = vector.outerproduct %109, %75, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.extract %86[12] : vector<16x8xf32>
        %112 = vector.outerproduct %111, %77, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.extract %86[13] : vector<16x8xf32>
        %114 = vector.outerproduct %113, %79, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.extract %86[14] : vector<16x8xf32>
        %116 = vector.outerproduct %115, %81, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %117 = vector.extract %86[15] : vector<16x8xf32>
        %118 = vector.outerproduct %117, %83, %116 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %118 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.insert %37, %cst_0 [0] : vector<16xf32> into vector<8x16xf32>
        %39 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.insert %39, %38 [1] : vector<16xf32> into vector<8x16xf32>
        %41 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.insert %41, %40 [2] : vector<16xf32> into vector<8x16xf32>
        %43 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.insert %43, %42 [3] : vector<16xf32> into vector<8x16xf32>
        %45 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %46 = vector.insert %45, %44 [4] : vector<16xf32> into vector<8x16xf32>
        %47 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %48 = vector.insert %47, %46 [5] : vector<16xf32> into vector<8x16xf32>
        %49 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %50 = vector.insert %49, %48 [6] : vector<16xf32> into vector<8x16xf32>
        %51 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %52 = vector.insert %51, %50 [7] : vector<16xf32> into vector<8x16xf32>
        %53 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %77 = vector.load %subview_1[%76, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %78 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %79 = vector.load %subview_1[%78, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %80 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %81 = vector.load %subview_1[%80, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %82 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %83 = vector.load %subview_1[%82, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %84 = vector.shape_cast %52 : vector<8x16xf32> to vector<128xf32>
        %85 = vector.shuffle %84, %84 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %86 = vector.shape_cast %85 : vector<128xf32> to vector<16x8xf32>
        %87 = vector.extract %86[0] : vector<16x8xf32>
        %88 = vector.outerproduct %87, %53, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.extract %86[1] : vector<16x8xf32>
        %90 = vector.outerproduct %89, %55, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.extract %86[2] : vector<16x8xf32>
        %92 = vector.outerproduct %91, %57, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.extract %86[3] : vector<16x8xf32>
        %94 = vector.outerproduct %93, %59, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.extract %86[4] : vector<16x8xf32>
        %96 = vector.outerproduct %95, %61, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.extract %86[5] : vector<16x8xf32>
        %98 = vector.outerproduct %97, %63, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.extract %86[6] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %65, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %86[7] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %67, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %86[8] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %69, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %86[9] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %71, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %86[10] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %73, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.extract %86[11] : vector<16x8xf32>
        %110 = vector.outerproduct %109, %75, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.extract %86[12] : vector<16x8xf32>
        %112 = vector.outerproduct %111, %77, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.extract %86[13] : vector<16x8xf32>
        %114 = vector.outerproduct %113, %79, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.extract %86[14] : vector<16x8xf32>
        %116 = vector.outerproduct %115, %81, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %117 = vector.extract %86[15] : vector<16x8xf32>
        %118 = vector.outerproduct %117, %83, %116 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %118 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.insert %37, %cst_0 [0] : vector<16xf32> into vector<8x16xf32>
        %39 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.insert %39, %38 [1] : vector<16xf32> into vector<8x16xf32>
        %41 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.insert %41, %40 [2] : vector<16xf32> into vector<8x16xf32>
        %43 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.insert %43, %42 [3] : vector<16xf32> into vector<8x16xf32>
        %45 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %46 = vector.insert %45, %44 [4] : vector<16xf32> into vector<8x16xf32>
        %47 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %48 = vector.insert %47, %46 [5] : vector<16xf32> into vector<8x16xf32>
        %49 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %50 = vector.insert %49, %48 [6] : vector<16xf32> into vector<8x16xf32>
        %51 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %52 = vector.insert %51, %50 [7] : vector<16xf32> into vector<8x16xf32>
        %53 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %77 = vector.load %subview_1[%76, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %78 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %79 = vector.load %subview_1[%78, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %80 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %81 = vector.load %subview_1[%80, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %82 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %83 = vector.load %subview_1[%82, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %84 = vector.shape_cast %52 : vector<8x16xf32> to vector<128xf32>
        %85 = vector.shuffle %84, %84 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %86 = vector.shape_cast %85 : vector<128xf32> to vector<16x8xf32>
        %87 = vector.extract %86[0] : vector<16x8xf32>
        %88 = vector.outerproduct %87, %53, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.extract %86[1] : vector<16x8xf32>
        %90 = vector.outerproduct %89, %55, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.extract %86[2] : vector<16x8xf32>
        %92 = vector.outerproduct %91, %57, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.extract %86[3] : vector<16x8xf32>
        %94 = vector.outerproduct %93, %59, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.extract %86[4] : vector<16x8xf32>
        %96 = vector.outerproduct %95, %61, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.extract %86[5] : vector<16x8xf32>
        %98 = vector.outerproduct %97, %63, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.extract %86[6] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %65, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %86[7] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %67, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %86[8] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %69, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %86[9] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %71, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %86[10] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %73, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.extract %86[11] : vector<16x8xf32>
        %110 = vector.outerproduct %109, %75, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.extract %86[12] : vector<16x8xf32>
        %112 = vector.outerproduct %111, %77, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.extract %86[13] : vector<16x8xf32>
        %114 = vector.outerproduct %113, %79, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.extract %86[14] : vector<16x8xf32>
        %116 = vector.outerproduct %115, %81, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %117 = vector.extract %86[15] : vector<16x8xf32>
        %118 = vector.outerproduct %117, %83, %116 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %118 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.insert %37, %cst_0 [0] : vector<16xf32> into vector<8x16xf32>
        %39 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.insert %39, %38 [1] : vector<16xf32> into vector<8x16xf32>
        %41 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.insert %41, %40 [2] : vector<16xf32> into vector<8x16xf32>
        %43 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.insert %43, %42 [3] : vector<16xf32> into vector<8x16xf32>
        %45 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %46 = vector.insert %45, %44 [4] : vector<16xf32> into vector<8x16xf32>
        %47 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %48 = vector.insert %47, %46 [5] : vector<16xf32> into vector<8x16xf32>
        %49 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %50 = vector.insert %49, %48 [6] : vector<16xf32> into vector<8x16xf32>
        %51 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %52 = vector.insert %51, %50 [7] : vector<16xf32> into vector<8x16xf32>
        %53 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %77 = vector.load %subview_1[%76, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %78 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %79 = vector.load %subview_1[%78, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %80 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %81 = vector.load %subview_1[%80, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %82 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %83 = vector.load %subview_1[%82, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %84 = vector.shape_cast %52 : vector<8x16xf32> to vector<128xf32>
        %85 = vector.shuffle %84, %84 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %86 = vector.shape_cast %85 : vector<128xf32> to vector<16x8xf32>
        %87 = vector.extract %86[0] : vector<16x8xf32>
        %88 = vector.outerproduct %87, %53, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.extract %86[1] : vector<16x8xf32>
        %90 = vector.outerproduct %89, %55, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.extract %86[2] : vector<16x8xf32>
        %92 = vector.outerproduct %91, %57, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.extract %86[3] : vector<16x8xf32>
        %94 = vector.outerproduct %93, %59, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.extract %86[4] : vector<16x8xf32>
        %96 = vector.outerproduct %95, %61, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.extract %86[5] : vector<16x8xf32>
        %98 = vector.outerproduct %97, %63, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.extract %86[6] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %65, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %86[7] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %67, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %86[8] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %69, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %86[9] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %71, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %86[10] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %73, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.extract %86[11] : vector<16x8xf32>
        %110 = vector.outerproduct %109, %75, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.extract %86[12] : vector<16x8xf32>
        %112 = vector.outerproduct %111, %77, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.extract %86[13] : vector<16x8xf32>
        %114 = vector.outerproduct %113, %79, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.extract %86[14] : vector<16x8xf32>
        %116 = vector.outerproduct %115, %81, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %117 = vector.extract %86[15] : vector<16x8xf32>
        %118 = vector.outerproduct %117, %83, %116 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %118 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.insert %37, %cst_0 [0] : vector<16xf32> into vector<8x16xf32>
        %39 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.insert %39, %38 [1] : vector<16xf32> into vector<8x16xf32>
        %41 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.insert %41, %40 [2] : vector<16xf32> into vector<8x16xf32>
        %43 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.insert %43, %42 [3] : vector<16xf32> into vector<8x16xf32>
        %45 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %46 = vector.insert %45, %44 [4] : vector<16xf32> into vector<8x16xf32>
        %47 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %48 = vector.insert %47, %46 [5] : vector<16xf32> into vector<8x16xf32>
        %49 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %50 = vector.insert %49, %48 [6] : vector<16xf32> into vector<8x16xf32>
        %51 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %52 = vector.insert %51, %50 [7] : vector<16xf32> into vector<8x16xf32>
        %53 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %77 = vector.load %subview_1[%76, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %78 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %79 = vector.load %subview_1[%78, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %80 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %81 = vector.load %subview_1[%80, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %82 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %83 = vector.load %subview_1[%82, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %84 = vector.shape_cast %52 : vector<8x16xf32> to vector<128xf32>
        %85 = vector.shuffle %84, %84 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %86 = vector.shape_cast %85 : vector<128xf32> to vector<16x8xf32>
        %87 = vector.extract %86[0] : vector<16x8xf32>
        %88 = vector.outerproduct %87, %53, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.extract %86[1] : vector<16x8xf32>
        %90 = vector.outerproduct %89, %55, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.extract %86[2] : vector<16x8xf32>
        %92 = vector.outerproduct %91, %57, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.extract %86[3] : vector<16x8xf32>
        %94 = vector.outerproduct %93, %59, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.extract %86[4] : vector<16x8xf32>
        %96 = vector.outerproduct %95, %61, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.extract %86[5] : vector<16x8xf32>
        %98 = vector.outerproduct %97, %63, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.extract %86[6] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %65, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %86[7] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %67, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %86[8] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %69, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %86[9] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %71, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %86[10] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %73, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.extract %86[11] : vector<16x8xf32>
        %110 = vector.outerproduct %109, %75, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.extract %86[12] : vector<16x8xf32>
        %112 = vector.outerproduct %111, %77, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.extract %86[13] : vector<16x8xf32>
        %114 = vector.outerproduct %113, %79, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.extract %86[14] : vector<16x8xf32>
        %116 = vector.outerproduct %115, %81, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %117 = vector.extract %86[15] : vector<16x8xf32>
        %118 = vector.outerproduct %117, %83, %116 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %118 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.insert %37, %cst_0 [0] : vector<16xf32> into vector<8x16xf32>
        %39 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.insert %39, %38 [1] : vector<16xf32> into vector<8x16xf32>
        %41 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.insert %41, %40 [2] : vector<16xf32> into vector<8x16xf32>
        %43 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.insert %43, %42 [3] : vector<16xf32> into vector<8x16xf32>
        %45 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %46 = vector.insert %45, %44 [4] : vector<16xf32> into vector<8x16xf32>
        %47 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %48 = vector.insert %47, %46 [5] : vector<16xf32> into vector<8x16xf32>
        %49 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %50 = vector.insert %49, %48 [6] : vector<16xf32> into vector<8x16xf32>
        %51 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %52 = vector.insert %51, %50 [7] : vector<16xf32> into vector<8x16xf32>
        %53 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %77 = vector.load %subview_1[%76, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %78 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %79 = vector.load %subview_1[%78, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %80 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %81 = vector.load %subview_1[%80, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %82 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %83 = vector.load %subview_1[%82, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %84 = vector.shape_cast %52 : vector<8x16xf32> to vector<128xf32>
        %85 = vector.shuffle %84, %84 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %86 = vector.shape_cast %85 : vector<128xf32> to vector<16x8xf32>
        %87 = vector.extract %86[0] : vector<16x8xf32>
        %88 = vector.outerproduct %87, %53, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.extract %86[1] : vector<16x8xf32>
        %90 = vector.outerproduct %89, %55, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.extract %86[2] : vector<16x8xf32>
        %92 = vector.outerproduct %91, %57, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.extract %86[3] : vector<16x8xf32>
        %94 = vector.outerproduct %93, %59, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.extract %86[4] : vector<16x8xf32>
        %96 = vector.outerproduct %95, %61, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.extract %86[5] : vector<16x8xf32>
        %98 = vector.outerproduct %97, %63, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.extract %86[6] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %65, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %86[7] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %67, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %86[8] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %69, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %86[9] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %71, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %86[10] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %73, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.extract %86[11] : vector<16x8xf32>
        %110 = vector.outerproduct %109, %75, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.extract %86[12] : vector<16x8xf32>
        %112 = vector.outerproduct %111, %77, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.extract %86[13] : vector<16x8xf32>
        %114 = vector.outerproduct %113, %79, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.extract %86[14] : vector<16x8xf32>
        %116 = vector.outerproduct %115, %81, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %117 = vector.extract %86[15] : vector<16x8xf32>
        %118 = vector.outerproduct %117, %83, %116 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %118 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x16xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.insert %37, %cst_0 [0] : vector<16xf32> into vector<8x16xf32>
        %39 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.insert %39, %38 [1] : vector<16xf32> into vector<8x16xf32>
        %41 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.insert %41, %40 [2] : vector<16xf32> into vector<8x16xf32>
        %43 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.insert %43, %42 [3] : vector<16xf32> into vector<8x16xf32>
        %45 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %46 = vector.insert %45, %44 [4] : vector<16xf32> into vector<8x16xf32>
        %47 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %48 = vector.insert %47, %46 [5] : vector<16xf32> into vector<8x16xf32>
        %49 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %50 = vector.insert %49, %48 [6] : vector<16xf32> into vector<8x16xf32>
        %51 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %52 = vector.insert %51, %50 [7] : vector<16xf32> into vector<8x16xf32>
        %53 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %77 = vector.load %subview_1[%76, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %78 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %79 = vector.load %subview_1[%78, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %80 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %81 = vector.load %subview_1[%80, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %82 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %83 = vector.load %subview_1[%82, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %84 = vector.shape_cast %52 : vector<8x16xf32> to vector<128xf32>
        %85 = vector.shuffle %84, %84 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %86 = vector.shape_cast %85 : vector<128xf32> to vector<16x8xf32>
        %87 = vector.extract %86[0] : vector<16x8xf32>
        %88 = vector.outerproduct %87, %53, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %89 = vector.extract %86[1] : vector<16x8xf32>
        %90 = vector.outerproduct %89, %55, %88 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %91 = vector.extract %86[2] : vector<16x8xf32>
        %92 = vector.outerproduct %91, %57, %90 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %93 = vector.extract %86[3] : vector<16x8xf32>
        %94 = vector.outerproduct %93, %59, %92 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %95 = vector.extract %86[4] : vector<16x8xf32>
        %96 = vector.outerproduct %95, %61, %94 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %97 = vector.extract %86[5] : vector<16x8xf32>
        %98 = vector.outerproduct %97, %63, %96 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %99 = vector.extract %86[6] : vector<16x8xf32>
        %100 = vector.outerproduct %99, %65, %98 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %101 = vector.extract %86[7] : vector<16x8xf32>
        %102 = vector.outerproduct %101, %67, %100 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.extract %86[8] : vector<16x8xf32>
        %104 = vector.outerproduct %103, %69, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.extract %86[9] : vector<16x8xf32>
        %106 = vector.outerproduct %105, %71, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.extract %86[10] : vector<16x8xf32>
        %108 = vector.outerproduct %107, %73, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.extract %86[11] : vector<16x8xf32>
        %110 = vector.outerproduct %109, %75, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.extract %86[12] : vector<16x8xf32>
        %112 = vector.outerproduct %111, %77, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.extract %86[13] : vector<16x8xf32>
        %114 = vector.outerproduct %113, %79, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.extract %86[14] : vector<16x8xf32>
        %116 = vector.outerproduct %115, %81, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %117 = vector.extract %86[15] : vector<16x8xf32>
        %118 = vector.outerproduct %117, %83, %116 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %118 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyLowerVectorsPass (iree-linalg-strategy-lower-vectors-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst_0 [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %39 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %41 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %43 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %45 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %46 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %47 = vector.load %subview_1[%46, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %48 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %49 = vector.load %subview_1[%48, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %50 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %51 = vector.load %subview_1[%50, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %52 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %53 = vector.load %subview_1[%52, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = vector.insert_strided_slice %37, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
        %77 = vector.insert_strided_slice %38, %76 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
        %78 = vector.insert_strided_slice %39, %77 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
        %79 = vector.insert_strided_slice %40, %78 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
        %80 = vector.insert_strided_slice %41, %79 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
        %81 = vector.insert_strided_slice %42, %80 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
        %82 = vector.insert_strided_slice %43, %81 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
        %83 = vector.insert_strided_slice %44, %82 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
        %84 = vector.shuffle %83, %83 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %85 = vector.extract_strided_slice %84 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %86 = vector.extract_strided_slice %84 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %87 = vector.extract_strided_slice %84 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %88 = vector.extract_strided_slice %84 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %89 = vector.extract_strided_slice %84 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %90 = vector.extract_strided_slice %84 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %91 = vector.extract_strided_slice %84 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %92 = vector.extract_strided_slice %84 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %93 = vector.extract_strided_slice %84 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %94 = vector.extract_strided_slice %84 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %95 = vector.extract_strided_slice %84 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %96 = vector.extract_strided_slice %84 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %97 = vector.extract_strided_slice %84 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %98 = vector.extract_strided_slice %84 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %99 = vector.extract_strided_slice %84 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %100 = vector.extract_strided_slice %84 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %101 = vector.outerproduct %85, %45, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %102 = vector.outerproduct %86, %47, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.outerproduct %87, %49, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %104 = vector.outerproduct %88, %51, %103 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.outerproduct %89, %53, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %106 = vector.outerproduct %90, %55, %105 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.outerproduct %91, %57, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %108 = vector.outerproduct %92, %59, %107 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.outerproduct %93, %61, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %110 = vector.outerproduct %94, %63, %109 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.outerproduct %95, %65, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %112 = vector.outerproduct %96, %67, %111 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.outerproduct %97, %69, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %114 = vector.outerproduct %98, %71, %113 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.outerproduct %99, %73, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %116 = vector.outerproduct %100, %75, %115 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %116 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst_0 [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %39 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %41 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %43 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %45 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %46 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %47 = vector.load %subview_1[%46, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %48 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %49 = vector.load %subview_1[%48, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %50 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %51 = vector.load %subview_1[%50, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %52 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %53 = vector.load %subview_1[%52, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = vector.insert_strided_slice %37, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
        %77 = vector.insert_strided_slice %38, %76 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
        %78 = vector.insert_strided_slice %39, %77 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
        %79 = vector.insert_strided_slice %40, %78 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
        %80 = vector.insert_strided_slice %41, %79 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
        %81 = vector.insert_strided_slice %42, %80 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
        %82 = vector.insert_strided_slice %43, %81 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
        %83 = vector.insert_strided_slice %44, %82 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
        %84 = vector.shuffle %83, %83 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %85 = vector.extract_strided_slice %84 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %86 = vector.extract_strided_slice %84 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %87 = vector.extract_strided_slice %84 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %88 = vector.extract_strided_slice %84 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %89 = vector.extract_strided_slice %84 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %90 = vector.extract_strided_slice %84 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %91 = vector.extract_strided_slice %84 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %92 = vector.extract_strided_slice %84 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %93 = vector.extract_strided_slice %84 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %94 = vector.extract_strided_slice %84 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %95 = vector.extract_strided_slice %84 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %96 = vector.extract_strided_slice %84 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %97 = vector.extract_strided_slice %84 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %98 = vector.extract_strided_slice %84 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %99 = vector.extract_strided_slice %84 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %100 = vector.extract_strided_slice %84 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %101 = vector.outerproduct %85, %45, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %102 = vector.outerproduct %86, %47, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.outerproduct %87, %49, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %104 = vector.outerproduct %88, %51, %103 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.outerproduct %89, %53, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %106 = vector.outerproduct %90, %55, %105 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.outerproduct %91, %57, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %108 = vector.outerproduct %92, %59, %107 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.outerproduct %93, %61, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %110 = vector.outerproduct %94, %63, %109 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.outerproduct %95, %65, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %112 = vector.outerproduct %96, %67, %111 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.outerproduct %97, %69, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %114 = vector.outerproduct %98, %71, %113 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.outerproduct %99, %73, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %116 = vector.outerproduct %100, %75, %115 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %116 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst_0 [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %39 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %41 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %43 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %45 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %46 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %47 = vector.load %subview_1[%46, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %48 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %49 = vector.load %subview_1[%48, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %50 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %51 = vector.load %subview_1[%50, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %52 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %53 = vector.load %subview_1[%52, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = vector.insert_strided_slice %37, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
        %77 = vector.insert_strided_slice %38, %76 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
        %78 = vector.insert_strided_slice %39, %77 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
        %79 = vector.insert_strided_slice %40, %78 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
        %80 = vector.insert_strided_slice %41, %79 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
        %81 = vector.insert_strided_slice %42, %80 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
        %82 = vector.insert_strided_slice %43, %81 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
        %83 = vector.insert_strided_slice %44, %82 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
        %84 = vector.shuffle %83, %83 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %85 = vector.extract_strided_slice %84 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %86 = vector.extract_strided_slice %84 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %87 = vector.extract_strided_slice %84 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %88 = vector.extract_strided_slice %84 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %89 = vector.extract_strided_slice %84 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %90 = vector.extract_strided_slice %84 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %91 = vector.extract_strided_slice %84 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %92 = vector.extract_strided_slice %84 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %93 = vector.extract_strided_slice %84 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %94 = vector.extract_strided_slice %84 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %95 = vector.extract_strided_slice %84 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %96 = vector.extract_strided_slice %84 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %97 = vector.extract_strided_slice %84 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %98 = vector.extract_strided_slice %84 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %99 = vector.extract_strided_slice %84 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %100 = vector.extract_strided_slice %84 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %101 = vector.outerproduct %85, %45, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %102 = vector.outerproduct %86, %47, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.outerproduct %87, %49, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %104 = vector.outerproduct %88, %51, %103 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.outerproduct %89, %53, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %106 = vector.outerproduct %90, %55, %105 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.outerproduct %91, %57, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %108 = vector.outerproduct %92, %59, %107 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.outerproduct %93, %61, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %110 = vector.outerproduct %94, %63, %109 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.outerproduct %95, %65, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %112 = vector.outerproduct %96, %67, %111 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.outerproduct %97, %69, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %114 = vector.outerproduct %98, %71, %113 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.outerproduct %99, %73, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %116 = vector.outerproduct %100, %75, %115 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %116 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyRemoveMarkersPass (iree-linalg-strategy-remove-markers-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst_0 [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %39 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %41 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %43 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %45 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %46 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %47 = vector.load %subview_1[%46, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %48 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %49 = vector.load %subview_1[%48, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %50 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %51 = vector.load %subview_1[%50, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %52 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %53 = vector.load %subview_1[%52, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = vector.insert_strided_slice %37, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
        %77 = vector.insert_strided_slice %38, %76 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
        %78 = vector.insert_strided_slice %39, %77 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
        %79 = vector.insert_strided_slice %40, %78 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
        %80 = vector.insert_strided_slice %41, %79 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
        %81 = vector.insert_strided_slice %42, %80 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
        %82 = vector.insert_strided_slice %43, %81 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
        %83 = vector.insert_strided_slice %44, %82 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
        %84 = vector.shuffle %83, %83 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %85 = vector.extract_strided_slice %84 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %86 = vector.extract_strided_slice %84 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %87 = vector.extract_strided_slice %84 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %88 = vector.extract_strided_slice %84 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %89 = vector.extract_strided_slice %84 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %90 = vector.extract_strided_slice %84 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %91 = vector.extract_strided_slice %84 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %92 = vector.extract_strided_slice %84 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %93 = vector.extract_strided_slice %84 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %94 = vector.extract_strided_slice %84 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %95 = vector.extract_strided_slice %84 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %96 = vector.extract_strided_slice %84 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %97 = vector.extract_strided_slice %84 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %98 = vector.extract_strided_slice %84 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %99 = vector.extract_strided_slice %84 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %100 = vector.extract_strided_slice %84 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %101 = vector.outerproduct %85, %45, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %102 = vector.outerproduct %86, %47, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.outerproduct %87, %49, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %104 = vector.outerproduct %88, %51, %103 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.outerproduct %89, %53, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %106 = vector.outerproduct %90, %55, %105 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.outerproduct %91, %57, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %108 = vector.outerproduct %92, %59, %107 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.outerproduct %93, %61, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %110 = vector.outerproduct %94, %63, %109 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.outerproduct %95, %65, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %112 = vector.outerproduct %96, %67, %111 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.outerproduct %97, %69, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %114 = vector.outerproduct %98, %71, %113 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.outerproduct %99, %73, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %116 = vector.outerproduct %100, %75, %115 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %116 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst_0 [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %39 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %41 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %43 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %45 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %46 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %47 = vector.load %subview_1[%46, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %48 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %49 = vector.load %subview_1[%48, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %50 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %51 = vector.load %subview_1[%50, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %52 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %53 = vector.load %subview_1[%52, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = vector.insert_strided_slice %37, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
        %77 = vector.insert_strided_slice %38, %76 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
        %78 = vector.insert_strided_slice %39, %77 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
        %79 = vector.insert_strided_slice %40, %78 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
        %80 = vector.insert_strided_slice %41, %79 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
        %81 = vector.insert_strided_slice %42, %80 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
        %82 = vector.insert_strided_slice %43, %81 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
        %83 = vector.insert_strided_slice %44, %82 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
        %84 = vector.shuffle %83, %83 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %85 = vector.extract_strided_slice %84 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %86 = vector.extract_strided_slice %84 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %87 = vector.extract_strided_slice %84 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %88 = vector.extract_strided_slice %84 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %89 = vector.extract_strided_slice %84 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %90 = vector.extract_strided_slice %84 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %91 = vector.extract_strided_slice %84 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %92 = vector.extract_strided_slice %84 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %93 = vector.extract_strided_slice %84 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %94 = vector.extract_strided_slice %84 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %95 = vector.extract_strided_slice %84 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %96 = vector.extract_strided_slice %84 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %97 = vector.extract_strided_slice %84 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %98 = vector.extract_strided_slice %84 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %99 = vector.extract_strided_slice %84 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %100 = vector.extract_strided_slice %84 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %101 = vector.outerproduct %85, %45, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %102 = vector.outerproduct %86, %47, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.outerproduct %87, %49, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %104 = vector.outerproduct %88, %51, %103 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.outerproduct %89, %53, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %106 = vector.outerproduct %90, %55, %105 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.outerproduct %91, %57, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %108 = vector.outerproduct %92, %59, %107 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.outerproduct %93, %61, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %110 = vector.outerproduct %94, %63, %109 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.outerproduct %95, %65, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %112 = vector.outerproduct %96, %67, %111 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.outerproduct %97, %69, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %114 = vector.outerproduct %98, %71, %113 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.outerproduct %99, %73, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %116 = vector.outerproduct %100, %75, %115 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %116 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgStrategyEnablePass (iree-linalg-strategy-enable-pass) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst_0 [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %39 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %41 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %43 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %45 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %46 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %47 = vector.load %subview_1[%46, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %48 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %49 = vector.load %subview_1[%48, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %50 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %51 = vector.load %subview_1[%50, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %52 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %53 = vector.load %subview_1[%52, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = vector.insert_strided_slice %37, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
        %77 = vector.insert_strided_slice %38, %76 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
        %78 = vector.insert_strided_slice %39, %77 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
        %79 = vector.insert_strided_slice %40, %78 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
        %80 = vector.insert_strided_slice %41, %79 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
        %81 = vector.insert_strided_slice %42, %80 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
        %82 = vector.insert_strided_slice %43, %81 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
        %83 = vector.insert_strided_slice %44, %82 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
        %84 = vector.shuffle %83, %83 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %85 = vector.extract_strided_slice %84 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %86 = vector.extract_strided_slice %84 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %87 = vector.extract_strided_slice %84 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %88 = vector.extract_strided_slice %84 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %89 = vector.extract_strided_slice %84 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %90 = vector.extract_strided_slice %84 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %91 = vector.extract_strided_slice %84 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %92 = vector.extract_strided_slice %84 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %93 = vector.extract_strided_slice %84 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %94 = vector.extract_strided_slice %84 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %95 = vector.extract_strided_slice %84 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %96 = vector.extract_strided_slice %84 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %97 = vector.extract_strided_slice %84 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %98 = vector.extract_strided_slice %84 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %99 = vector.extract_strided_slice %84 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %100 = vector.extract_strided_slice %84 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %101 = vector.outerproduct %85, %45, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %102 = vector.outerproduct %86, %47, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.outerproduct %87, %49, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %104 = vector.outerproduct %88, %51, %103 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.outerproduct %89, %53, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %106 = vector.outerproduct %90, %55, %105 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.outerproduct %91, %57, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %108 = vector.outerproduct %92, %59, %107 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.outerproduct %93, %61, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %110 = vector.outerproduct %94, %63, %109 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.outerproduct %95, %65, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %112 = vector.outerproduct %96, %67, %111 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.outerproduct %97, %69, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %114 = vector.outerproduct %98, %71, %113 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.outerproduct %99, %73, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %116 = vector.outerproduct %100, %75, %115 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %116 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LinalgVectorLowering (linalg-vector-lowering) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst_0 [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %39 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %41 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %43 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %45 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %46 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %47 = vector.load %subview_1[%46, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %48 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %49 = vector.load %subview_1[%48, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %50 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %51 = vector.load %subview_1[%50, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %52 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %53 = vector.load %subview_1[%52, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = vector.insert_strided_slice %37, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
        %77 = vector.insert_strided_slice %38, %76 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
        %78 = vector.insert_strided_slice %39, %77 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
        %79 = vector.insert_strided_slice %40, %78 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
        %80 = vector.insert_strided_slice %41, %79 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
        %81 = vector.insert_strided_slice %42, %80 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
        %82 = vector.insert_strided_slice %43, %81 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
        %83 = vector.insert_strided_slice %44, %82 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
        %84 = vector.shuffle %83, %83 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %85 = vector.extract_strided_slice %84 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %86 = vector.extract_strided_slice %84 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %87 = vector.extract_strided_slice %84 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %88 = vector.extract_strided_slice %84 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %89 = vector.extract_strided_slice %84 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %90 = vector.extract_strided_slice %84 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %91 = vector.extract_strided_slice %84 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %92 = vector.extract_strided_slice %84 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %93 = vector.extract_strided_slice %84 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %94 = vector.extract_strided_slice %84 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %95 = vector.extract_strided_slice %84 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %96 = vector.extract_strided_slice %84 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %97 = vector.extract_strided_slice %84 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %98 = vector.extract_strided_slice %84 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %99 = vector.extract_strided_slice %84 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %100 = vector.extract_strided_slice %84 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %101 = vector.outerproduct %85, %45, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %102 = vector.outerproduct %86, %47, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.outerproduct %87, %49, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %104 = vector.outerproduct %88, %51, %103 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.outerproduct %89, %53, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %106 = vector.outerproduct %90, %55, %105 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.outerproduct %91, %57, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %108 = vector.outerproduct %92, %59, %107 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.outerproduct %93, %61, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %110 = vector.outerproduct %94, %63, %109 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.outerproduct %95, %65, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %112 = vector.outerproduct %96, %67, %111 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.outerproduct %97, %69, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %114 = vector.outerproduct %98, %71, %113 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.outerproduct %99, %73, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %116 = vector.outerproduct %100, %75, %115 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %116 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst_0 [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %39 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %41 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %43 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %45 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %46 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %47 = vector.load %subview_1[%46, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %48 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %49 = vector.load %subview_1[%48, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %50 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %51 = vector.load %subview_1[%50, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %52 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %53 = vector.load %subview_1[%52, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = vector.insert_strided_slice %37, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
        %77 = vector.insert_strided_slice %38, %76 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
        %78 = vector.insert_strided_slice %39, %77 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
        %79 = vector.insert_strided_slice %40, %78 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
        %80 = vector.insert_strided_slice %41, %79 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
        %81 = vector.insert_strided_slice %42, %80 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
        %82 = vector.insert_strided_slice %43, %81 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
        %83 = vector.insert_strided_slice %44, %82 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
        %84 = vector.shuffle %83, %83 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %85 = vector.extract_strided_slice %84 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %86 = vector.extract_strided_slice %84 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %87 = vector.extract_strided_slice %84 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %88 = vector.extract_strided_slice %84 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %89 = vector.extract_strided_slice %84 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %90 = vector.extract_strided_slice %84 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %91 = vector.extract_strided_slice %84 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %92 = vector.extract_strided_slice %84 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %93 = vector.extract_strided_slice %84 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %94 = vector.extract_strided_slice %84 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %95 = vector.extract_strided_slice %84 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %96 = vector.extract_strided_slice %84 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %97 = vector.extract_strided_slice %84 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %98 = vector.extract_strided_slice %84 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %99 = vector.extract_strided_slice %84 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %100 = vector.extract_strided_slice %84 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %101 = vector.outerproduct %85, %45, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %102 = vector.outerproduct %86, %47, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.outerproduct %87, %49, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %104 = vector.outerproduct %88, %51, %103 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.outerproduct %89, %53, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %106 = vector.outerproduct %90, %55, %105 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.outerproduct %91, %57, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %108 = vector.outerproduct %92, %59, %107 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.outerproduct %93, %61, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %110 = vector.outerproduct %94, %63, %109 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.outerproduct %95, %65, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %112 = vector.outerproduct %96, %67, %111 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.outerproduct %97, %69, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %114 = vector.outerproduct %98, %71, %113 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.outerproduct %99, %73, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %116 = vector.outerproduct %100, %75, %115 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %116 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
  %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
  memref.assume_alignment %0, 64 : memref<512x256xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
  memref.assume_alignment %1, 64 : memref<256x1024xf32>
  %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
  memref.assume_alignment %2, 64 : memref<512x1024xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_id_y = hal.interface.workgroup.id[1] : index
  %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
  %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
  %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
  %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
  scf.for %arg0 = %c0 to %c256 step %c8 {
    %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
    %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
    %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
    %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
    %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
    %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
    %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
    scf.for %arg1 = %c0 to %c128 step %c32 {
      %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %13 = vector.insert %12, %cst_0 [0] : vector<32xf32> into vector<8x32xf32>
      %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
      %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
      %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
      %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
      %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
      %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
      %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
      %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
        %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %38 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %39 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %40 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %41 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %42 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %43 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %44 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
        %45 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %46 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
        %47 = vector.load %subview_1[%46, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %48 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
        %49 = vector.load %subview_1[%48, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %50 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
        %51 = vector.load %subview_1[%50, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %52 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
        %53 = vector.load %subview_1[%52, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %54 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
        %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %56 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
        %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %58 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
        %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %60 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
        %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %62 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
        %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %64 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
        %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %66 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
        %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %68 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
        %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %70 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
        %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %72 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
        %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %74 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
        %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        %76 = vector.insert_strided_slice %37, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
        %77 = vector.insert_strided_slice %38, %76 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
        %78 = vector.insert_strided_slice %39, %77 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
        %79 = vector.insert_strided_slice %40, %78 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
        %80 = vector.insert_strided_slice %41, %79 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
        %81 = vector.insert_strided_slice %42, %80 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
        %82 = vector.insert_strided_slice %43, %81 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
        %83 = vector.insert_strided_slice %44, %82 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
        %84 = vector.shuffle %83, %83 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
        %85 = vector.extract_strided_slice %84 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %86 = vector.extract_strided_slice %84 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %87 = vector.extract_strided_slice %84 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %88 = vector.extract_strided_slice %84 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %89 = vector.extract_strided_slice %84 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %90 = vector.extract_strided_slice %84 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %91 = vector.extract_strided_slice %84 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %92 = vector.extract_strided_slice %84 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %93 = vector.extract_strided_slice %84 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %94 = vector.extract_strided_slice %84 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %95 = vector.extract_strided_slice %84 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %96 = vector.extract_strided_slice %84 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %97 = vector.extract_strided_slice %84 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %98 = vector.extract_strided_slice %84 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %99 = vector.extract_strided_slice %84 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %100 = vector.extract_strided_slice %84 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
        %101 = vector.outerproduct %85, %45, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %102 = vector.outerproduct %86, %47, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %103 = vector.outerproduct %87, %49, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %104 = vector.outerproduct %88, %51, %103 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %105 = vector.outerproduct %89, %53, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %106 = vector.outerproduct %90, %55, %105 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %107 = vector.outerproduct %91, %57, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %108 = vector.outerproduct %92, %59, %107 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %109 = vector.outerproduct %93, %61, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %110 = vector.outerproduct %94, %63, %109 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %111 = vector.outerproduct %95, %65, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %112 = vector.outerproduct %96, %67, %111 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %113 = vector.outerproduct %97, %69, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %114 = vector.outerproduct %98, %71, %113 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %115 = vector.outerproduct %99, %73, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        %116 = vector.outerproduct %100, %75, %115 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
        scf.yield %116 : vector<8x32xf32>
      }
      %29 = vector.extract %28[0] : vector<8x32xf32>
      vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %30 = vector.extract %28[1] : vector<8x32xf32>
      vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %31 = vector.extract %28[2] : vector<8x32xf32>
      vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %32 = vector.extract %28[3] : vector<8x32xf32>
      vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %33 = vector.extract %28[4] : vector<8x32xf32>
      vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %34 = vector.extract %28[5] : vector<8x32xf32>
      vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %35 = vector.extract %28[6] : vector<8x32xf32>
      vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
      %36 = vector.extract %28[7] : vector<8x32xf32>
      vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
    }
  }
  return
}

// -----// IR Dump After LLVMCPULowerExecutableTarget (iree-llvmcpu-lower-executable-target) //----- //
hal.executable.variant public @embedded_elf_x86_64, target = <"llvm-cpu", "embedded-elf-x86_64", {cpu = "generic", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", native_vector_size = 16 : index, target_triple = "x86_64-unknown-unknown-eabi-elf"}> {
  hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingPadExpert>} {
  ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
    %c8 = arith.constant 8 : index
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    hal.return %c8, %c2, %c1 : index, index, index
  }
  builtin.module {
    func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
      %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
      %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
      %c16 = arith.constant 16 : index
      %c8 = arith.constant 8 : index
      %c32 = arith.constant 32 : index
      %c256 = arith.constant 256 : index
      %c128 = arith.constant 128 : index
      %c0 = arith.constant 0 : index
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
      memref.assume_alignment %0, 64 : memref<512x256xf32>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
      memref.assume_alignment %1, 64 : memref<256x1024xf32>
      %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
      memref.assume_alignment %2, 64 : memref<512x1024xf32>
      %workgroup_id_x = hal.interface.workgroup.id[0] : index
      %workgroup_id_y = hal.interface.workgroup.id[1] : index
      %3 = affine.apply affine_map<()[s0] -> (s0 * 256)>()[%workgroup_id_y]
      %4 = affine.apply affine_map<()[s0] -> (s0 * 128)>()[%workgroup_id_x]
      %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
      %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
      %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
      scf.for %arg0 = %c0 to %c256 step %c8 {
        %5 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg0)
        %6 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg0)
        %7 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg0)
        %8 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg0)
        %9 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg0)
        %10 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg0)
        %11 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg0)
        scf.for %arg1 = %c0 to %c128 step %c32 {
          %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %13 = vector.insert %12, %cst_0 [0] : vector<32xf32> into vector<8x32xf32>
          %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
          %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
          %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
          %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
          %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
          %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
          %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
          %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
            %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
            %38 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
            %39 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
            %40 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
            %41 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
            %42 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
            %43 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
            %44 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
            %45 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %46 = affine.apply affine_map<(d0) -> (d0 + 1)>(%arg2)
            %47 = vector.load %subview_1[%46, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %48 = affine.apply affine_map<(d0) -> (d0 + 2)>(%arg2)
            %49 = vector.load %subview_1[%48, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %50 = affine.apply affine_map<(d0) -> (d0 + 3)>(%arg2)
            %51 = vector.load %subview_1[%50, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %52 = affine.apply affine_map<(d0) -> (d0 + 4)>(%arg2)
            %53 = vector.load %subview_1[%52, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %54 = affine.apply affine_map<(d0) -> (d0 + 5)>(%arg2)
            %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %56 = affine.apply affine_map<(d0) -> (d0 + 6)>(%arg2)
            %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %58 = affine.apply affine_map<(d0) -> (d0 + 7)>(%arg2)
            %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %60 = affine.apply affine_map<(d0) -> (d0 + 8)>(%arg2)
            %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %62 = affine.apply affine_map<(d0) -> (d0 + 9)>(%arg2)
            %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %64 = affine.apply affine_map<(d0) -> (d0 + 10)>(%arg2)
            %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %66 = affine.apply affine_map<(d0) -> (d0 + 11)>(%arg2)
            %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %68 = affine.apply affine_map<(d0) -> (d0 + 12)>(%arg2)
            %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %70 = affine.apply affine_map<(d0) -> (d0 + 13)>(%arg2)
            %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %72 = affine.apply affine_map<(d0) -> (d0 + 14)>(%arg2)
            %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %74 = affine.apply affine_map<(d0) -> (d0 + 15)>(%arg2)
            %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            %76 = vector.insert_strided_slice %37, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
            %77 = vector.insert_strided_slice %38, %76 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
            %78 = vector.insert_strided_slice %39, %77 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
            %79 = vector.insert_strided_slice %40, %78 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
            %80 = vector.insert_strided_slice %41, %79 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
            %81 = vector.insert_strided_slice %42, %80 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
            %82 = vector.insert_strided_slice %43, %81 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
            %83 = vector.insert_strided_slice %44, %82 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
            %84 = vector.shuffle %83, %83 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
            %85 = vector.extract_strided_slice %84 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %86 = vector.extract_strided_slice %84 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %87 = vector.extract_strided_slice %84 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %88 = vector.extract_strided_slice %84 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %89 = vector.extract_strided_slice %84 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %90 = vector.extract_strided_slice %84 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %91 = vector.extract_strided_slice %84 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %92 = vector.extract_strided_slice %84 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %93 = vector.extract_strided_slice %84 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %94 = vector.extract_strided_slice %84 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %95 = vector.extract_strided_slice %84 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %96 = vector.extract_strided_slice %84 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %97 = vector.extract_strided_slice %84 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %98 = vector.extract_strided_slice %84 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %99 = vector.extract_strided_slice %84 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %100 = vector.extract_strided_slice %84 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
            %101 = vector.outerproduct %85, %45, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %102 = vector.outerproduct %86, %47, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %103 = vector.outerproduct %87, %49, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %104 = vector.outerproduct %88, %51, %103 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %105 = vector.outerproduct %89, %53, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %106 = vector.outerproduct %90, %55, %105 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %107 = vector.outerproduct %91, %57, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %108 = vector.outerproduct %92, %59, %107 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %109 = vector.outerproduct %93, %61, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %110 = vector.outerproduct %94, %63, %109 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %111 = vector.outerproduct %95, %65, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %112 = vector.outerproduct %96, %67, %111 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %113 = vector.outerproduct %97, %69, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %114 = vector.outerproduct %98, %71, %113 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %115 = vector.outerproduct %99, %73, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            %116 = vector.outerproduct %100, %75, %115 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
            scf.yield %116 : vector<8x32xf32>
          }
          %29 = vector.extract %28[0] : vector<8x32xf32>
          vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %30 = vector.extract %28[1] : vector<8x32xf32>
          vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %31 = vector.extract %28[2] : vector<8x32xf32>
          vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %32 = vector.extract %28[3] : vector<8x32xf32>
          vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %33 = vector.extract %28[4] : vector<8x32xf32>
          vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %34 = vector.extract %28[5] : vector<8x32xf32>
          vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %35 = vector.extract %28[6] : vector<8x32xf32>
          vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
          %36 = vector.extract %28[7] : vector<8x32xf32>
          vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
        }
      }
      return
    }
  }
}

#executable_target_embedded_elf_x86_64_ = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "generic", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128", native_vector_size = 16 : index, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<()[s0] -> (s0 * 256)>
#map1 = affine_map<()[s0] -> (s0 * 128)>
#map2 = affine_map<(d0) -> (d0 + 1)>
#map3 = affine_map<(d0) -> (d0 + 2)>
#map4 = affine_map<(d0) -> (d0 + 3)>
#map5 = affine_map<(d0) -> (d0 + 4)>
#map6 = affine_map<(d0) -> (d0 + 5)>
#map7 = affine_map<(d0) -> (d0 + 6)>
#map8 = affine_map<(d0) -> (d0 + 7)>
#map9 = affine_map<(d0) -> (d0 + 8)>
#map10 = affine_map<(d0) -> (d0 + 9)>
#map11 = affine_map<(d0) -> (d0 + 10)>
#map12 = affine_map<(d0) -> (d0 + 11)>
#map13 = affine_map<(d0) -> (d0 + 12)>
#map14 = affine_map<(d0) -> (d0 + 13)>
#map15 = affine_map<(d0) -> (d0 + 14)>
#map16 = affine_map<(d0) -> (d0 + 15)>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer, ReadOnly>, <2, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingPadExpert>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_embedded_elf_x86_64_]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  hal.executable private @matmul_static_dispatch_0 {
    hal.executable.variant public @embedded_elf_x86_64, target = #executable_target_embedded_elf_x86_64_ {
      hal.executable.export public @matmul_static_dispatch_0_matmul_512x1024x256 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device, %arg1: index, %arg2: index, %arg3: index):
        %c8 = arith.constant 8 : index
        %c2 = arith.constant 2 : index
        %c1 = arith.constant 1 : index
        hal.return %c8, %c2, %c1 : index, index, index
      }
      builtin.module {
        func.func @matmul_static_dispatch_0_matmul_512x1024x256() {
          %cst = arith.constant dense<0.000000e+00> : vector<128xf32>
          %cst_0 = arith.constant dense<0.000000e+00> : vector<8x32xf32>
          %c16 = arith.constant 16 : index
          %c8 = arith.constant 8 : index
          %c32 = arith.constant 32 : index
          %c256 = arith.constant 256 : index
          %c128 = arith.constant 128 : index
          %c0 = arith.constant 0 : index
          %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<512x256xf32>
          memref.assume_alignment %0, 64 : memref<512x256xf32>
          %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<256x1024xf32>
          memref.assume_alignment %1, 64 : memref<256x1024xf32>
          %2 = hal.interface.binding.subspan set(0) binding(2) type(storage_buffer) alignment(64) offset(%c0) : memref<512x1024xf32>
          memref.assume_alignment %2, 64 : memref<512x1024xf32>
          %workgroup_id_x = hal.interface.workgroup.id[0] : index
          %workgroup_id_y = hal.interface.workgroup.id[1] : index
          %3 = affine.apply #map()[%workgroup_id_y]
          %4 = affine.apply #map1()[%workgroup_id_x]
          %subview = memref.subview %0[%3, 0] [256, 256] [1, 1] : memref<512x256xf32> to memref<256x256xf32, strided<[256, 1], offset: ?>>
          %subview_1 = memref.subview %1[0, %4] [256, 128] [1, 1] : memref<256x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
          %subview_2 = memref.subview %2[%3, %4] [256, 128] [1, 1] : memref<512x1024xf32> to memref<256x128xf32, strided<[1024, 1], offset: ?>>
          scf.for %arg0 = %c0 to %c256 step %c8 {
            %5 = affine.apply #map2(%arg0)
            %6 = affine.apply #map3(%arg0)
            %7 = affine.apply #map4(%arg0)
            %8 = affine.apply #map5(%arg0)
            %9 = affine.apply #map6(%arg0)
            %10 = affine.apply #map7(%arg0)
            %11 = affine.apply #map8(%arg0)
            scf.for %arg1 = %c0 to %c128 step %c32 {
              %12 = vector.load %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %13 = vector.insert %12, %cst_0 [0] : vector<32xf32> into vector<8x32xf32>
              %14 = vector.load %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %15 = vector.insert %14, %13 [1] : vector<32xf32> into vector<8x32xf32>
              %16 = vector.load %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %17 = vector.insert %16, %15 [2] : vector<32xf32> into vector<8x32xf32>
              %18 = vector.load %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %19 = vector.insert %18, %17 [3] : vector<32xf32> into vector<8x32xf32>
              %20 = vector.load %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %21 = vector.insert %20, %19 [4] : vector<32xf32> into vector<8x32xf32>
              %22 = vector.load %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %23 = vector.insert %22, %21 [5] : vector<32xf32> into vector<8x32xf32>
              %24 = vector.load %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %25 = vector.insert %24, %23 [6] : vector<32xf32> into vector<8x32xf32>
              %26 = vector.load %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %27 = vector.insert %26, %25 [7] : vector<32xf32> into vector<8x32xf32>
              %28 = scf.for %arg2 = %c0 to %c256 step %c16 iter_args(%arg3 = %27) -> (vector<8x32xf32>) {
                %37 = vector.load %subview[%arg0, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
                %38 = vector.load %subview[%5, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
                %39 = vector.load %subview[%6, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
                %40 = vector.load %subview[%7, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
                %41 = vector.load %subview[%8, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
                %42 = vector.load %subview[%9, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
                %43 = vector.load %subview[%10, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
                %44 = vector.load %subview[%11, %arg2] : memref<256x256xf32, strided<[256, 1], offset: ?>>, vector<16xf32>
                %45 = vector.load %subview_1[%arg2, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %46 = affine.apply #map2(%arg2)
                %47 = vector.load %subview_1[%46, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %48 = affine.apply #map3(%arg2)
                %49 = vector.load %subview_1[%48, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %50 = affine.apply #map4(%arg2)
                %51 = vector.load %subview_1[%50, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %52 = affine.apply #map5(%arg2)
                %53 = vector.load %subview_1[%52, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %54 = affine.apply #map6(%arg2)
                %55 = vector.load %subview_1[%54, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %56 = affine.apply #map7(%arg2)
                %57 = vector.load %subview_1[%56, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %58 = affine.apply #map8(%arg2)
                %59 = vector.load %subview_1[%58, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %60 = affine.apply #map9(%arg2)
                %61 = vector.load %subview_1[%60, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %62 = affine.apply #map10(%arg2)
                %63 = vector.load %subview_1[%62, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %64 = affine.apply #map11(%arg2)
                %65 = vector.load %subview_1[%64, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %66 = affine.apply #map12(%arg2)
                %67 = vector.load %subview_1[%66, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %68 = affine.apply #map13(%arg2)
                %69 = vector.load %subview_1[%68, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %70 = affine.apply #map14(%arg2)
                %71 = vector.load %subview_1[%70, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %72 = affine.apply #map15(%arg2)
                %73 = vector.load %subview_1[%72, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %74 = affine.apply #map16(%arg2)
                %75 = vector.load %subview_1[%74, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
                %76 = vector.insert_strided_slice %37, %cst {offsets = [0], strides = [1]} : vector<16xf32> into vector<128xf32>
                %77 = vector.insert_strided_slice %38, %76 {offsets = [16], strides = [1]} : vector<16xf32> into vector<128xf32>
                %78 = vector.insert_strided_slice %39, %77 {offsets = [32], strides = [1]} : vector<16xf32> into vector<128xf32>
                %79 = vector.insert_strided_slice %40, %78 {offsets = [48], strides = [1]} : vector<16xf32> into vector<128xf32>
                %80 = vector.insert_strided_slice %41, %79 {offsets = [64], strides = [1]} : vector<16xf32> into vector<128xf32>
                %81 = vector.insert_strided_slice %42, %80 {offsets = [80], strides = [1]} : vector<16xf32> into vector<128xf32>
                %82 = vector.insert_strided_slice %43, %81 {offsets = [96], strides = [1]} : vector<16xf32> into vector<128xf32>
                %83 = vector.insert_strided_slice %44, %82 {offsets = [112], strides = [1]} : vector<16xf32> into vector<128xf32>
                %84 = vector.shuffle %83, %83 [0, 16, 32, 48, 64, 80, 96, 112, 1, 17, 33, 49, 65, 81, 97, 113, 2, 18, 34, 50, 66, 82, 98, 114, 3, 19, 35, 51, 67, 83, 99, 115, 4, 20, 36, 52, 68, 84, 100, 116, 5, 21, 37, 53, 69, 85, 101, 117, 6, 22, 38, 54, 70, 86, 102, 118, 7, 23, 39, 55, 71, 87, 103, 119, 8, 24, 40, 56, 72, 88, 104, 120, 9, 25, 41, 57, 73, 89, 105, 121, 10, 26, 42, 58, 74, 90, 106, 122, 11, 27, 43, 59, 75, 91, 107, 123, 12, 28, 44, 60, 76, 92, 108, 124, 13, 29, 45, 61, 77, 93, 109, 125, 14, 30, 46, 62, 78, 94, 110, 126, 15, 31, 47, 63, 79, 95, 111, 127] : vector<128xf32>, vector<128xf32>
                %85 = vector.extract_strided_slice %84 {offsets = [0], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %86 = vector.extract_strided_slice %84 {offsets = [8], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %87 = vector.extract_strided_slice %84 {offsets = [16], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %88 = vector.extract_strided_slice %84 {offsets = [24], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %89 = vector.extract_strided_slice %84 {offsets = [32], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %90 = vector.extract_strided_slice %84 {offsets = [40], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %91 = vector.extract_strided_slice %84 {offsets = [48], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %92 = vector.extract_strided_slice %84 {offsets = [56], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %93 = vector.extract_strided_slice %84 {offsets = [64], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %94 = vector.extract_strided_slice %84 {offsets = [72], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %95 = vector.extract_strided_slice %84 {offsets = [80], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %96 = vector.extract_strided_slice %84 {offsets = [88], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %97 = vector.extract_strided_slice %84 {offsets = [96], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %98 = vector.extract_strided_slice %84 {offsets = [104], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %99 = vector.extract_strided_slice %84 {offsets = [112], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %100 = vector.extract_strided_slice %84 {offsets = [120], sizes = [8], strides = [1]} : vector<128xf32> to vector<8xf32>
                %101 = vector.outerproduct %85, %45, %arg3 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %102 = vector.outerproduct %86, %47, %101 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %103 = vector.outerproduct %87, %49, %102 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %104 = vector.outerproduct %88, %51, %103 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %105 = vector.outerproduct %89, %53, %104 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %106 = vector.outerproduct %90, %55, %105 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %107 = vector.outerproduct %91, %57, %106 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %108 = vector.outerproduct %92, %59, %107 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %109 = vector.outerproduct %93, %61, %108 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %110 = vector.outerproduct %94, %63, %109 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %111 = vector.outerproduct %95, %65, %110 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %112 = vector.outerproduct %96, %67, %111 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %113 = vector.outerproduct %97, %69, %112 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %114 = vector.outerproduct %98, %71, %113 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %115 = vector.outerproduct %99, %73, %114 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                %116 = vector.outerproduct %100, %75, %115 {kind = #vector.kind<add>} : vector<8xf32>, vector<32xf32>
                scf.yield %116 : vector<8x32xf32>
              }
              %29 = vector.extract %28[0] : vector<8x32xf32>
              vector.store %29, %subview_2[%arg0, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %30 = vector.extract %28[1] : vector<8x32xf32>
              vector.store %30, %subview_2[%5, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %31 = vector.extract %28[2] : vector<8x32xf32>
              vector.store %31, %subview_2[%6, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %32 = vector.extract %28[3] : vector<8x32xf32>
              vector.store %32, %subview_2[%7, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %33 = vector.extract %28[4] : vector<8x32xf32>
              vector.store %33, %subview_2[%8, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %34 = vector.extract %28[5] : vector<8x32xf32>
              vector.store %34, %subview_2[%9, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %35 = vector.extract %28[6] : vector<8x32xf32>
              vector.store %35, %subview_2[%10, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
              %36 = vector.extract %28[7] : vector<8x32xf32>
              vector.store %36, %subview_2[%11, %arg1] : memref<256x128xf32, strided<[1024, 1], offset: ?>>, vector<32xf32>
            }
          }
          return
        }
      }
    }
  }
}

